{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"S00-intro/L1-course-intro/","text":"Welcome! Hello and welcome to ConsenSys Academy\u2019s 2021 Blockchain Developer Bootcamp, where you will learn everything you need to know to become a world-class blockchain developer. The blockchain field develops at an incredibly fast pace. This course will give you a solid foundation of blockchain principles. It will also introduce you to the tools used to build blockchain projects today. It can be overwhelming to enter such a rapidly developing field and we hope to give you mental model to incorporate new information as it comes in. This cohort will launch on September 1st, 2021. You're in good company! Developers from all over the world will be your classmates, and this course is lead by some of the best in the business, and will feature presentations from builders, developers, and entrepreneurs in the space. We want to share the ConsenSys network with you. Course content In this course, we start off by teaching you about the underpinnings of the blockchain and show how it all comes together to allow us to build the next generation of web applications. We will start with an overview of what we call blockchain primitives: The technology brought together underneath the name blockchain. We'll then dive deeper into the specifics of the Ethereum protocol and learn how those blockchain primitives show up there. Next, we'll move on to understanding writing smart contracts. We introduce you to Solidity, a programming language for Smart Contracts that lets you interact with the Ethereum Virtual Machine. We'll learn about Truffle so you can try out your new Solidity skills! We'll also discuss smart contract design patterns and security for your smart contract. After that, we'll introduce you to developer tooling, including Web 3 libraries, and other development frameworks. Next, we'll discuss the way in which people are taking blockchain development to the next level, with an introduction to Decentralized Finance (DeFi) and Decentralized Autonomous Organizations (DAOs). While this material may seem a bit overwhelming, it's important to have available to you as you continue to develop in the space. And we won't be testing you on it! Next, we'll touch on scalability: What it is, why it's necessary, and how some folks are proposing we approach it. We'll have a few tutorials allowing you to setup your own development environment for working on some of these scalability solutions. Last, we'll cover Ethereum 2.0, the upgrading of the Ethereum consensus mechanism from Proof of Work to Proof of Stake. We'll touch on some of the key terms and the current roadmap. As this course is self-paced, you can feel free to jump around between lessons. The earlier modules have more video and provide more of an overview of the relevant technology, while later modules get technical and have more hands on coding exercises and walk-throughs. Most modules have graded quizzes at the end of the module. Live Presentations Throughout the course, you'll see that some of the lessons are \"Live Presentations,\" these are demonstrations and walkthroughs of the concepts we're learning in the course. They will be hosted by wide cast of characters, including Academy staff, ConsenSys engineers, and developers working in the space. So without further ado, we wish you the best of luck, and let\u2019s get started!","title":"Welcome!"},{"location":"S00-intro/L1-course-intro/#welcome","text":"Hello and welcome to ConsenSys Academy\u2019s 2021 Blockchain Developer Bootcamp, where you will learn everything you need to know to become a world-class blockchain developer. The blockchain field develops at an incredibly fast pace. This course will give you a solid foundation of blockchain principles. It will also introduce you to the tools used to build blockchain projects today. It can be overwhelming to enter such a rapidly developing field and we hope to give you mental model to incorporate new information as it comes in. This cohort will launch on September 1st, 2021. You're in good company! Developers from all over the world will be your classmates, and this course is lead by some of the best in the business, and will feature presentations from builders, developers, and entrepreneurs in the space. We want to share the ConsenSys network with you.","title":"Welcome!"},{"location":"S00-intro/L1-course-intro/#course-content","text":"In this course, we start off by teaching you about the underpinnings of the blockchain and show how it all comes together to allow us to build the next generation of web applications. We will start with an overview of what we call blockchain primitives: The technology brought together underneath the name blockchain. We'll then dive deeper into the specifics of the Ethereum protocol and learn how those blockchain primitives show up there. Next, we'll move on to understanding writing smart contracts. We introduce you to Solidity, a programming language for Smart Contracts that lets you interact with the Ethereum Virtual Machine. We'll learn about Truffle so you can try out your new Solidity skills! We'll also discuss smart contract design patterns and security for your smart contract. After that, we'll introduce you to developer tooling, including Web 3 libraries, and other development frameworks. Next, we'll discuss the way in which people are taking blockchain development to the next level, with an introduction to Decentralized Finance (DeFi) and Decentralized Autonomous Organizations (DAOs). While this material may seem a bit overwhelming, it's important to have available to you as you continue to develop in the space. And we won't be testing you on it! Next, we'll touch on scalability: What it is, why it's necessary, and how some folks are proposing we approach it. We'll have a few tutorials allowing you to setup your own development environment for working on some of these scalability solutions. Last, we'll cover Ethereum 2.0, the upgrading of the Ethereum consensus mechanism from Proof of Work to Proof of Stake. We'll touch on some of the key terms and the current roadmap. As this course is self-paced, you can feel free to jump around between lessons. The earlier modules have more video and provide more of an overview of the relevant technology, while later modules get technical and have more hands on coding exercises and walk-throughs. Most modules have graded quizzes at the end of the module.","title":"Course content"},{"location":"S00-intro/L1-course-intro/#live-presentations","text":"Throughout the course, you'll see that some of the lessons are \"Live Presentations,\" these are demonstrations and walkthroughs of the concepts we're learning in the course. They will be hosted by wide cast of characters, including Academy staff, ConsenSys engineers, and developers working in the space. So without further ado, we wish you the best of luck, and let\u2019s get started!","title":"Live Presentations"},{"location":"S02-ethereum/M4-clients-workshop/L1/","text":"What is an Ethereum Client? Running Hyperledger Besu Hyperledger Besu is an Ethereum client with mainnet compatibility. It can also be used to run a private network. It is the open-source protocol layer that delivers all the perks from Ethereum and addresses enterprise requirements. Hyperledger Besu is actually part of a bundled suite of Ethereum protocol clients called ConsenSys Quorum, which consists of Hyperledger Besu and GoQuorum, private key managers, consensus mechanisms, transaction managers, APIs, plugins, libraries, deployment tools, etc. In this tutorial, we are going to walk you through how the client works, and take some detours to explain concepts in detail. Let\u2019s get started At the end of this guide, we will have walked you through deploying the Hyperledger Besu via the ConsenSys Quorum quickstart AND learned generally about private and public networks. As part of this process you will: Part 1: Download all necessary dependencies required of your system to run the ConsenSys Quorum Quickstart Part 2: Download and Start the ConsenSys Quorum Quickstart Part 3: Set up MetaMask and import a private key from the README Part 4: Deploy a decentralized application to the private network Part 5: Use the monitoring tools used in the quickstart Part 6: Deploy a second Smart Contract to the network and send a private transaction We will walk you through every step of the process, helping you get setup and providing helpful resources in case you get stuck. We will go into more detail than the documentation, where appropriate, to give you a deeper understanding of Hyperledger Besu We will be referring to the documentation found here throughout this article and linking to other useful documentation if you would like to do deeper into any particular topic. Part 1: Installing Dependencies This section will walk you through the installation of dependencies. This is written for new developers or those unfamiliar with the technology used by Hyperledger Besu. Feel free to move at a pace that meets your needs. MacOS users will start by installing NodeJS. Open up the terminal. If this is the first that you are hearing the term terminal or directory, check out this guide to understanding how the shell, terminal, and directories on a computer will work. This will be important to understanding how to navigate to the appropriate location on your computer. Type in node --version to see if node is already installed on your computer. If it is, you will get the version returned, for example: If node is not installed, you will see command not found: node . Click on this link to go to the Nodejs website and download the MacOS installer (the file name will end in .pkg). Open it up and follow the instructions. Once completed, restart your terminal. Then type node --version into the terminal again to confirm Node has properly installed. If you are having problems with installation of node, you can access the help from the Node community via the \u201chelp\u201d section of their GitHub, which includes links to their Slack channel. MacOS users will now move onto installing Docker and Docker Compose Open up the terminal and type in docker --version to see if Docker is already installed on your computer. If it is, you will get the version returned, for example: If Docker is not installed, you will see command not found: docker . Click on this link to go to Docker website and download Docker Desktop, which will include Docker and Docker Compose. Click the button for the appropriate chip within your Mac (The Apple chip was released in late 2020, if you have a Mac from before then it\u2019s most likely an Intel chip) Follow along with the rest of the instructions in the link provided. For a tutorial on Docker itself, you can find that here, along with another Getting Started Guide here. If you run into issues, access the troubleshooting guide for Docker. Open up Docker on your desktop, and you should see the following: Click on the Gear icon in the upper right corner. Navigate to resources and ensure under the \u201cAdvanced\u201d tab there is at least 6 GB of memory allocated to allow us to run all the examples in this Getting Started guide. Your settings should look like something below: Linux users will need first need to install Docker Engine and then can follow the steps contained within the Docker Compose documentation. After installing Docker, Linux users MUST follow the post-installation steps. Specifically, you need to add the user to the docker group, this step is often overlooked and every docker command will fail if this is not done. Part 2: Installing and Starting the Quorum Quickstart Open up the terminal and type in npx quorum-dev-quickstart and press enter ( npx , as opposed to the command npm , will cut down on pollution on your computer by not installing dependencies) The quickstart will download and we will get the following prompt. For the purposes of this guide, we will use the Hyperledger Besu option - which is 1. We are not trying out Codefi Orchestrate in this demo, so hit \u201cN for the prompt: Do you want to try out Codefi Orchestrate? Note: choosing yes will direct you to a login/registration page. [Y/n] N We do want to enable support for private transactions, so choose Y for the following prompt: Do you wish to enable support for private transactions? [Y/n] Y Do you wish to enable support for logging with ELK (Elasticsearch, Logstash & Kibana)? [y/N] Y This will look like: The default directory that will be created to store these files will be named quorum-test-network , but you can rename the network on this next line: Where should we create the config files for this network? Please choose either an empty directory, or a path to a new directory that does not yet exist. Default: ./quorum-test-network For example, if I wanted to name the directory besu-test-network , I would enter ./besu-test-network and this would create a new directory called besu-test-network . The name of the new directory has to be a directory that does not exist, or else we will get an error. The Quorum Quickstart will be installed on your computer in the directory named quorum-test-network . In the terminal, change directory to the quorum-test-network folder with the command cd quorum-test-network and the list out the sub-directories and files within this folder by typing the command ls to list the contents of quorum-test-network Type in the command less README.md to see the instructions for the quickstart. Hit q to exit out of this view. Alternatively, you can see the READ.ME within the Quorum Dev Quickstart by viewing it within the ConsenSys Quorum GitHub Repository. We are now going to start a private network that will run using Hyperledger Besu as the Ethereum client. This private network will run locally on our computer. Run the command ./run.sh in the terminal. The private network will now begin running. If you run into any issues with starting the Quickstart, use the Hyperledger Besu chat channel to reach out for help. How will we know if we have successfully launched the network? You should see the following (it may take a few minutes for this to load): Congratulations! You\u2019ve spun up a private Ethereum network on your local computer using Hyperledger Besu. Issues you may encounter, and how to resolve them: If you have use Docker and Docker compose previously, particularly with a previous version of this quickstart, it is likely that you could run into an error upon start up: ERROR: Pool overlaps with other one on this address space If you get this error, it means that even though the docker containers are down, the networks may still exist. A longer thread explaining this issue can be found here. A helpful resolution is to open up the command line and run the following command: docker network prune If that does not work, run the following combination of commands docker-compose down docker network prune This will restart the Docker service, and hopefully resolve any overlap conflicts. You can also run run docker ps to check for any overlapping registered containers. Once those are identified, use docker rm to remove the containers that are overlapping. A Brief Discussion of the Consensus Mechanism Let\u2019s quickly discuss the consensus mechanism you are using in your network. As we discussed earlier, the consensus mechanism is the coordinating process that the network uses to confirm, sync, and finalize new blocks on your blockchain. In this section, we are going to explore the two Proof of Authority (PoA) consensus mechanisms that you can use with Hyperledger Besu: Clique and IBFT 2.0. In a private network, we want to avoid using the consensus mechanism that is used on Ethereum mainnet - Ethash Proof of Work (PoW). Why? Ethash works well for Ethereum mainnet because anyone can participate in Ethereum mainnet by running a node and providing computation power to the network. Therefore, this computationally intense consensus mechanism ensures that any participant must compete with the other participants to produce and propagate blocks. The incentive of block rewards serves to ensure they are following the rules of the network. The more computing power that each node brings to the network, the greater benefits to the security of the network when PoW is the consensus mechanism. We are going to assume that you are not using the Ethash Proof of Work (PoW) consensus mechanism that is used for Ethereum mainnet because your network has participants that are: Known to each other, and Have a certain degree of inherent trust in each other. These two assumptions allow for your network to use a consensus mechanism which is more computationally efficient for the participants - they do not have to have the same computing power required of a node on Ethereum mainnet. These assumptions also allow for the removal of the incentive of block rewards to ensure compliance. Using a PoA consensus mechanism gives your network: Faster block creation as compared to a network running Ethash: Your network can create new blocks in a one or two seconds. As a comparison, Ethash on mainnet averages about 15 seconds for creation of a new block. Greater transaction throughput for the network: Without the need to incentivize the participants, the size of each block can be arbitrarily large, allowing for blocks with many transactions included as your network needs them to be included. Large transactions will not significantly or noticeably slow down the network (smaller blocks will be processed faster than larger blocks, the end user should not notice a difference). PoA assumes that the creators of the blocks - which are sometimes called minters, signers, sealers, or validators (we will refer to nodes on a PoA network as validators from here on) - are authorized to be validators. At the creation of the PoA network we declare a group (or \u201cpool\u201d) of validators, and as the network runs, those starting validators can add and/or subtract other validators. A short explanation of PoA To understand PoA, it helps to specifically discuss the consensus mechanism Clique PoA, as it was designed to approximate Ethereum mainnet for use in Ethereum testnets. The Ropsten testnet used PoW, but was plagued by attackers forcing reorgs and forks in the testnet and increasing the block gas limit and sending large transactions, which effectively stopped the network. A new consensus mechanism was designed by P\u00e9ter Szil\u00e1gyi called Clique, which could replace PoW on networks like testnets. Clique PoA was designed to use the existing Ethereum block data structure to add voting functionality. Generally in any PoA consensus mechanism, validators alternate adding blocks to the blockchain in a series. In addition to adding blocks, they can choose to vote to add or kick out a validator. This allows malicious validators to be removed from the network, and trusted validators to be added. This gives PoA resistance against certain kinds of attack vectors like: A malicious validator - if a machine is compromised, and a validator proposes incorrect blocks, they can be voted out by the other validators A censoring validator - if a compromised validator proposes votes to remove good actors, they can be removed by being voted out A spamming validator - if a validator makes a voting proposal in every block that the validator creates, the network is spammed by having to consistently tally the votes (votes being to add or remove a validator). By resetting voting after a set period of time (this period of time is referred to as an epoch and is set to be 30,000 blocks in our tutorials), this problem can be mitigated. Concurrent block creation by a validator can also be prevented by ensuring there is a substantial enough lag between each block being added to the blockchain. In short, many of the features of mainnet Ethereum can be approximated, but without needing to use PoW and the challenges that using PoW on a private or test blockchain would entail. Clique PoA Compared to IBFT 2.0 PoA Now that we know a bit more about PoA, we\u2019ll compare Clique PoA with IBFT 2.0 PoA in regards to: Finality - the amount of time required to guarantee a block has been added to the blockchain. Validators required - the minimum amount of validator nodes that have to participate in the network in order for the PoA consensus mechanism to work as intended. Liveness - the number of validator nodes that must be online or \u201clive\u201d to allow the network to continue running. This can also be thought of as a kind of fault tolerance - the number of validators that can fail at a given point in time and the network will still run. Relative Speed of Block Creation - how long it takes for each PoA consensus mechanism to create and sync new blocks and the factors that impact the speed of block creation. Finality Instantaneous finality is possible when only one block can be added at a point in time to the blockchain. If there is a possibility that multiple blocks are proposed at the same time, then a fork or reorganization can occur, and the validators are forced to make a choice about which block is a part of the final chain. In Clique PoA, a block will not be guaranteed to be added to the chain upon creation because there is a possibility of two blocks being proposed at the same time if a validator proposes a block out of turn. Blocks proposed simultaneously cause a fork in the network, as there are now two possible blocks for the next validator to create a block subsequent to (the second signer is prevented from signing subsequent blocks by the consensus mechanism). Because there are now two possible chains that could be built, the network must reorganize (or reorg), choosing which chain to build on. Due to this possibility, finality can be delayed when using Clique PoA if validators create blocks out of turn, and a block is not definitively confirmed upon creation. IBFT 2.0 has immediate finality, provided there are four validators. Once a block is created it becomes part of the blockchain. There are no forks. Validators Required Clique can run with a single validator (the use case for a single validator is purely demonstration purposes). As the number of validators in a Clique network increases, the probability of forks in the network increases. IBFT 2.0 requires four validators to be byzantine fault tolerant. Three or less validators running on an IBFT 2.0 network will no longer guarantee immediate finality or prevent network manipulation, particularly if one or more of the validators is acting in an adversarial nature. Liveness Clique can operate with half of the validators running (half of the validators can go down and the network will still work). IBFT 2.0 can operate as long as at least \u2154 of the validators are working. As noted in the validators required section, three or less validators running on an IBFT 2.0 network will no longer guarantee immediate finality, particularly if one or more of the validators is acting in an adversarial nature. Speed Clique is faster than IBFT 2.0 at adding blocks. IBFT 2.0 is slower at adding blocks. As more validators are added the time to add new blocks increases. Specific Examples Guitar Manufacturing A custom guitar manufacturer wants to create transparency for their customers to show the woods they are using in building custom guitars. They source woods locally and use reclaimed woods from the city they are based in. They want to write a transaction to a blockchain so that a customer can scan a QR code and see each transaction associated with the various woods used in the guitars. They work directly with two wood suppliers, and with an additional individual who helps them with salvage work to get reclaimed wood from old buildings. Looking at this case, we see the following: There is anywhere from a single to maybe four participants to start (the two wood suppliers and the reclaimed wood supplier) This is a project to provide transparency to the end-user, so transactions can be sent as needed, with addresses being created to represent specific guitars that are being built. The transactions will represent different woods and provide information on their sourcing If a single node were run, if it were to go offline, it would only impact the guitar builder. Because immediate finality is not required, there is the potential of only needing a single validator at the start, it would be possible to use a Clique PoA network. If the suppliers wanted to join the network, they could all do so, and if one of them left the network, there would not be an impact to running the blockchain. Casinos A group of six casinos decide to create a sportsbook together, in order to share in winnings and decrease the individual exposure of each casino. Each casino will run a validator node and transactions will be the bets that bettors place on basketball games. As the games being bet on have a specific start time, bets must be included in the blockchain to receive a payout. If bets are not included, the payout is not able to be made to the bettor. Looking at this use case, we see the following: There are at least six participants at start, and therefore we assume there will be six validators The bets that are made have to be included in a block. Immediate finality is required If a certain number of casinos go offline, we may have to question whether to take bets or not, since the other casinos become more exposed to the betting positions. There should be a high volume of transactions allowed, as the scenario of the number of placed bets is likely to increase the closer a basketball game approaches the start time. Because immediate finality is required, we are drawn to using an IBFT 2.0 PoA network. If we were to do this, then we would need at least 4 validators. Since there are six casinos participating, we can use an IBFT 2.0 PoA network. If more than two casinos were to leave then the network would no longer be able to run. In terms of liveness, the network can withstand a validator going offline (due to a power outage or other reason), and continue to operate, which is very important for the overall success of the sports book. Determining the PoA Consensus Mechanism to Use in Your Network Based on these examples, here are questions to ask yourself, which can help you determine which PoA consensus mechanism may make more sense. What are my finality requirements? Does my use case require that each block created instantly be confirmed? If a block is not instantly confirmed, what are the potential impacts? How many validators do I intend my network to have? How many will I have at the start? How many could be added to the network over time? Do I think validators could leave my network? Would I stop running the network if a certain number of validators left? Do I need participants to join the network without being validators? Should they be able to submit transactions? What fault tolerance does my network require? Does the number of validators that will participate at start allow for the network to continue running if one goes down? What happens if two or more validators go down? How fast do I need to be able to create new blocks? Will the speed at which I need to create new blocks stay constant or can it slow down if new validators are added? These questions can serve as a guide in thinking which PoA consensus mechanism makes sense for your use case. Part 3: Set up MetaMask Now that we have the network up and running, we are going to set up our MetaMask wallet. If you already have MetaMask installed, you can skip this step. If not, follow the links below. First, download the MetaMask browser extension. If you want to explore the features of MetaMask, this article can help you walk through how to send your first transaction using MetaMask. Follow the instructions to set up your MetaMask account. For the purposes of this guide, the accounts we will be using will not have mainnet Ether. Still get in the habit of the following: - Copy down your seed phrase and store it in a secure location. - Do not share your seed phrase with anyone - Ensure that you can use your seed phrase to recover your account MetaMask is a self-custodial wallet, so if you lose the seed phrase associated with it, you have lost access to that Ethereum account. If you give that seed phrase to someone, they now have control over that account. Open up your MetaMask account and navigate to to the top of the browser extension click on the Ethereum Mainnet menu. From the drop down select Localhost 8545. This is where the blockchain that we spun up with the quickstart is running. If you want to learn more about connecting to different networks, this documentation from MetaMask will give you more insight on how it works. MetaMask will allow us to send transactions between Ethereum accounts on our network, which we will call Externally Owned Accounts (EOA). When we ran the quickstart, the private blockchain network that we created was initialized with three accounts pre-loaded with test Ether. Looking into the quorum-test-network folder, you will see a structure within the config -> besu folder that shows the three member folders. In each folder is a keys folder, which has a file called key, which is the private key and key.pub, which is the public key. We will open the file named \u201ckey\u201d in member1 to see what is inside Here we see the private key, without the 0x prefix. The private key for this address would be 0x8f2a55949038a9610f50fb23b5883af3b4ecb3c3bb792cbcefbd1542c692be63 To import this address into our MetaMask, we will click the circle in the upper right corner of the MetaMask extension. This circle is called a favicon by the MetaMask team: We will hit the Import Account button: And will copy paste the private key, repeated below: 0x8f2a55949038a9610f50fb23b5883af3b4ecb3c3bb792cbcefbd1542c692be63 Into the following field This will create our account, which is pre-loaded with 200 test Ether: Below you can find information about the pre-loaded accounts:, in the event you want to import them: Test Account 1 (address 0xfe3b557e8fb62b89f4916b721be55ceb828dbd73) Private key to copy : 0x8f2a55949038a9610f50fb23b5883af3b4ecb3c3bb792cbcefbd1542c692be63 Initial balance : 200 Eth (200000000000000000000 Wei) Test Account 2 (address 0x627306090abaB3A6e1400e9345bC60c78a8BEf57) Private key to copy : 0xc87509a1c067bbde78beb793e6fa76530b6382a4c0241e5e4a9ec0a0f44dc0d3 Initial balance : 90000 Eth (90000000000000000000000 Wei) Test Account 3 (address 0xf17f52151EbEF6C7334FAD080c5704D77216b732) Private key to copy : 0xae6ae8e5ccbfb04590405997ee2d52d2b330726137b875053c36d94e974d162f Initial balance : 90000 Eth (90000000000000000000000 Wei) If we import these accounts and send 7 ETH from Test Account 1 to Test Account 2: we can see within the block explorer (accessible in our browser at http://localhost:25000/), Account 1 will decrement by 7 ETH: And that Account 2 will increase by just under 7 ETH Why didn\u2019t Account 2 increase by 7 ETH? In this specific example, we have paid a transaction cost to send the Ether from one account to another. The transaction cost is called the gas, and is sometimes also called the gas fee. It is paid in increments of Ether called Wei, which is a fractional amount of ETH (1 Wei is 10^-18 ETH). There is a 21000 wei cost we pay to send Ether, and that is deducted from the transaction, which was the gas limit (the upper amount of gas we are willing to pay) stipulated for this transaction on MetaMask. The private network we have created is a gas-less network, so there is no need to pay that fee, and we could reduce the gas limit to 0 and the transaction would go through. Understanding gas is critical for understanding how mainnet Ethereum works, as well as how it may be useful in other private networks we create, but for now, you can read a brief overview about gas here to better understand this concept. How were these accounts created? When we ran ./run.sh in our terminal, this script told docker-compose to run a .yml file, which contained information about the accounts, keys, and initialization information for our network in a genesis file. This genesis file - which is a .json file, provided our blockchain with information about the accounts, including the address and the amount of Ether they held. To see a genesis file in the quickstart, you can open and look at cliqueGenesis.json, which is in the quorum-test-network -> config -> besu directory. It looks like the following: { \"config\":{ \"chainId\":1337, \"constantinoplefixblock\": 0, \"clique\":{ \"blockperiodseconds\":15, \"epochlength\":30000 } }, \"coinbase\":\"0x0000000000000000000000000000000000000000\", \"difficulty\":\"0x1\", \"extraData\":\"0x00000000000000000000000000000000000000000000000000000000000000004592c8e45706cc08b8f44b11e43cba0cfc5892cb0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", \"gasLimit\":\"0xa00000\", \"mixHash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\", \"nonce\":\"0x0\", \"timestamp\":\"0x5c51a607\", \"alloc\": { \"fe3b557e8fb62b89f4916b721be55ceb828dbd73\": { \"privateKey\": \"8f2a55949038a9610f50fb23b5883af3b4ecb3c3bb792cbcefbd1542c692be63\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"0xad78ebc5ac6200000\" }, \"627306090abaB3A6e1400e9345bC60c78a8BEf57\": { \"privateKey\": \"c87509a1c067bbde78beb793e6fa76530b6382a4c0241e5e4a9ec0a0f44dc0d3\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" }, \"f17f52151EbEF6C7334FAD080c5704D77216b732\": { \"privateKey\": \"ae6ae8e5ccbfb04590405997ee2d52d2b330726137b875053c36d94e974d162f\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" } }, \"number\":\"0x0\", \"gasUsed\":\"0x0\", \"parentHash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\" } Within this file, I have highlighted the accounts that were created and the balances they were created with (the balances are in wei, not Ether. Remember 1 wei is 10 x -18 Ether). The quickstart and the scripts we run take care of creating these accounts, but we are able to create our own network where we can initialize accounts and configure our network to be customized to our particular use case. Understanding a Genesis File: The Basics Setting up a private network or joining a public network requires an Ethereum node to create a new blockchain. Whether it is a private or public network, each node / validator will have a full copy of the blockchain. In order to build this copy, the node / validator has to have instructions on how to build the first block and the subsequent blocks in the chain. In this article, we are going to walk through the components of a genesis file, within the context of the Ethereum client Hyperledger Besu as the client for our network. These concepts are applicable to all Ethereum clients. The first block in a blockchain is called the genesis block. Ethereum mainnet\u2019s genesis block - block 0 - was mined on July 30, 2015. In order to join or create any network, the data for the genesis block must be included. Therefore, the genesis file defines the data that is in the first block of a blockchain, as well as rules for the blockchain itself. If a new node or validator attempts to join the blockchain, it will use the genesis file as the starting point in recreating the history of the chain in order to synchronize with the existing network. The genesis file for Ethereum mainnet, along with the supported testnets, is included in the download of Besu. When creating a private network, a custom genesis file must be provided. The genesis file is a JSON formatted file. It looks like the below: { \"config\": { \"chainId\": 2018, \"muirglacierblock\": 0, \"ibft2\": { \"blockperiodseconds\": 2, \"epochlength\": 30000, \"requesttimeoutseconds\": 4 } }, \"nonce\": \"0x0\", \"timestamp\": \"0x58ee40ba\", \"extraData\": \"0xf83ea00000000000000000000000000000000000000000000000000000000000000000d5949811ebc35d7b06b3fa8dc5809a1f9c52751e1deb808400000000c0\", \"gasLimit\": \"0x1fffffffffffff\", \"difficulty\": \"0x1\", \"mixHash\": \"0x63746963616c2062797a616e74696e65206661756c7420746f6c6572616e6365\", \"coinbase\": \"0x0000000000000000000000000000000000000000\", \"alloc\": { \"9811ebc35d7b06b3fa8dc5809a1f9c52751e1deb\": { \"balance\": \"0xad78ebc5ac6200000\" } } } In this specific example, the genesis file is for an IBFT 2.0 private network. { \"config\": { \"chainId\": 2018, \"muirglacierblock\": 0, \"ibft2\": { \"blockperiodseconds\": 2, \"epochlength\": 30000, \"requesttimeoutseconds\": 4 } The config key section contains the following information about the blockchain \u201cchainId\u201d: 2018 The chainId controls the transaction signature process, providing a unique identifier to allow for the hashing of signed transactions to only work on the network associated with the corresponding chainId. Ethereum Improvement Proposal 155 (EIP-155) provides more information on the relationale behind the chainID. Most chainIDs match the networkID of the network. In this case, 2018 refers to the chainID associated with a development chain. For a list of the network and chain IDs, please see the Documentation. \"muirglacierblock\": 0, This field is called a \u201cmilestone block\u201d. Muir glacier refers to a specific network upgrade that occurred at block 9,200,000 on Ethereum mainnet. For private networks, like the one that is being created in this example, the name of the latest milestone block can be listed, and set to be the genesis block. Here you can see a continuously updated list of network upgrades and the associated blocks for Ethereum. \u201cibft2\u201d: This specifies that the consensus protocol for the blockchain is IBFT 2.0. The options available for specifying the consensus mechanism are available in the documentation, with an overview of the proof-of-authority (PoA) consensus protocols available here. Within the specification, the following three fields are provided: \"blockperiodseconds\": 2, The minimum block time, in seconds. In this case, after two seconds, a new block will be proposed by the network with transactions stored in the memory pool packaged and distributed to the network. \"epochlength\": 30000, The number of blocks at which to reset all votes. The votes refer to validators voting to add or remove validators to the network. In this case, after 30,000 blocks are created, this IBFT 2.0 network will discard all pending votes collected from received blocks. Existing proposals remain in effect and validators re-add their vote the next time they create a block. \"requesttimeoutseconds\": 4 The time by which a new block must be proposed or else a new validator will be assigned by the network. If a validator goes down, the request time out ensures that the proposal of a new block passes on to another validator. The request time out seconds should be set to be double the minimum block time (blockperiodseconds), hence why it is 4. The second section, starting with \"nonce\": \"0x0\", contains information about the genesis block \"nonce\": \"0x0\", \"timestamp\": \"0x58ee40ba\", \"extraData\": \"0xf83ea00000000000000000000000000000000000000000000000000000000000000000d5949811ebc35d7b06b3fa8dc5809a1f9c52751e1deb808400000000c0\", \"gasLimit\": \"0x1fffffffffffff\", \"difficulty\": \"0x1\", \"mixHash\": \"0x63746963616c2062797a616e74696e65206661756c7420746f6c6572616e6365\", \"coinbase\": \"0x0000000000000000000000000000000000000000\", \"alloc\": { \"9811ebc35d7b06b3fa8dc5809a1f9c52751e1deb\": { \"balance\": \"0xad78ebc5ac6200000\" } } } \"nonce\": \"0x0\", The number used once aka nonce, that is a part of the blockheader for the first block. Set to 0x0. \"timestamp\": \"0x58ee40ba\", The creation date and time of the block. Often it can be set to 0x0, but as long as it is any value in the past, it will work. In this case 0x58ee40ba is a hexadecimal which converts to 1492009146 and represents Wed Apr 12 2017 14:59:06 GMT+0000 \"extraData\": \"0xf83ea00000000000000000000000000000000000000000000000000000000000000000d5949811ebc35d7b06b3fa8dc5809a1f9c52751e1deb808400000000c0\", Extra data is a recursive length prefix (RLP) encoded string (which is space efficient) containing the validator addresses of the IBFT 2.0 private network. The validator addresses are unique to the validators, so if there are four validators are the start of the network, this field should contain a list of their addresses.. Instructions on how to create an RLP using Besu can be found here. \"gasLimit\": \"0x1fffffffffffff\", The block gas limit, which is the total gas limit for all transactions included in a block. It defines how large the block size can be for the block, and is represented by an hexadecimal string. For this network, the gas limit is the maximum size, and is therefore a \u201cfree gas network\u201d \"difficulty\": \"0x1\", The difficulty of creating a new block. Represented as a hexadecimal string, the difficulty is set to 1, effectively the lowest difficulty. \"mixHash\": \"0x63746963616c2062797a616e74696e65206661756c7420746f6c6572616e6365\", The mixHash is the unique identifier for the block, which for this genesis file is 0x63746963616c2062797a616e74696e65206661756c7420746f6c6572616e6365 \"coinbase\": \"0x0000000000000000000000000000000000000000\", The network coinbase account, which is where all block rewards for this network will be paid. In this case it is to 0x0000000000000000000000000000000000000000, which is sometimes called address(0) or the zero address. \"alloc\": { \"9811ebc35d7b06b3fa8dc5809a1f9c52751e1deb\": { \"balance\": \"0xad78ebc5ac6200000\" The alloc field creates an address on our network, which is sometimes also referred to as an externally owned account, as it is an account not associated with a smart contract (referred to as a contract account). The number starting with \u201c98\u201d is the public key of the address. The balance can be passed in as a decimal OR a hexadecimal (like it has in this case and corresponds to 200 ETH, or 2*10^20 Wei). The balance is always in Wei, or 10^-18 Ether. A Second Genesis File Below we provide another genesis file with a different consensus mechanism, Clique PoA, and different information. Take a look below and see what values stick out. This is the genesis file for the chain that you can build in the ConsenSys Quorum quickstart: { \"config\":{ \"chainId\":1337, \"muirglacierblock\": 0, \"clique\":{ \"blockperiodseconds\":15, \"epochlength\":30000 } }, \"coinbase\":\"0x0000000000000000000000000000000000000000\", \"difficulty\":\"0x1\", \"extraData\":\"0x00000000000000000000000000000000000000000000000000000000000000004592c8e45706cc08b8f44b11e43cba0cfc5892cb0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", \"gasLimit\":\"0xa00000\", \"mixHash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\", \"nonce\":\"0x0\", \"timestamp\":\"0x5c51a607\", \"alloc\": { \"fe3b557e8fb62b89f4916b721be55ceb828dbd73\": { \"privateKey\": \"8f2a55949038a9610f50fb23b5883af3b4ecb3c3bb792cbcefbd1542c692be63\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"0xad78ebc5ac6200000\" }, \"627306090abaB3A6e1400e9345bC60c78a8BEf57\": { \"privateKey\": \"c87509a1c067bbde78beb793e6fa76530b6382a4c0241e5e4a9ec0a0f44dc0d3\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" }, \"f17f52151EbEF6C7334FAD080c5704D77216b732\": { \"privateKey\": \"ae6ae8e5ccbfb04590405997ee2d52d2b330726137b875053c36d94e974d162f\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" } }, \"number\":\"0x0\", \"gasUsed\":\"0x0\", \"parentHash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\" } Once again, we will look at the fields and explain them. { \"config\":{ \"chainId\":1337, \"constantinoplefixblock\": 0, \"clique\":{ \"blockperiodseconds\":15, \"epochlength\":30000 } } The config key section contains the following information about the blockchain \u201cchainId\u201d: 1337 In this case. 1337 refers to a local chainID. MetaMask, a self-custodial wallet, and Ganache, which is Truffle\u2019s Private Blockchain App, both use this as the local chain ID, and so to follow convention, in this genesis file we are doing the same. For a list of the network and chain IDs, please see the Documentation. \"muirglacierblock\": 0, Once again, we have set the milestone block to Muir Glacier. Something to note - The \u201cmilestone block\u201d could be for this configuration file, which is something you may see in some tutorials. For example, if we saw constantinopleBlock\": 0, this refers to a specific network upgrade that occurred at block 7,280,000 on Ethereum mainnet. For private networks, like the one that is being created in this example, the name of the latest milestone block can be listed, and set to be the genesis block. Here you can see a continuously updated list of network upgrades and the associated blocks for Ethereum. \u201cclique\u201d: This specifies that the consensus protocol for the blockchain is Clique. The options available for specifying the consensus mechanism are available in the documentation, with an overview of the proof-of-authority (PoA) consensus protocols available here. Within the specification, the following two fields are provided: \"blockperiodseconds\": 15, The minimum block time, in seconds. In this case, after 15 seconds, a new block will be proposed by the network. Given this genesis file is modeled after the testnet, it is made to approximate the minimum blocktime of mainnet, which is 15 seconds. \"epochlength\": 30000, The number of blocks at which to reset all votes. The votes refer to validators voting to add or remove validators to the network. In this case, after 30,000 blocks are created, this Clique network will discard all pending votes collected from received blocks. Existing proposals remain in effect and validators re-add their vote the next time they create a block. Starting at the coinbase we now have the information available in the genesis block \"coinbase\":\"0x0000000000000000000000000000000000000000\", \"difficulty\":\"0x1\", \"extraData\":\"0x00000000000000000000000000000000000000000000000000000000000000004592c8e45706cc08b8f44b11e43cba0cfc5892cb0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", \"gasLimit\":\"0xa00000\", \"mixHash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\", \"nonce\":\"0x0\", \"timestamp\":\"0x5c51a607\", \"alloc\": { \"fe3b557e8fb62b89f4916b721be55ceb828dbd73\": { \"privateKey\": \"8f2a55949038a9610f50fb23b5883af3b4ecb3c3bb792cbcefbd1542c692be63\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"0xad78ebc5ac6200000\" }, \"627306090abaB3A6e1400e9345bC60c78a8BEf57\": { \"privateKey\": \"c87509a1c067bbde78beb793e6fa76530b6382a4c0241e5e4a9ec0a0f44dc0d3\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" }, \"f17f52151EbEF6C7334FAD080c5704D77216b732\": { \"privateKey\": \"ae6ae8e5ccbfb04590405997ee2d52d2b330726137b875053c36d94e974d162f\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" } } \"coinbase\": \"0x0000000000000000000000000000000000000000\", The coinbase account, which is where all block rewards for this network will be paid. In this case it is to 0x0000000000000000000000000000000000000000, which is sometimes called address(0) or the zero address. \"difficulty\": \"0x1\", The difficulty of creating a new block. Represented as a hexadecimal string, the difficulty is set to 1, effectively the lowest difficulty. \"extraData\":\"0x00000000000000000000000000000000000000000000000000000000000000004592c8e45706cc08b8f44b11e43cba0cfc5892cb0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", Extra data is a recursive length prefix (RLP) encoded string (which is space efficient) containing the validator address of the Clique private network, which in this case is 4592c8e45706cc08b8f44b11e43cba0cfc5892cb. Instructions on how to create an RLP using Besu can be found here and how to add additional addresses for a Clique network with additional signers can be found here. \"gasLimit\": \"0xa00000\", The block gas limit, which is the total gas limit for all transactions included in a block. It defines how large the block size can be for the block, and is represented by an hexadecimal string. For this network, the gas limit is \"mixHash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\", The mixHash is the unique identifier for the block, which for this genesis file is 0x0000000000000000000000000000000000000000000000000000000000000000 \"nonce\": \"0x0\", \"timestamp\": \"0x5c51a607\", In this case 0x5c51a607 is a hexadecimal which converts to 1548854791 and represents Wed Jan 30 2019 13:26:31 GMT+0000. \"alloc\": { \"fe3b557e8fb62b89f4916b721be55ceb828dbd73\": { \"privateKey\": \"8f2a55949038a9610f50fb23b5883af3b4ecb3c3bb792cbcefbd1542c692be63\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"0xad78ebc5ac6200000\" }, \"627306090abaB3A6e1400e9345bC60c78a8BEf57\": { \"privateKey\": \"c87509a1c067bbde78beb793e6fa76530b6382a4c0241e5e4a9ec0a0f44dc0d3\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" }, \"f17f52151EbEF6C7334FAD080c5704D77216b732\": { \"privateKey\": \"ae6ae8e5ccbfb04590405997ee2d52d2b330726137b875053c36d94e974d162f\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" } The alloc field creates three addresses on our network, The balance can be passed in as a decimal (like the second and third account, which are both 900 ETH, or 2 10^20 Wei) OR a hexadecimal (like the first account, which is 200 ETH, or 2 10^20 Wei). The balance is always in Wei, or 10^-18 Ether. Deploying Your Genesis File Once you have created your genesis file, you will save it within the directory where your blockchain networks files will be kept. Do not save it within any of the Node or data folders, but rather at the top level. When it is time to start your network, you will use the flag --genesis-file=../genesis.json To start up your network using the genesis file use the following command: besu --genesis-file=../genesis.json For more information, please see the Documentation. Part 4: Deploy the Pet Shop Decentralized Application within the dapps folder of the quorum-test-network directory We are now going to deploy a decentralized application (dapp) to our private network. From your terminal, change directory to the folder dapps, and then into pet-shop cd dapps/pet-shop/ Within this directory, you will find a file called custom_config and a script called run_dapp.sh run_dapp.sh looks like the following: !/bin/bash set -e hash truffle 2>/dev/null || { echo >&2 \"This script requires truffle but it's not installed.\" echo >&2 \"Refer to documentation to fulfill requirements.\" exit 1 } rm -rf pet-shop-box git clone https://github.com/truffle-box/pet-shop-box.git cp -r custom_config/* ./pet-shop-box/ cd pet-shop-box/ npm install npm install truffle npm install @truffle/hdwallet-provider truffle compile truffle migrate --network quickstartWallet truffle test --network quickstartWallet docker build . -t quorum-dev-quickstart_pet_shop docker run -p 3001:3001 --name quorum-dev-quickstart_pet_shop --detach quorum-dev-quickstart_pet_shop This script is going to download from GitHub a Truffle box, which are pre-built dapps or templates for building your own dapp. In this case, the pet-shop dapp contains a smart contract called Adoption.sol, a front-end for this dapp written in react, which is a javascript library specifically for building front-ends and user interfaces, and the necessary dependencies needed to deploy the smart contract to an Ethereum network. The script is going to install truffle which is required in order to run commands using truffle, like truffle migrate. Truffle migrate is a command which specifically compiles our smart contract (Adoption.sol) into EVM bytecode and deploys it to the private network we have created. The script also downloads hdwallet-provider, which is used to create Ethereum accounts that we can use to interact with our decentralized application, and to send transactions on the private network we have created. The Ethereum accounts we have created will be imported into MetaMask. If you want to familiarize yourself with Truffle, you can walk through a tutorial here. For our current tutorial, we are using the private network created by the quorum quickstart as our local test blockchain network. Within the folder custom_config, we have our Smart Contract - Adoption.sol Adoption.sol is a simple smart contract written in solidity. It looks like this: pragma solidity ^0.5.0; contract Adoption { address[16] public adopters; // Adopting a pet function adopt(uint petId) public returns (uint) { require(petId >= 0 && petId <= 15); adopters[petId] = msg.sender; return petId; } // Retrieving the adopters function getAdopters() public view returns (address[16] memory) { return adopters; } } A detailed description of this smart contract, along with a tutorial to build it can be found on the Truffle website. We are going to give you a quick overview of the contract in order to contextualize what happens once it is deployed to our private blockchain. Let us take a look at the smart contract to better understand what it does: pragma solidity ^0.5.0; This first line of code tells us the smart contract is written for Solidity version 0.5.0 or higher, and will be compatible with 0.5.0 and higher. contract Adoption { address[16] public adopters; These two lines of code do the following: contract Adoption { declares an object of type contract. If you want to learn more about this object type, the Solidity documentation has a nice explanation of a Contract object. In this example, anything after the { (the brackets) will define the contract object - what it can do and what it can store on the blockchain. address[16] public adopters; instantiates 16 addresses in an array which we have given the variable name \u201cadopters\u201d. These addresses are public, which means they can be accessed from outside of this contract as well as inside the contract. // Adopting a pet function adopt(uint petId) public returns (uint) { require(petId >= 0 && petId <= 15); adopters[petId] = msg.sender; return petId; } The first line is a comment, which will not be compiled but gives us information that the following function is for adopting a pet. The function, adopt, takes in a unsigned integer called petID, and checks if it is between 0 and 15. If it is, then the address at the location of the the petID within the adopters array is assigned to the account that called the adopt function (adopters[petID] = msg.sender is the specific code). Then the petID is returned. The second function: // Retrieving the adopters function getAdopters() public view returns (address[16] memory) { return adopters; } returns the array of addresses of the adopters. Now that we understand the smart contract,we run the command ./run_dapp.sh in the terminal within the pet-shop folder (which we should have already navigated into if you have been following along with each step). This is going to download all the dependencies, as well as compile and migrate our smart contract to the private network we have created using the Quorum quickstart, and spin up a front end that will allow us to \u201cadopt\u201d some group of 16 pets. When you run it, there will be many downloads. You know that the script has successfully run when you reach the following: Here we can see that Truffle has deployed the Migrationsmart contract and has now deployed it to the Hyperledger Besu based blockchain network we have spun up. If we take the contract address: 0x345cA3e014Aaf5dcA488057592ee47305D9B3e10 that is listed here and navigate in a web browser to the block explorer (go to your browser and type in: http://localhost:25000/, then paste in the contract address: 0x345cA3e014Aaf5dcA488057592ee47305D9B3e10 We will see the following information about our deployed smart contract: We can also use the block explorer to look up other information, like the externally owned account (in the >account field, which for our specific example is: 0x627306090abaB3A6e1400e9345bC60c78a8BEf57) We are able to see the ETH balance for that account. The ETH used in this example is not mainnet ETH, and it is only used for testing purposes. From this step, we hope that you can see how you can use the quickstart to deploy a decentralized application. Thus, the quickstart provides you with a template of being able to test a decentralized application of your own that you build. Let\u2019s look at the dapp by going to our browser and typing in http://localhost:3001. We will arrive at the following screen, and MetaMask, which we installed in Part 3, will pop up for us and ask which account we want to connect to the application. Hitting adopt will prompt MetaMask to pop-up and adopt the pet: Since this is a gasless network, there is no fee associated with adopting a pet. Issues you may encounter and how to solve them When you run /.run_dapp.sh you may get the following error: The Error: Deployment Failed \u201cMigrations\u201d --Wrong chainId means that within the deployment.js file, the chainID (which chain you are deploying to) was incorrectly specified. If that is the case, you should run the npx quorum-dev-quickstart again, as this means you are on an older version of the quickstart that does not have the most up to date versions of truffle and hd-wallet provider. Specifically, you need to be on 1.4.0 and above of hd-wallet provider. Versions 1.3.0 and 1.3.1 will not work work with the quickstart (though users will see that version 1.2.6 will still work with the quickstart). Make sure you are on version 0.0.21 of the quickstart or higher to avoid this issue. If there is no UI that appears at localhost:3001 after running the script ./run_dapp.sh, navigate to the pet-shop-box folder: cd pet-shop-box amd use the command npm run dev to start up the front end in our browser. If this runs successfully, we will get the following output in the terminal: Then you can navigate to the URLs listed in the output to see your application. Part 5: Use the monitoring tools used in the quickstart There are monitoring tools included in the quickstart, which allow us to learn about how our private blockchain network is working. In part 4, we use the block explorer to look at the contract address of the deployed smart contract and the externally owned account that was responsible for deploying the smart contract to our network. Now we are going to explore two additional tools we can use to monitor our private network. The first is Prometheus - an open source monitoring tool that pulls data from the services that it is connected to - like nodes, databases, validators, servers, etc - and allows for that data from these services to be use in alerts and notifications. For example, if the memory usage of a certain node on your network exceeds a defined threshold, Prometheus is able to track this and alert you so you can take the appropriate action to ensure your network continues to run properly. In the case of a failure, Prometheus provides the data from the various services to allow for troubleshooting the issue. Prometheus pulls the data and stores it in a database, and then allows you or a data visualization tool to query that data. When you install and run the quickstart, you will get access to Prometheus at http://localhost:9090/graph. If you enter this address into a web browser, you will see the following: What exactly is Prometheus monitoring? To see the details, you can navigate to the config folder within your quorum-test-network directory. Opening the prometheus folder will reveal a prometheus.yml file. If you open that with a text editor (like Visual Studio Code or Vim) you will see that Prometheus is configured by the quickstart to scrape data from each validator, the rpcnode, and each member of the network every 15 seconds, and to store that data in the /metrics pathway, so that you can query it. Below is a screenshot of the first 28 of the total 129 lines of the prometheus.yml file The metrics we have access to can be seen by navigating and clicking on the curricular icon next to the Execute button in the right upper side of the screen. This will reveal the list of metrics that you can query Promtheus to return data for: If we choose ethereum_peer_count, it will return the number of peers on the network, which for this example is seven If we type in the following query into the search bar (the part of the user interface with \u201cExpression (press Shift+Enter for newlines) besu_blockchain_difficulty_total We will get back the following a result of our query that will look similar to the the below screenshot: This query is asking Prometheus to return data on the total difficulty for each participant node in our network. Prometheus has an integration with Granfana - an observability platform that takes in data - in this case from our private blockchain via Prometheus - and displays the outputs as visualizations. This allows us to make sense of what is occurring in our private network over time. The quickstart manages the connection of the data from Prometheus to pre-built dashboards in Grafana. If you navigate to http://localhost:3000/d/XE4V0WGZz/besu-overview?orgId=1&refresh=10s&from=now-30m&to=now&var-system=All, the metrics from Prometheus will be displayed in a Grafana dashboard, providing us with the metrics in tabular and graphical format. This format makes exploring the data retrieved from Prometheus easier, and allows us to explore many metrics simultaneously. Logs are available via Elasticsearch, Logstash, and Kibana (ELK). ELK allows us to take in process, search, transform, and display data. In the Quickstart, this is configured specifically to ingest the Besu logs. As a reminder, navigating to the terminal and running the script ./list.sh This script will show the various endpoints and services available in the quickstart Quorum Dev Quickstart Setting up the index patterns in kibana ... List endpoints and services JSON-RPC HTTP service endpoint : http://localhost:8545 JSON-RPC WebSocket service endpoint : ws://localhost:8546 Web block explorer address : http://localhost:25000/ Prometheus address : http://localhost:9090/graph Grafana address : http://localhost:3000/d/XE4V0WGZz/besu-overview?orgId=1&refresh=10s&from=now-30m&to=now&var-system=All Collated logs using Kibana endpoint : http://localhost:5601/app/kibana#/discover For more information on the endpoints and services, refer to README.md in the installation directory. Navigating to the Collated logs using Kibana endpoint using your browser http://localhost:5601/app/kibana#/discover This will pull up a Kibana dashboard The logs are pulled automatically by the setup of the Quickstart. For more information about configuring Kibana on your own, you can see an overview within the documentation. Part 6: Deploy a second Smart Contract to the network and send a private transaction Now we are going to take another smart contract and deploy it to our network. This will be a simple smart contract, and the goal of deploying it is to demonstrate the use of private transactions in the network.. This smart contract is already installed as part of the quickstart, and you can find it in the folder smart_contracts. Navigate to the smart_contracts folder using the command cd smart_contracts and you can look at that smart contract EventEmitter.sol: pragma solidity ^0.7.0; // compile with: // solc EventEmitter.sol --bin --abi --optimize --overwrite -o . // then create web3j wrappers with: // web3j solidity generate -b ./generated/EventEmitter.bin -a ./generated/EventEmitter.abi -o ../../../../../ -p org.hyperledger.besu.tests.web3j.generated contract EventEmitter { address owner; event stored(address _to, uint _amount); address _sender; uint _value; constructor () public { owner = msg . sender ; } function store ( uint _amount ) public { emit stored ( msg . sender , _amount ); _value = _amount ; _sender = msg . sender ; } function value () view public returns ( uint ) { return _value ; } function sender () view public returns ( address ) { return _sender ; } } Run the commands npm install node scripts/deploy.js which will deploy the EventEmitter.sol smart contract that is in the contracts folder and send a private transaction from Member1 to Member3 of the network. The private transaction, in this example, is the sending of a value from Member1 to Member3. If we look inside the deploy.js script, we will see that the value being sent from Member1 to Member3 is 47 (see line 89 within the deploy.js file) Alt Text: Private Transaction example output Looking at this output in the terminal, the following has occurred. The smart contract EventEmitter.sol was migrated and deployed to our network. This is confirmed by the following outputs: Getting contractAddress from txHash: 0x022cc35254299433dbda514a979ed7a9a93a45ead383efec4ad5a84e8a817f3e Waiting for transaction to be mined ... Private Transaction Receipt: [object Object] Contract deployed at address: 0xdb6172b4ed41f7039cac4d0be4dbb9992c755809 Once the EventEmitter.sol contract was deployed, Member1 sent a private transaction to Member3. Member1 and Member3 can see the value within the transaction, which has been hashed, as Member1 value from deployed contract is: 0x000000000000000000000000000000000000000000000000000000000000002f Member3 value from deployed contract is: 0x000000000000000000000000000000000000000000000000000000000000002f Member2, which was not included in the private transaction, is unable to see that value, and gets a returned value of 0x. Member2 value from deployed contract is: 0x What has been demonstrated here is that two Members can send transactions between each other, and other members cannot observe the contents of the transaction. For more information, please see the Documentation. Congratulations! You have successfully run through many of the features of Hyperledger Besu. Other Ways to Start Hyperledger Besu Hyperledger Besu starts and is controlled through the command line interface (CLI) of your computer, as seen in this tutorial. If we did not use the quickstart, we would download Hyperledger Besu and run it via the command line. For example, when a Hyperledger Besu node is started up to connect to mainnet, this is done by running the command: besu In the command line. This command is actually calling a bunch of default options and subcommands that tell the client software how to set up the node. When we call the above command, behind the scenes we are actually calling: besu --network=mainnet --datapath=besu --api-gas-price-blocks=100 --api-gas-price-max=500000000000 --api-gas-price-percentile=50.0 --bootnodes=enode://c35c3...d615f@1.2.3.4:30303,enode://f42c13...fc456@1.2.3.5:30303 --config-file=none --discovery-enabled=true --miner-coinbase= --min-gas-price=1000 This is quite a bit longer than running the besu command! What we are trying to illustrate is the fact that besu is very customizable, but we have to use specific syntax (options and subcommands) in the command line in order to customize the node(s) and network created upon starting up Besu. The above is an example of the options and subcommands that apply to creating a full node running proof of work (PoW) consensus for mainnet Ethereum. However, these options and subcommands can also be used to create private and consortium networks. The Hyperledger Besu documentation has all the options and subcommands that you can apply when using Hyperledger Besu. In this article, we are going to explain options and subcommands, and introduce what certain combinations of options and subcommands allow us to do. What is an option? What is a subcommand? When using the CLI, we call commands. These commands are programs (they may also sometimes be classified as scripts) that tell the computer to do a specific set of actions. When Hyperledger Besu is installed on your computer, it creates a global command besu - which tells your computer to run the Besu client with the default options and subcommands we showed above. Let\u2019s walk through some of those in more detail. Options: --network Options are variables that work with the base command of besu. One example option is --network= When we run the besu command, the default network is mainnet, so running besu or besu --network=mainnet accomplishes the same outcome - Hyperledger Besu starts the process of connecting to mainnet. But, we can tell Hyperledger Besu to connect to different networks, including a local testnet on our computer. If we wanted Hyperledger Besu to connect to the testnet Rinkeby, we would use the option: besu --network=rinkeby If we want Besu to start locally on our computer as a private PoW network, the network option becomes besu --network=dev You are now starting to see how an option modifies the command besu. Subcommands: blocks Subcommands are programs that tell the computer to run an additional operation. An example of a subcommand is besu blocks export [--start-block= ] [--end-block= ] --to= This highlighted subcommand tells Hyperledger Besu to export a block or a range of blocks from storage to a file in an RLP format. In practice this would be called like: besu blocks export --start-block=100 --end-block=300 --to=/home/exportblock.bin We have told Hyperledger Besu to export from mainnet the blocks 100 to 300 from mainnet to the location on our computer ~/home/exportblock.bin The subcommand differs from an option because it is a subcommand telling the computer to run another program, whereas an option is telling the computer to modify the parameters of the original command. Subcommands are short lived and give the user functionality related to a specific quick task, and then exit. They do not keep the command running. A subcommand and option are different in the ways they interact with the program. As we saw earlier, an option modifies a program by providing it with certain parameters. The subcommand, on the other hand, is actually telling the program to run another operation entirely.","title":"What is an Ethereum Client? Running Hyperledger Besu"},{"location":"S02-ethereum/M4-clients-workshop/L1/#what-is-an-ethereum-client-running-hyperledger-besu","text":"Hyperledger Besu is an Ethereum client with mainnet compatibility. It can also be used to run a private network. It is the open-source protocol layer that delivers all the perks from Ethereum and addresses enterprise requirements. Hyperledger Besu is actually part of a bundled suite of Ethereum protocol clients called ConsenSys Quorum, which consists of Hyperledger Besu and GoQuorum, private key managers, consensus mechanisms, transaction managers, APIs, plugins, libraries, deployment tools, etc. In this tutorial, we are going to walk you through how the client works, and take some detours to explain concepts in detail.","title":"What is an Ethereum Client? Running Hyperledger Besu"},{"location":"S02-ethereum/M4-clients-workshop/L1/#lets-get-started","text":"At the end of this guide, we will have walked you through deploying the Hyperledger Besu via the ConsenSys Quorum quickstart AND learned generally about private and public networks. As part of this process you will: Part 1: Download all necessary dependencies required of your system to run the ConsenSys Quorum Quickstart Part 2: Download and Start the ConsenSys Quorum Quickstart Part 3: Set up MetaMask and import a private key from the README Part 4: Deploy a decentralized application to the private network Part 5: Use the monitoring tools used in the quickstart Part 6: Deploy a second Smart Contract to the network and send a private transaction We will walk you through every step of the process, helping you get setup and providing helpful resources in case you get stuck. We will go into more detail than the documentation, where appropriate, to give you a deeper understanding of Hyperledger Besu We will be referring to the documentation found here throughout this article and linking to other useful documentation if you would like to do deeper into any particular topic.","title":"Let\u2019s get started"},{"location":"S02-ethereum/M4-clients-workshop/L1/#part-1-installing-dependencies","text":"This section will walk you through the installation of dependencies. This is written for new developers or those unfamiliar with the technology used by Hyperledger Besu. Feel free to move at a pace that meets your needs. MacOS users will start by installing NodeJS. Open up the terminal. If this is the first that you are hearing the term terminal or directory, check out this guide to understanding how the shell, terminal, and directories on a computer will work. This will be important to understanding how to navigate to the appropriate location on your computer. Type in node --version to see if node is already installed on your computer. If it is, you will get the version returned, for example: If node is not installed, you will see command not found: node . Click on this link to go to the Nodejs website and download the MacOS installer (the file name will end in .pkg). Open it up and follow the instructions. Once completed, restart your terminal. Then type node --version into the terminal again to confirm Node has properly installed. If you are having problems with installation of node, you can access the help from the Node community via the \u201chelp\u201d section of their GitHub, which includes links to their Slack channel. MacOS users will now move onto installing Docker and Docker Compose Open up the terminal and type in docker --version to see if Docker is already installed on your computer. If it is, you will get the version returned, for example: If Docker is not installed, you will see command not found: docker . Click on this link to go to Docker website and download Docker Desktop, which will include Docker and Docker Compose. Click the button for the appropriate chip within your Mac (The Apple chip was released in late 2020, if you have a Mac from before then it\u2019s most likely an Intel chip) Follow along with the rest of the instructions in the link provided. For a tutorial on Docker itself, you can find that here, along with another Getting Started Guide here. If you run into issues, access the troubleshooting guide for Docker. Open up Docker on your desktop, and you should see the following: Click on the Gear icon in the upper right corner. Navigate to resources and ensure under the \u201cAdvanced\u201d tab there is at least 6 GB of memory allocated to allow us to run all the examples in this Getting Started guide. Your settings should look like something below: Linux users will need first need to install Docker Engine and then can follow the steps contained within the Docker Compose documentation. After installing Docker, Linux users MUST follow the post-installation steps. Specifically, you need to add the user to the docker group, this step is often overlooked and every docker command will fail if this is not done.","title":"Part 1: Installing Dependencies"},{"location":"S02-ethereum/M4-clients-workshop/L1/#part-2-installing-and-starting-the-quorum-quickstart","text":"Open up the terminal and type in npx quorum-dev-quickstart and press enter ( npx , as opposed to the command npm , will cut down on pollution on your computer by not installing dependencies) The quickstart will download and we will get the following prompt. For the purposes of this guide, we will use the Hyperledger Besu option - which is 1. We are not trying out Codefi Orchestrate in this demo, so hit \u201cN for the prompt: Do you want to try out Codefi Orchestrate? Note: choosing yes will direct you to a login/registration page. [Y/n] N We do want to enable support for private transactions, so choose Y for the following prompt: Do you wish to enable support for private transactions? [Y/n] Y Do you wish to enable support for logging with ELK (Elasticsearch, Logstash & Kibana)? [y/N] Y This will look like: The default directory that will be created to store these files will be named quorum-test-network , but you can rename the network on this next line: Where should we create the config files for this network? Please choose either an empty directory, or a path to a new directory that does not yet exist. Default: ./quorum-test-network For example, if I wanted to name the directory besu-test-network , I would enter ./besu-test-network and this would create a new directory called besu-test-network . The name of the new directory has to be a directory that does not exist, or else we will get an error. The Quorum Quickstart will be installed on your computer in the directory named quorum-test-network . In the terminal, change directory to the quorum-test-network folder with the command cd quorum-test-network and the list out the sub-directories and files within this folder by typing the command ls to list the contents of quorum-test-network Type in the command less README.md to see the instructions for the quickstart. Hit q to exit out of this view. Alternatively, you can see the READ.ME within the Quorum Dev Quickstart by viewing it within the ConsenSys Quorum GitHub Repository. We are now going to start a private network that will run using Hyperledger Besu as the Ethereum client. This private network will run locally on our computer. Run the command ./run.sh in the terminal. The private network will now begin running. If you run into any issues with starting the Quickstart, use the Hyperledger Besu chat channel to reach out for help. How will we know if we have successfully launched the network? You should see the following (it may take a few minutes for this to load): Congratulations! You\u2019ve spun up a private Ethereum network on your local computer using Hyperledger Besu. Issues you may encounter, and how to resolve them: If you have use Docker and Docker compose previously, particularly with a previous version of this quickstart, it is likely that you could run into an error upon start up: ERROR: Pool overlaps with other one on this address space If you get this error, it means that even though the docker containers are down, the networks may still exist. A longer thread explaining this issue can be found here. A helpful resolution is to open up the command line and run the following command: docker network prune If that does not work, run the following combination of commands docker-compose down docker network prune This will restart the Docker service, and hopefully resolve any overlap conflicts. You can also run run docker ps to check for any overlapping registered containers. Once those are identified, use docker rm to remove the containers that are overlapping.","title":"Part 2: Installing and Starting the Quorum Quickstart"},{"location":"S02-ethereum/M4-clients-workshop/L1/#a-brief-discussion-of-the-consensus-mechanism","text":"Let\u2019s quickly discuss the consensus mechanism you are using in your network. As we discussed earlier, the consensus mechanism is the coordinating process that the network uses to confirm, sync, and finalize new blocks on your blockchain. In this section, we are going to explore the two Proof of Authority (PoA) consensus mechanisms that you can use with Hyperledger Besu: Clique and IBFT 2.0. In a private network, we want to avoid using the consensus mechanism that is used on Ethereum mainnet - Ethash Proof of Work (PoW). Why? Ethash works well for Ethereum mainnet because anyone can participate in Ethereum mainnet by running a node and providing computation power to the network. Therefore, this computationally intense consensus mechanism ensures that any participant must compete with the other participants to produce and propagate blocks. The incentive of block rewards serves to ensure they are following the rules of the network. The more computing power that each node brings to the network, the greater benefits to the security of the network when PoW is the consensus mechanism. We are going to assume that you are not using the Ethash Proof of Work (PoW) consensus mechanism that is used for Ethereum mainnet because your network has participants that are: Known to each other, and Have a certain degree of inherent trust in each other. These two assumptions allow for your network to use a consensus mechanism which is more computationally efficient for the participants - they do not have to have the same computing power required of a node on Ethereum mainnet. These assumptions also allow for the removal of the incentive of block rewards to ensure compliance. Using a PoA consensus mechanism gives your network: Faster block creation as compared to a network running Ethash: Your network can create new blocks in a one or two seconds. As a comparison, Ethash on mainnet averages about 15 seconds for creation of a new block. Greater transaction throughput for the network: Without the need to incentivize the participants, the size of each block can be arbitrarily large, allowing for blocks with many transactions included as your network needs them to be included. Large transactions will not significantly or noticeably slow down the network (smaller blocks will be processed faster than larger blocks, the end user should not notice a difference). PoA assumes that the creators of the blocks - which are sometimes called minters, signers, sealers, or validators (we will refer to nodes on a PoA network as validators from here on) - are authorized to be validators. At the creation of the PoA network we declare a group (or \u201cpool\u201d) of validators, and as the network runs, those starting validators can add and/or subtract other validators.","title":"A Brief Discussion of the Consensus Mechanism"},{"location":"S02-ethereum/M4-clients-workshop/L1/#a-short-explanation-of-poa","text":"To understand PoA, it helps to specifically discuss the consensus mechanism Clique PoA, as it was designed to approximate Ethereum mainnet for use in Ethereum testnets. The Ropsten testnet used PoW, but was plagued by attackers forcing reorgs and forks in the testnet and increasing the block gas limit and sending large transactions, which effectively stopped the network. A new consensus mechanism was designed by P\u00e9ter Szil\u00e1gyi called Clique, which could replace PoW on networks like testnets. Clique PoA was designed to use the existing Ethereum block data structure to add voting functionality. Generally in any PoA consensus mechanism, validators alternate adding blocks to the blockchain in a series. In addition to adding blocks, they can choose to vote to add or kick out a validator. This allows malicious validators to be removed from the network, and trusted validators to be added. This gives PoA resistance against certain kinds of attack vectors like: A malicious validator - if a machine is compromised, and a validator proposes incorrect blocks, they can be voted out by the other validators A censoring validator - if a compromised validator proposes votes to remove good actors, they can be removed by being voted out A spamming validator - if a validator makes a voting proposal in every block that the validator creates, the network is spammed by having to consistently tally the votes (votes being to add or remove a validator). By resetting voting after a set period of time (this period of time is referred to as an epoch and is set to be 30,000 blocks in our tutorials), this problem can be mitigated. Concurrent block creation by a validator can also be prevented by ensuring there is a substantial enough lag between each block being added to the blockchain. In short, many of the features of mainnet Ethereum can be approximated, but without needing to use PoW and the challenges that using PoW on a private or test blockchain would entail.","title":"A short explanation of PoA"},{"location":"S02-ethereum/M4-clients-workshop/L1/#clique-poa-compared-to-ibft-20-poa","text":"Now that we know a bit more about PoA, we\u2019ll compare Clique PoA with IBFT 2.0 PoA in regards to: Finality - the amount of time required to guarantee a block has been added to the blockchain. Validators required - the minimum amount of validator nodes that have to participate in the network in order for the PoA consensus mechanism to work as intended. Liveness - the number of validator nodes that must be online or \u201clive\u201d to allow the network to continue running. This can also be thought of as a kind of fault tolerance - the number of validators that can fail at a given point in time and the network will still run. Relative Speed of Block Creation - how long it takes for each PoA consensus mechanism to create and sync new blocks and the factors that impact the speed of block creation. Finality Instantaneous finality is possible when only one block can be added at a point in time to the blockchain. If there is a possibility that multiple blocks are proposed at the same time, then a fork or reorganization can occur, and the validators are forced to make a choice about which block is a part of the final chain. In Clique PoA, a block will not be guaranteed to be added to the chain upon creation because there is a possibility of two blocks being proposed at the same time if a validator proposes a block out of turn. Blocks proposed simultaneously cause a fork in the network, as there are now two possible blocks for the next validator to create a block subsequent to (the second signer is prevented from signing subsequent blocks by the consensus mechanism). Because there are now two possible chains that could be built, the network must reorganize (or reorg), choosing which chain to build on. Due to this possibility, finality can be delayed when using Clique PoA if validators create blocks out of turn, and a block is not definitively confirmed upon creation. IBFT 2.0 has immediate finality, provided there are four validators. Once a block is created it becomes part of the blockchain. There are no forks. Validators Required Clique can run with a single validator (the use case for a single validator is purely demonstration purposes). As the number of validators in a Clique network increases, the probability of forks in the network increases. IBFT 2.0 requires four validators to be byzantine fault tolerant. Three or less validators running on an IBFT 2.0 network will no longer guarantee immediate finality or prevent network manipulation, particularly if one or more of the validators is acting in an adversarial nature. Liveness Clique can operate with half of the validators running (half of the validators can go down and the network will still work). IBFT 2.0 can operate as long as at least \u2154 of the validators are working. As noted in the validators required section, three or less validators running on an IBFT 2.0 network will no longer guarantee immediate finality, particularly if one or more of the validators is acting in an adversarial nature. Speed Clique is faster than IBFT 2.0 at adding blocks. IBFT 2.0 is slower at adding blocks. As more validators are added the time to add new blocks increases.","title":"Clique PoA Compared to IBFT 2.0 PoA"},{"location":"S02-ethereum/M4-clients-workshop/L1/#specific-examples","text":"","title":"Specific Examples"},{"location":"S02-ethereum/M4-clients-workshop/L1/#guitar-manufacturing","text":"A custom guitar manufacturer wants to create transparency for their customers to show the woods they are using in building custom guitars. They source woods locally and use reclaimed woods from the city they are based in. They want to write a transaction to a blockchain so that a customer can scan a QR code and see each transaction associated with the various woods used in the guitars. They work directly with two wood suppliers, and with an additional individual who helps them with salvage work to get reclaimed wood from old buildings. Looking at this case, we see the following: There is anywhere from a single to maybe four participants to start (the two wood suppliers and the reclaimed wood supplier) This is a project to provide transparency to the end-user, so transactions can be sent as needed, with addresses being created to represent specific guitars that are being built. The transactions will represent different woods and provide information on their sourcing If a single node were run, if it were to go offline, it would only impact the guitar builder. Because immediate finality is not required, there is the potential of only needing a single validator at the start, it would be possible to use a Clique PoA network. If the suppliers wanted to join the network, they could all do so, and if one of them left the network, there would not be an impact to running the blockchain.","title":"Guitar Manufacturing"},{"location":"S02-ethereum/M4-clients-workshop/L1/#casinos","text":"A group of six casinos decide to create a sportsbook together, in order to share in winnings and decrease the individual exposure of each casino. Each casino will run a validator node and transactions will be the bets that bettors place on basketball games. As the games being bet on have a specific start time, bets must be included in the blockchain to receive a payout. If bets are not included, the payout is not able to be made to the bettor. Looking at this use case, we see the following: There are at least six participants at start, and therefore we assume there will be six validators The bets that are made have to be included in a block. Immediate finality is required If a certain number of casinos go offline, we may have to question whether to take bets or not, since the other casinos become more exposed to the betting positions. There should be a high volume of transactions allowed, as the scenario of the number of placed bets is likely to increase the closer a basketball game approaches the start time. Because immediate finality is required, we are drawn to using an IBFT 2.0 PoA network. If we were to do this, then we would need at least 4 validators. Since there are six casinos participating, we can use an IBFT 2.0 PoA network. If more than two casinos were to leave then the network would no longer be able to run. In terms of liveness, the network can withstand a validator going offline (due to a power outage or other reason), and continue to operate, which is very important for the overall success of the sports book.","title":"Casinos"},{"location":"S02-ethereum/M4-clients-workshop/L1/#determining-the-poa-consensus-mechanism-to-use-in-your-network","text":"Based on these examples, here are questions to ask yourself, which can help you determine which PoA consensus mechanism may make more sense. What are my finality requirements? Does my use case require that each block created instantly be confirmed? If a block is not instantly confirmed, what are the potential impacts? How many validators do I intend my network to have? How many will I have at the start? How many could be added to the network over time? Do I think validators could leave my network? Would I stop running the network if a certain number of validators left? Do I need participants to join the network without being validators? Should they be able to submit transactions? What fault tolerance does my network require? Does the number of validators that will participate at start allow for the network to continue running if one goes down? What happens if two or more validators go down? How fast do I need to be able to create new blocks? Will the speed at which I need to create new blocks stay constant or can it slow down if new validators are added? These questions can serve as a guide in thinking which PoA consensus mechanism makes sense for your use case.","title":"Determining the PoA Consensus Mechanism to Use in Your Network"},{"location":"S02-ethereum/M4-clients-workshop/L1/#part-3-set-up-metamask","text":"Now that we have the network up and running, we are going to set up our MetaMask wallet. If you already have MetaMask installed, you can skip this step. If not, follow the links below. First, download the MetaMask browser extension. If you want to explore the features of MetaMask, this article can help you walk through how to send your first transaction using MetaMask. Follow the instructions to set up your MetaMask account. For the purposes of this guide, the accounts we will be using will not have mainnet Ether. Still get in the habit of the following: - Copy down your seed phrase and store it in a secure location. - Do not share your seed phrase with anyone - Ensure that you can use your seed phrase to recover your account MetaMask is a self-custodial wallet, so if you lose the seed phrase associated with it, you have lost access to that Ethereum account. If you give that seed phrase to someone, they now have control over that account. Open up your MetaMask account and navigate to to the top of the browser extension click on the Ethereum Mainnet menu. From the drop down select Localhost 8545. This is where the blockchain that we spun up with the quickstart is running. If you want to learn more about connecting to different networks, this documentation from MetaMask will give you more insight on how it works. MetaMask will allow us to send transactions between Ethereum accounts on our network, which we will call Externally Owned Accounts (EOA). When we ran the quickstart, the private blockchain network that we created was initialized with three accounts pre-loaded with test Ether. Looking into the quorum-test-network folder, you will see a structure within the config -> besu folder that shows the three member folders. In each folder is a keys folder, which has a file called key, which is the private key and key.pub, which is the public key. We will open the file named \u201ckey\u201d in member1 to see what is inside Here we see the private key, without the 0x prefix. The private key for this address would be 0x8f2a55949038a9610f50fb23b5883af3b4ecb3c3bb792cbcefbd1542c692be63 To import this address into our MetaMask, we will click the circle in the upper right corner of the MetaMask extension. This circle is called a favicon by the MetaMask team: We will hit the Import Account button: And will copy paste the private key, repeated below: 0x8f2a55949038a9610f50fb23b5883af3b4ecb3c3bb792cbcefbd1542c692be63 Into the following field This will create our account, which is pre-loaded with 200 test Ether: Below you can find information about the pre-loaded accounts:, in the event you want to import them: Test Account 1 (address 0xfe3b557e8fb62b89f4916b721be55ceb828dbd73) Private key to copy : 0x8f2a55949038a9610f50fb23b5883af3b4ecb3c3bb792cbcefbd1542c692be63 Initial balance : 200 Eth (200000000000000000000 Wei) Test Account 2 (address 0x627306090abaB3A6e1400e9345bC60c78a8BEf57) Private key to copy : 0xc87509a1c067bbde78beb793e6fa76530b6382a4c0241e5e4a9ec0a0f44dc0d3 Initial balance : 90000 Eth (90000000000000000000000 Wei) Test Account 3 (address 0xf17f52151EbEF6C7334FAD080c5704D77216b732) Private key to copy : 0xae6ae8e5ccbfb04590405997ee2d52d2b330726137b875053c36d94e974d162f Initial balance : 90000 Eth (90000000000000000000000 Wei) If we import these accounts and send 7 ETH from Test Account 1 to Test Account 2: we can see within the block explorer (accessible in our browser at http://localhost:25000/), Account 1 will decrement by 7 ETH: And that Account 2 will increase by just under 7 ETH Why didn\u2019t Account 2 increase by 7 ETH? In this specific example, we have paid a transaction cost to send the Ether from one account to another. The transaction cost is called the gas, and is sometimes also called the gas fee. It is paid in increments of Ether called Wei, which is a fractional amount of ETH (1 Wei is 10^-18 ETH). There is a 21000 wei cost we pay to send Ether, and that is deducted from the transaction, which was the gas limit (the upper amount of gas we are willing to pay) stipulated for this transaction on MetaMask. The private network we have created is a gas-less network, so there is no need to pay that fee, and we could reduce the gas limit to 0 and the transaction would go through. Understanding gas is critical for understanding how mainnet Ethereum works, as well as how it may be useful in other private networks we create, but for now, you can read a brief overview about gas here to better understand this concept. How were these accounts created? When we ran ./run.sh in our terminal, this script told docker-compose to run a .yml file, which contained information about the accounts, keys, and initialization information for our network in a genesis file. This genesis file - which is a .json file, provided our blockchain with information about the accounts, including the address and the amount of Ether they held. To see a genesis file in the quickstart, you can open and look at cliqueGenesis.json, which is in the quorum-test-network -> config -> besu directory. It looks like the following: { \"config\":{ \"chainId\":1337, \"constantinoplefixblock\": 0, \"clique\":{ \"blockperiodseconds\":15, \"epochlength\":30000 } }, \"coinbase\":\"0x0000000000000000000000000000000000000000\", \"difficulty\":\"0x1\", \"extraData\":\"0x00000000000000000000000000000000000000000000000000000000000000004592c8e45706cc08b8f44b11e43cba0cfc5892cb0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", \"gasLimit\":\"0xa00000\", \"mixHash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\", \"nonce\":\"0x0\", \"timestamp\":\"0x5c51a607\", \"alloc\": { \"fe3b557e8fb62b89f4916b721be55ceb828dbd73\": { \"privateKey\": \"8f2a55949038a9610f50fb23b5883af3b4ecb3c3bb792cbcefbd1542c692be63\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"0xad78ebc5ac6200000\" }, \"627306090abaB3A6e1400e9345bC60c78a8BEf57\": { \"privateKey\": \"c87509a1c067bbde78beb793e6fa76530b6382a4c0241e5e4a9ec0a0f44dc0d3\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" }, \"f17f52151EbEF6C7334FAD080c5704D77216b732\": { \"privateKey\": \"ae6ae8e5ccbfb04590405997ee2d52d2b330726137b875053c36d94e974d162f\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" } }, \"number\":\"0x0\", \"gasUsed\":\"0x0\", \"parentHash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\" } Within this file, I have highlighted the accounts that were created and the balances they were created with (the balances are in wei, not Ether. Remember 1 wei is 10 x -18 Ether). The quickstart and the scripts we run take care of creating these accounts, but we are able to create our own network where we can initialize accounts and configure our network to be customized to our particular use case. Understanding a Genesis File: The Basics Setting up a private network or joining a public network requires an Ethereum node to create a new blockchain. Whether it is a private or public network, each node / validator will have a full copy of the blockchain. In order to build this copy, the node / validator has to have instructions on how to build the first block and the subsequent blocks in the chain. In this article, we are going to walk through the components of a genesis file, within the context of the Ethereum client Hyperledger Besu as the client for our network. These concepts are applicable to all Ethereum clients. The first block in a blockchain is called the genesis block. Ethereum mainnet\u2019s genesis block - block 0 - was mined on July 30, 2015. In order to join or create any network, the data for the genesis block must be included. Therefore, the genesis file defines the data that is in the first block of a blockchain, as well as rules for the blockchain itself. If a new node or validator attempts to join the blockchain, it will use the genesis file as the starting point in recreating the history of the chain in order to synchronize with the existing network. The genesis file for Ethereum mainnet, along with the supported testnets, is included in the download of Besu. When creating a private network, a custom genesis file must be provided. The genesis file is a JSON formatted file. It looks like the below: { \"config\": { \"chainId\": 2018, \"muirglacierblock\": 0, \"ibft2\": { \"blockperiodseconds\": 2, \"epochlength\": 30000, \"requesttimeoutseconds\": 4 } }, \"nonce\": \"0x0\", \"timestamp\": \"0x58ee40ba\", \"extraData\": \"0xf83ea00000000000000000000000000000000000000000000000000000000000000000d5949811ebc35d7b06b3fa8dc5809a1f9c52751e1deb808400000000c0\", \"gasLimit\": \"0x1fffffffffffff\", \"difficulty\": \"0x1\", \"mixHash\": \"0x63746963616c2062797a616e74696e65206661756c7420746f6c6572616e6365\", \"coinbase\": \"0x0000000000000000000000000000000000000000\", \"alloc\": { \"9811ebc35d7b06b3fa8dc5809a1f9c52751e1deb\": { \"balance\": \"0xad78ebc5ac6200000\" } } } In this specific example, the genesis file is for an IBFT 2.0 private network. { \"config\": { \"chainId\": 2018, \"muirglacierblock\": 0, \"ibft2\": { \"blockperiodseconds\": 2, \"epochlength\": 30000, \"requesttimeoutseconds\": 4 } The config key section contains the following information about the blockchain \u201cchainId\u201d: 2018 The chainId controls the transaction signature process, providing a unique identifier to allow for the hashing of signed transactions to only work on the network associated with the corresponding chainId. Ethereum Improvement Proposal 155 (EIP-155) provides more information on the relationale behind the chainID. Most chainIDs match the networkID of the network. In this case, 2018 refers to the chainID associated with a development chain. For a list of the network and chain IDs, please see the Documentation. \"muirglacierblock\": 0, This field is called a \u201cmilestone block\u201d. Muir glacier refers to a specific network upgrade that occurred at block 9,200,000 on Ethereum mainnet. For private networks, like the one that is being created in this example, the name of the latest milestone block can be listed, and set to be the genesis block. Here you can see a continuously updated list of network upgrades and the associated blocks for Ethereum. \u201cibft2\u201d: This specifies that the consensus protocol for the blockchain is IBFT 2.0. The options available for specifying the consensus mechanism are available in the documentation, with an overview of the proof-of-authority (PoA) consensus protocols available here. Within the specification, the following three fields are provided: \"blockperiodseconds\": 2, The minimum block time, in seconds. In this case, after two seconds, a new block will be proposed by the network with transactions stored in the memory pool packaged and distributed to the network. \"epochlength\": 30000, The number of blocks at which to reset all votes. The votes refer to validators voting to add or remove validators to the network. In this case, after 30,000 blocks are created, this IBFT 2.0 network will discard all pending votes collected from received blocks. Existing proposals remain in effect and validators re-add their vote the next time they create a block. \"requesttimeoutseconds\": 4 The time by which a new block must be proposed or else a new validator will be assigned by the network. If a validator goes down, the request time out ensures that the proposal of a new block passes on to another validator. The request time out seconds should be set to be double the minimum block time (blockperiodseconds), hence why it is 4. The second section, starting with \"nonce\": \"0x0\", contains information about the genesis block \"nonce\": \"0x0\", \"timestamp\": \"0x58ee40ba\", \"extraData\": \"0xf83ea00000000000000000000000000000000000000000000000000000000000000000d5949811ebc35d7b06b3fa8dc5809a1f9c52751e1deb808400000000c0\", \"gasLimit\": \"0x1fffffffffffff\", \"difficulty\": \"0x1\", \"mixHash\": \"0x63746963616c2062797a616e74696e65206661756c7420746f6c6572616e6365\", \"coinbase\": \"0x0000000000000000000000000000000000000000\", \"alloc\": { \"9811ebc35d7b06b3fa8dc5809a1f9c52751e1deb\": { \"balance\": \"0xad78ebc5ac6200000\" } } } \"nonce\": \"0x0\", The number used once aka nonce, that is a part of the blockheader for the first block. Set to 0x0. \"timestamp\": \"0x58ee40ba\", The creation date and time of the block. Often it can be set to 0x0, but as long as it is any value in the past, it will work. In this case 0x58ee40ba is a hexadecimal which converts to 1492009146 and represents Wed Apr 12 2017 14:59:06 GMT+0000 \"extraData\": \"0xf83ea00000000000000000000000000000000000000000000000000000000000000000d5949811ebc35d7b06b3fa8dc5809a1f9c52751e1deb808400000000c0\", Extra data is a recursive length prefix (RLP) encoded string (which is space efficient) containing the validator addresses of the IBFT 2.0 private network. The validator addresses are unique to the validators, so if there are four validators are the start of the network, this field should contain a list of their addresses.. Instructions on how to create an RLP using Besu can be found here. \"gasLimit\": \"0x1fffffffffffff\", The block gas limit, which is the total gas limit for all transactions included in a block. It defines how large the block size can be for the block, and is represented by an hexadecimal string. For this network, the gas limit is the maximum size, and is therefore a \u201cfree gas network\u201d \"difficulty\": \"0x1\", The difficulty of creating a new block. Represented as a hexadecimal string, the difficulty is set to 1, effectively the lowest difficulty. \"mixHash\": \"0x63746963616c2062797a616e74696e65206661756c7420746f6c6572616e6365\", The mixHash is the unique identifier for the block, which for this genesis file is 0x63746963616c2062797a616e74696e65206661756c7420746f6c6572616e6365 \"coinbase\": \"0x0000000000000000000000000000000000000000\", The network coinbase account, which is where all block rewards for this network will be paid. In this case it is to 0x0000000000000000000000000000000000000000, which is sometimes called address(0) or the zero address. \"alloc\": { \"9811ebc35d7b06b3fa8dc5809a1f9c52751e1deb\": { \"balance\": \"0xad78ebc5ac6200000\" The alloc field creates an address on our network, which is sometimes also referred to as an externally owned account, as it is an account not associated with a smart contract (referred to as a contract account). The number starting with \u201c98\u201d is the public key of the address. The balance can be passed in as a decimal OR a hexadecimal (like it has in this case and corresponds to 200 ETH, or 2*10^20 Wei). The balance is always in Wei, or 10^-18 Ether. A Second Genesis File Below we provide another genesis file with a different consensus mechanism, Clique PoA, and different information. Take a look below and see what values stick out. This is the genesis file for the chain that you can build in the ConsenSys Quorum quickstart: { \"config\":{ \"chainId\":1337, \"muirglacierblock\": 0, \"clique\":{ \"blockperiodseconds\":15, \"epochlength\":30000 } }, \"coinbase\":\"0x0000000000000000000000000000000000000000\", \"difficulty\":\"0x1\", \"extraData\":\"0x00000000000000000000000000000000000000000000000000000000000000004592c8e45706cc08b8f44b11e43cba0cfc5892cb0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", \"gasLimit\":\"0xa00000\", \"mixHash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\", \"nonce\":\"0x0\", \"timestamp\":\"0x5c51a607\", \"alloc\": { \"fe3b557e8fb62b89f4916b721be55ceb828dbd73\": { \"privateKey\": \"8f2a55949038a9610f50fb23b5883af3b4ecb3c3bb792cbcefbd1542c692be63\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"0xad78ebc5ac6200000\" }, \"627306090abaB3A6e1400e9345bC60c78a8BEf57\": { \"privateKey\": \"c87509a1c067bbde78beb793e6fa76530b6382a4c0241e5e4a9ec0a0f44dc0d3\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" }, \"f17f52151EbEF6C7334FAD080c5704D77216b732\": { \"privateKey\": \"ae6ae8e5ccbfb04590405997ee2d52d2b330726137b875053c36d94e974d162f\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" } }, \"number\":\"0x0\", \"gasUsed\":\"0x0\", \"parentHash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\" } Once again, we will look at the fields and explain them. { \"config\":{ \"chainId\":1337, \"constantinoplefixblock\": 0, \"clique\":{ \"blockperiodseconds\":15, \"epochlength\":30000 } } The config key section contains the following information about the blockchain \u201cchainId\u201d: 1337 In this case. 1337 refers to a local chainID. MetaMask, a self-custodial wallet, and Ganache, which is Truffle\u2019s Private Blockchain App, both use this as the local chain ID, and so to follow convention, in this genesis file we are doing the same. For a list of the network and chain IDs, please see the Documentation. \"muirglacierblock\": 0, Once again, we have set the milestone block to Muir Glacier. Something to note - The \u201cmilestone block\u201d could be for this configuration file, which is something you may see in some tutorials. For example, if we saw constantinopleBlock\": 0, this refers to a specific network upgrade that occurred at block 7,280,000 on Ethereum mainnet. For private networks, like the one that is being created in this example, the name of the latest milestone block can be listed, and set to be the genesis block. Here you can see a continuously updated list of network upgrades and the associated blocks for Ethereum. \u201cclique\u201d: This specifies that the consensus protocol for the blockchain is Clique. The options available for specifying the consensus mechanism are available in the documentation, with an overview of the proof-of-authority (PoA) consensus protocols available here. Within the specification, the following two fields are provided: \"blockperiodseconds\": 15, The minimum block time, in seconds. In this case, after 15 seconds, a new block will be proposed by the network. Given this genesis file is modeled after the testnet, it is made to approximate the minimum blocktime of mainnet, which is 15 seconds. \"epochlength\": 30000, The number of blocks at which to reset all votes. The votes refer to validators voting to add or remove validators to the network. In this case, after 30,000 blocks are created, this Clique network will discard all pending votes collected from received blocks. Existing proposals remain in effect and validators re-add their vote the next time they create a block. Starting at the coinbase we now have the information available in the genesis block \"coinbase\":\"0x0000000000000000000000000000000000000000\", \"difficulty\":\"0x1\", \"extraData\":\"0x00000000000000000000000000000000000000000000000000000000000000004592c8e45706cc08b8f44b11e43cba0cfc5892cb0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", \"gasLimit\":\"0xa00000\", \"mixHash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\", \"nonce\":\"0x0\", \"timestamp\":\"0x5c51a607\", \"alloc\": { \"fe3b557e8fb62b89f4916b721be55ceb828dbd73\": { \"privateKey\": \"8f2a55949038a9610f50fb23b5883af3b4ecb3c3bb792cbcefbd1542c692be63\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"0xad78ebc5ac6200000\" }, \"627306090abaB3A6e1400e9345bC60c78a8BEf57\": { \"privateKey\": \"c87509a1c067bbde78beb793e6fa76530b6382a4c0241e5e4a9ec0a0f44dc0d3\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" }, \"f17f52151EbEF6C7334FAD080c5704D77216b732\": { \"privateKey\": \"ae6ae8e5ccbfb04590405997ee2d52d2b330726137b875053c36d94e974d162f\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" } } \"coinbase\": \"0x0000000000000000000000000000000000000000\", The coinbase account, which is where all block rewards for this network will be paid. In this case it is to 0x0000000000000000000000000000000000000000, which is sometimes called address(0) or the zero address. \"difficulty\": \"0x1\", The difficulty of creating a new block. Represented as a hexadecimal string, the difficulty is set to 1, effectively the lowest difficulty. \"extraData\":\"0x00000000000000000000000000000000000000000000000000000000000000004592c8e45706cc08b8f44b11e43cba0cfc5892cb0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", Extra data is a recursive length prefix (RLP) encoded string (which is space efficient) containing the validator address of the Clique private network, which in this case is 4592c8e45706cc08b8f44b11e43cba0cfc5892cb. Instructions on how to create an RLP using Besu can be found here and how to add additional addresses for a Clique network with additional signers can be found here. \"gasLimit\": \"0xa00000\", The block gas limit, which is the total gas limit for all transactions included in a block. It defines how large the block size can be for the block, and is represented by an hexadecimal string. For this network, the gas limit is \"mixHash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\", The mixHash is the unique identifier for the block, which for this genesis file is 0x0000000000000000000000000000000000000000000000000000000000000000 \"nonce\": \"0x0\", \"timestamp\": \"0x5c51a607\", In this case 0x5c51a607 is a hexadecimal which converts to 1548854791 and represents Wed Jan 30 2019 13:26:31 GMT+0000. \"alloc\": { \"fe3b557e8fb62b89f4916b721be55ceb828dbd73\": { \"privateKey\": \"8f2a55949038a9610f50fb23b5883af3b4ecb3c3bb792cbcefbd1542c692be63\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"0xad78ebc5ac6200000\" }, \"627306090abaB3A6e1400e9345bC60c78a8BEf57\": { \"privateKey\": \"c87509a1c067bbde78beb793e6fa76530b6382a4c0241e5e4a9ec0a0f44dc0d3\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" }, \"f17f52151EbEF6C7334FAD080c5704D77216b732\": { \"privateKey\": \"ae6ae8e5ccbfb04590405997ee2d52d2b330726137b875053c36d94e974d162f\", \"comment\": \"private key and this comment are ignored. In a real chain, the private key should NOT be stored\", \"balance\": \"90000000000000000000000\" } The alloc field creates three addresses on our network, The balance can be passed in as a decimal (like the second and third account, which are both 900 ETH, or 2 10^20 Wei) OR a hexadecimal (like the first account, which is 200 ETH, or 2 10^20 Wei). The balance is always in Wei, or 10^-18 Ether. Deploying Your Genesis File Once you have created your genesis file, you will save it within the directory where your blockchain networks files will be kept. Do not save it within any of the Node or data folders, but rather at the top level. When it is time to start your network, you will use the flag --genesis-file=../genesis.json To start up your network using the genesis file use the following command: besu --genesis-file=../genesis.json For more information, please see the Documentation. Part 4: Deploy the Pet Shop Decentralized Application within the dapps folder of the quorum-test-network directory We are now going to deploy a decentralized application (dapp) to our private network. From your terminal, change directory to the folder dapps, and then into pet-shop cd dapps/pet-shop/ Within this directory, you will find a file called custom_config and a script called run_dapp.sh run_dapp.sh looks like the following:","title":"Part 3: Set up MetaMask"},{"location":"S02-ethereum/M4-clients-workshop/L1/#binbash","text":"set -e hash truffle 2>/dev/null || { echo >&2 \"This script requires truffle but it's not installed.\" echo >&2 \"Refer to documentation to fulfill requirements.\" exit 1 } rm -rf pet-shop-box git clone https://github.com/truffle-box/pet-shop-box.git cp -r custom_config/* ./pet-shop-box/ cd pet-shop-box/ npm install npm install truffle npm install @truffle/hdwallet-provider truffle compile truffle migrate --network quickstartWallet truffle test --network quickstartWallet docker build . -t quorum-dev-quickstart_pet_shop docker run -p 3001:3001 --name quorum-dev-quickstart_pet_shop --detach quorum-dev-quickstart_pet_shop This script is going to download from GitHub a Truffle box, which are pre-built dapps or templates for building your own dapp. In this case, the pet-shop dapp contains a smart contract called Adoption.sol, a front-end for this dapp written in react, which is a javascript library specifically for building front-ends and user interfaces, and the necessary dependencies needed to deploy the smart contract to an Ethereum network. The script is going to install truffle which is required in order to run commands using truffle, like truffle migrate. Truffle migrate is a command which specifically compiles our smart contract (Adoption.sol) into EVM bytecode and deploys it to the private network we have created. The script also downloads hdwallet-provider, which is used to create Ethereum accounts that we can use to interact with our decentralized application, and to send transactions on the private network we have created. The Ethereum accounts we have created will be imported into MetaMask. If you want to familiarize yourself with Truffle, you can walk through a tutorial here. For our current tutorial, we are using the private network created by the quorum quickstart as our local test blockchain network. Within the folder custom_config, we have our Smart Contract - Adoption.sol Adoption.sol is a simple smart contract written in solidity. It looks like this: pragma solidity ^0.5.0; contract Adoption { address[16] public adopters; // Adopting a pet function adopt(uint petId) public returns (uint) { require(petId >= 0 && petId <= 15); adopters[petId] = msg.sender; return petId; } // Retrieving the adopters function getAdopters() public view returns (address[16] memory) { return adopters; } } A detailed description of this smart contract, along with a tutorial to build it can be found on the Truffle website. We are going to give you a quick overview of the contract in order to contextualize what happens once it is deployed to our private blockchain. Let us take a look at the smart contract to better understand what it does: pragma solidity ^0.5.0; This first line of code tells us the smart contract is written for Solidity version 0.5.0 or higher, and will be compatible with 0.5.0 and higher. contract Adoption { address[16] public adopters; These two lines of code do the following: contract Adoption { declares an object of type contract. If you want to learn more about this object type, the Solidity documentation has a nice explanation of a Contract object. In this example, anything after the { (the brackets) will define the contract object - what it can do and what it can store on the blockchain. address[16] public adopters; instantiates 16 addresses in an array which we have given the variable name \u201cadopters\u201d. These addresses are public, which means they can be accessed from outside of this contract as well as inside the contract. // Adopting a pet function adopt(uint petId) public returns (uint) { require(petId >= 0 && petId <= 15); adopters[petId] = msg.sender; return petId; } The first line is a comment, which will not be compiled but gives us information that the following function is for adopting a pet. The function, adopt, takes in a unsigned integer called petID, and checks if it is between 0 and 15. If it is, then the address at the location of the the petID within the adopters array is assigned to the account that called the adopt function (adopters[petID] = msg.sender is the specific code). Then the petID is returned. The second function: // Retrieving the adopters function getAdopters() public view returns (address[16] memory) { return adopters; } returns the array of addresses of the adopters. Now that we understand the smart contract,we run the command ./run_dapp.sh in the terminal within the pet-shop folder (which we should have already navigated into if you have been following along with each step). This is going to download all the dependencies, as well as compile and migrate our smart contract to the private network we have created using the Quorum quickstart, and spin up a front end that will allow us to \u201cadopt\u201d some group of 16 pets. When you run it, there will be many downloads. You know that the script has successfully run when you reach the following: Here we can see that Truffle has deployed the Migrationsmart contract and has now deployed it to the Hyperledger Besu based blockchain network we have spun up. If we take the contract address: 0x345cA3e014Aaf5dcA488057592ee47305D9B3e10 that is listed here and navigate in a web browser to the block explorer (go to your browser and type in: http://localhost:25000/, then paste in the contract address: 0x345cA3e014Aaf5dcA488057592ee47305D9B3e10 We will see the following information about our deployed smart contract: We can also use the block explorer to look up other information, like the externally owned account (in the >account field, which for our specific example is: 0x627306090abaB3A6e1400e9345bC60c78a8BEf57) We are able to see the ETH balance for that account. The ETH used in this example is not mainnet ETH, and it is only used for testing purposes. From this step, we hope that you can see how you can use the quickstart to deploy a decentralized application. Thus, the quickstart provides you with a template of being able to test a decentralized application of your own that you build. Let\u2019s look at the dapp by going to our browser and typing in http://localhost:3001. We will arrive at the following screen, and MetaMask, which we installed in Part 3, will pop up for us and ask which account we want to connect to the application. Hitting adopt will prompt MetaMask to pop-up and adopt the pet: Since this is a gasless network, there is no fee associated with adopting a pet. Issues you may encounter and how to solve them When you run /.run_dapp.sh you may get the following error: The Error: Deployment Failed \u201cMigrations\u201d --Wrong chainId means that within the deployment.js file, the chainID (which chain you are deploying to) was incorrectly specified. If that is the case, you should run the npx quorum-dev-quickstart again, as this means you are on an older version of the quickstart that does not have the most up to date versions of truffle and hd-wallet provider. Specifically, you need to be on 1.4.0 and above of hd-wallet provider. Versions 1.3.0 and 1.3.1 will not work work with the quickstart (though users will see that version 1.2.6 will still work with the quickstart). Make sure you are on version 0.0.21 of the quickstart or higher to avoid this issue. If there is no UI that appears at localhost:3001 after running the script ./run_dapp.sh, navigate to the pet-shop-box folder: cd pet-shop-box amd use the command npm run dev to start up the front end in our browser. If this runs successfully, we will get the following output in the terminal: Then you can navigate to the URLs listed in the output to see your application. Part 5: Use the monitoring tools used in the quickstart There are monitoring tools included in the quickstart, which allow us to learn about how our private blockchain network is working. In part 4, we use the block explorer to look at the contract address of the deployed smart contract and the externally owned account that was responsible for deploying the smart contract to our network. Now we are going to explore two additional tools we can use to monitor our private network. The first is Prometheus - an open source monitoring tool that pulls data from the services that it is connected to - like nodes, databases, validators, servers, etc - and allows for that data from these services to be use in alerts and notifications. For example, if the memory usage of a certain node on your network exceeds a defined threshold, Prometheus is able to track this and alert you so you can take the appropriate action to ensure your network continues to run properly. In the case of a failure, Prometheus provides the data from the various services to allow for troubleshooting the issue. Prometheus pulls the data and stores it in a database, and then allows you or a data visualization tool to query that data. When you install and run the quickstart, you will get access to Prometheus at http://localhost:9090/graph. If you enter this address into a web browser, you will see the following: What exactly is Prometheus monitoring? To see the details, you can navigate to the config folder within your quorum-test-network directory. Opening the prometheus folder will reveal a prometheus.yml file. If you open that with a text editor (like Visual Studio Code or Vim) you will see that Prometheus is configured by the quickstart to scrape data from each validator, the rpcnode, and each member of the network every 15 seconds, and to store that data in the /metrics pathway, so that you can query it. Below is a screenshot of the first 28 of the total 129 lines of the prometheus.yml file The metrics we have access to can be seen by navigating and clicking on the curricular icon next to the Execute button in the right upper side of the screen. This will reveal the list of metrics that you can query Promtheus to return data for: If we choose ethereum_peer_count, it will return the number of peers on the network, which for this example is seven If we type in the following query into the search bar (the part of the user interface with \u201cExpression (press Shift+Enter for newlines) besu_blockchain_difficulty_total We will get back the following a result of our query that will look similar to the the below screenshot: This query is asking Prometheus to return data on the total difficulty for each participant node in our network. Prometheus has an integration with Granfana - an observability platform that takes in data - in this case from our private blockchain via Prometheus - and displays the outputs as visualizations. This allows us to make sense of what is occurring in our private network over time. The quickstart manages the connection of the data from Prometheus to pre-built dashboards in Grafana. If you navigate to http://localhost:3000/d/XE4V0WGZz/besu-overview?orgId=1&refresh=10s&from=now-30m&to=now&var-system=All, the metrics from Prometheus will be displayed in a Grafana dashboard, providing us with the metrics in tabular and graphical format. This format makes exploring the data retrieved from Prometheus easier, and allows us to explore many metrics simultaneously. Logs are available via Elasticsearch, Logstash, and Kibana (ELK). ELK allows us to take in process, search, transform, and display data. In the Quickstart, this is configured specifically to ingest the Besu logs. As a reminder, navigating to the terminal and running the script ./list.sh This script will show the various endpoints and services available in the quickstart Quorum Dev Quickstart","title":"!/bin/bash"},{"location":"S02-ethereum/M4-clients-workshop/L1/#setting-up-the-index-patterns-in-kibana","text":"","title":"Setting up the index patterns in kibana ..."},{"location":"S02-ethereum/M4-clients-workshop/L1/#list-endpoints-and-services","text":"JSON-RPC HTTP service endpoint : http://localhost:8545 JSON-RPC WebSocket service endpoint : ws://localhost:8546 Web block explorer address : http://localhost:25000/ Prometheus address : http://localhost:9090/graph Grafana address : http://localhost:3000/d/XE4V0WGZz/besu-overview?orgId=1&refresh=10s&from=now-30m&to=now&var-system=All Collated logs using Kibana endpoint : http://localhost:5601/app/kibana#/discover For more information on the endpoints and services, refer to README.md in the installation directory. Navigating to the Collated logs using Kibana endpoint using your browser http://localhost:5601/app/kibana#/discover This will pull up a Kibana dashboard The logs are pulled automatically by the setup of the Quickstart. For more information about configuring Kibana on your own, you can see an overview within the documentation. Part 6: Deploy a second Smart Contract to the network and send a private transaction Now we are going to take another smart contract and deploy it to our network. This will be a simple smart contract, and the goal of deploying it is to demonstrate the use of private transactions in the network.. This smart contract is already installed as part of the quickstart, and you can find it in the folder smart_contracts. Navigate to the smart_contracts folder using the command cd smart_contracts and you can look at that smart contract EventEmitter.sol: pragma solidity ^0.7.0; // compile with: // solc EventEmitter.sol --bin --abi --optimize --overwrite -o . // then create web3j wrappers with: // web3j solidity generate -b ./generated/EventEmitter.bin -a ./generated/EventEmitter.abi -o ../../../../../ -p org.hyperledger.besu.tests.web3j.generated contract EventEmitter { address owner; event stored(address _to, uint _amount); address _sender; uint _value; constructor () public { owner = msg . sender ; } function store ( uint _amount ) public { emit stored ( msg . sender , _amount ); _value = _amount ; _sender = msg . sender ; } function value () view public returns ( uint ) { return _value ; } function sender () view public returns ( address ) { return _sender ; } } Run the commands npm install node scripts/deploy.js which will deploy the EventEmitter.sol smart contract that is in the contracts folder and send a private transaction from Member1 to Member3 of the network. The private transaction, in this example, is the sending of a value from Member1 to Member3. If we look inside the deploy.js script, we will see that the value being sent from Member1 to Member3 is 47 (see line 89 within the deploy.js file) Alt Text: Private Transaction example output Looking at this output in the terminal, the following has occurred. The smart contract EventEmitter.sol was migrated and deployed to our network. This is confirmed by the following outputs: Getting contractAddress from txHash: 0x022cc35254299433dbda514a979ed7a9a93a45ead383efec4ad5a84e8a817f3e Waiting for transaction to be mined ... Private Transaction Receipt: [object Object] Contract deployed at address: 0xdb6172b4ed41f7039cac4d0be4dbb9992c755809 Once the EventEmitter.sol contract was deployed, Member1 sent a private transaction to Member3. Member1 and Member3 can see the value within the transaction, which has been hashed, as Member1 value from deployed contract is: 0x000000000000000000000000000000000000000000000000000000000000002f Member3 value from deployed contract is: 0x000000000000000000000000000000000000000000000000000000000000002f Member2, which was not included in the private transaction, is unable to see that value, and gets a returned value of 0x. Member2 value from deployed contract is: 0x What has been demonstrated here is that two Members can send transactions between each other, and other members cannot observe the contents of the transaction. For more information, please see the Documentation. Congratulations! You have successfully run through many of the features of Hyperledger Besu. Other Ways to Start Hyperledger Besu Hyperledger Besu starts and is controlled through the command line interface (CLI) of your computer, as seen in this tutorial. If we did not use the quickstart, we would download Hyperledger Besu and run it via the command line. For example, when a Hyperledger Besu node is started up to connect to mainnet, this is done by running the command: besu In the command line. This command is actually calling a bunch of default options and subcommands that tell the client software how to set up the node. When we call the above command, behind the scenes we are actually calling: besu --network=mainnet --datapath=besu --api-gas-price-blocks=100 --api-gas-price-max=500000000000 --api-gas-price-percentile=50.0 --bootnodes=enode://c35c3...d615f@1.2.3.4:30303,enode://f42c13...fc456@1.2.3.5:30303 --config-file=none --discovery-enabled=true --miner-coinbase= --min-gas-price=1000 This is quite a bit longer than running the besu command! What we are trying to illustrate is the fact that besu is very customizable, but we have to use specific syntax (options and subcommands) in the command line in order to customize the node(s) and network created upon starting up Besu. The above is an example of the options and subcommands that apply to creating a full node running proof of work (PoW) consensus for mainnet Ethereum. However, these options and subcommands can also be used to create private and consortium networks. The Hyperledger Besu documentation has all the options and subcommands that you can apply when using Hyperledger Besu. In this article, we are going to explain options and subcommands, and introduce what certain combinations of options and subcommands allow us to do. What is an option? What is a subcommand? When using the CLI, we call commands. These commands are programs (they may also sometimes be classified as scripts) that tell the computer to do a specific set of actions. When Hyperledger Besu is installed on your computer, it creates a global command besu - which tells your computer to run the Besu client with the default options and subcommands we showed above. Let\u2019s walk through some of those in more detail. Options: --network Options are variables that work with the base command of besu. One example option is --network= When we run the besu command, the default network is mainnet, so running besu or besu --network=mainnet accomplishes the same outcome - Hyperledger Besu starts the process of connecting to mainnet. But, we can tell Hyperledger Besu to connect to different networks, including a local testnet on our computer. If we wanted Hyperledger Besu to connect to the testnet Rinkeby, we would use the option: besu --network=rinkeby If we want Besu to start locally on our computer as a private PoW network, the network option becomes besu --network=dev You are now starting to see how an option modifies the command besu. Subcommands: blocks Subcommands are programs that tell the computer to run an additional operation. An example of a subcommand is besu blocks export [--start-block= ] [--end-block= ] --to= This highlighted subcommand tells Hyperledger Besu to export a block or a range of blocks from storage to a file in an RLP format. In practice this would be called like: besu blocks export --start-block=100 --end-block=300 --to=/home/exportblock.bin We have told Hyperledger Besu to export from mainnet the blocks 100 to 300 from mainnet to the location on our computer ~/home/exportblock.bin The subcommand differs from an option because it is a subcommand telling the computer to run another program, whereas an option is telling the computer to modify the parameters of the original command. Subcommands are short lived and give the user functionality related to a specific quick task, and then exit. They do not keep the command running. A subcommand and option are different in the ways they interact with the program. As we saw earlier, an option modifies a program by providing it with certain parameters. The subcommand, on the other hand, is actually telling the program to run another operation entirely.","title":"List endpoints and services"},{"location":"S03-smart-contracts/M2-intro-to-truffle/L1-background/","text":"Introduction to Truffle Suite Before we go into the Solidity section, we want to make sure you have a place where you can play around with the Solidity code you're starting to learn. Enter the Truffle Suite! Truffle is an excellent tool to learn Solidity development but, as we'll show later in the course, it's also got advanced features. Don't be deceived by how easy it is! History and Evolution As with any emergent ecosystem, tooling in the early days of EVM-based dapp development was somewhat primitive. Developers were required to install and utilize multiple, disjointed tools and services making for a complex workflow with a steep learning curve. Things have come a long way since then, with tools and services like Codefi, Infura, Metamask, and OpenZeppelin Contracts that simplify a significant number of smart contract software development lifecycle stages. Despite these advancements, there are still numerous pitfalls, particularly related to the security of your contracts, which makes web3 development different to what you might be used to when building against a more centralized paradigms. Why Truffle Suite? The Truffle Suite was built to streamline the smart contract development process. Out of the box it includes a large, and growing, collection of commands that you can execute as you write, troubleshoot, and maintain your smart contracts. A good example of this is contract compilation (wherein you convert your high-level contract code to something that can be natively understood by an Ethereum-node). As part of this feature, Truffle can also intelligently download the necessary compiler version(s) and even enable you to write your contracts in different language versions. Additional reasons why developers might Truffle to build their dapps include the following... Built-in support for compiling, deploying and linking your contract An automated contract testing framework built on Mocha and Chai A built-in console that allows you to directly interact with your compiled contracts This is just scratching the surface; as you\u2019ll see when we dive-in, the Truffle Suite and the broader tooling ecosystem makes your life as a dapp developer both productive and fun! Installation The following walks you through installation of both Truffle CLI and Ganache. Truffle CLI The Truffle Suite requires the following... Node.js v8.9.4 or later NPM v5.0.3 or later Windows, Linux or Mac OS X Truffle also requires that you have a running Ethereum client which supports the standard JSON RPC API (which is nearly all of them). While there are many clients, the Truffle Suite also ships with Ganache, essentially a one-click EVM-based blockchain node for local testing. Once you have the proper Node and npm installed, please run the following command from your terminal to install Truffle: $ npm install -g truffle Once succesful, this will allow you to run Truffle from your command line anywhere on your machine. Ganache Ganache has the same requirements as Truffle (as specified above). In addition, it also comes in two flavors, both a standalone CLI for more intermediate-advanced users and a UI version which is great for users that are just starting out. It\u2019s worth noting that a version of Ganache also ships directly with Truffle which can be instantiated with the truffle develop command. Ganache CLI can be installed via the following: $ npm install -g ganache-cli Ganache UI is available as download here. You can also install the latest beta (at the time of writing) that includes Filecoin support here. Congratulations! You've just successfully installed Truffle and Ganache and are ready to get started developing. Introducing the Truffle Suite Now that we have Truffle (and optionally a standalone Ganache version) installed we\u2019re nearly ready to begin diving in and writing our first smart contract. We'd like to cover a few more things before diving in. Network Support As we discussed previously in the course, we are living in an increasingly multi-chain world. There multiple popular public blockchain networks and Truffle Suite aims to serve as many of these as is realistic. At the time of writing, Truffle Suite supports development on the following networks: Ethereum Quorum Hyperledger Fabric Corda Filecoin Tezos Polygon Arbritrum Optimism PBC That said, Truffle\u2019s richest support is for that of EVM (Ethereum Virtual Machine) based blockchains. This is in part due to Truffle\u2019s lineage and the fact that supporting every blockchain would be futile, particularly given the rapid evolution of the space. Given the above, the bulk of this section of the course will be specifically focused on EVM based chains (unless specified otherwise). Language Support As highlighted earlier, amongst many other things, Truffle handles the compilation of your contracts from that of a higher-level language to Ethereum bytecode, which is the language \u201cspoken\u201d by the nodes on the network. Out of the box, Truffle supports the following: Solidity Vyper Yul (experimental and not for beginners) At the time of writing, Solidity is by far the most popular language for writing smart contracts, although as with everything in the space, things are moving rapidly and it\u2019s recently there\u2019s been some major projects built using Vyper. Core Truffle Commands Truffle is built around a large collection of commands that you use as part of your contract development workflow. Examples of these include: truffle init truffle compile truffle test truffle debug truffle migrate Note that you can see a complete list of the available commands by running truffle help . As you can likely infer from the commands, they map to key stages of the development lifecycle. More on this in the upcoming section! Truffle Boxes Truffle also provides Boxes, or pre-built templates and Truffle codebases that allow you to focus on either learning more about smart contract development or building quickly. In addition to Truffle code, Truffle Boxes can contain other helpful modules, Solidity contracts & libraries, front-end views and more; all the way up to complete example dapps. Truffle Boxes Up until now we\u2019ve been writing all the code, scripts, and config ourselves and while this follows the mantra of \u201clearning by doing\u201d, there\u2019s another great resource at your disposal and that is Truffle boxes. Learning with Truffle boxes As per the description, boxes are \u201chelpful boilerplates\u201d that comprise of sample contracts, front-end code (using a variety of different frameworks), and applied boxes that focus on a particular theme or protocol such as L2. From a learning standpoint they\u2019re a useful to way to augment your learning by immediately getting hands-on. At the time of writing some of the boxes that would be a great s Layer 2 (examples targeting Optimism, Arbitrum, and Polygon respectively) Filecoin Aave Flashloan example Oracles with ChainLink Installing a box is simply a case of using the unbox command, for example: $ truffle unbox optimism Beyond this, simply follow along with the readme .. We'll discuss boxes more when we dive deeper into developer tooling and more advanced Truffle, but feel free to explore available boxes. Two popular boxes to for folks new to Truffle are Petshop and Metacoin. In the next section, we'll take a simple smart contract and use it to explore the initial commands for developing using Truffle.","title":"Index"},{"location":"S03-smart-contracts/M2-intro-to-truffle/L1-background/#introduction-to-truffle-suite","text":"Before we go into the Solidity section, we want to make sure you have a place where you can play around with the Solidity code you're starting to learn. Enter the Truffle Suite! Truffle is an excellent tool to learn Solidity development but, as we'll show later in the course, it's also got advanced features. Don't be deceived by how easy it is!","title":"Introduction to Truffle Suite"},{"location":"S03-smart-contracts/M2-intro-to-truffle/L1-background/#history-and-evolution","text":"As with any emergent ecosystem, tooling in the early days of EVM-based dapp development was somewhat primitive. Developers were required to install and utilize multiple, disjointed tools and services making for a complex workflow with a steep learning curve. Things have come a long way since then, with tools and services like Codefi, Infura, Metamask, and OpenZeppelin Contracts that simplify a significant number of smart contract software development lifecycle stages. Despite these advancements, there are still numerous pitfalls, particularly related to the security of your contracts, which makes web3 development different to what you might be used to when building against a more centralized paradigms.","title":"History and Evolution"},{"location":"S03-smart-contracts/M2-intro-to-truffle/L1-background/#why-truffle-suite","text":"The Truffle Suite was built to streamline the smart contract development process. Out of the box it includes a large, and growing, collection of commands that you can execute as you write, troubleshoot, and maintain your smart contracts. A good example of this is contract compilation (wherein you convert your high-level contract code to something that can be natively understood by an Ethereum-node). As part of this feature, Truffle can also intelligently download the necessary compiler version(s) and even enable you to write your contracts in different language versions. Additional reasons why developers might Truffle to build their dapps include the following... Built-in support for compiling, deploying and linking your contract An automated contract testing framework built on Mocha and Chai A built-in console that allows you to directly interact with your compiled contracts This is just scratching the surface; as you\u2019ll see when we dive-in, the Truffle Suite and the broader tooling ecosystem makes your life as a dapp developer both productive and fun!","title":"Why Truffle Suite?"},{"location":"S03-smart-contracts/M2-intro-to-truffle/L1-background/#installation","text":"The following walks you through installation of both Truffle CLI and Ganache. Truffle CLI The Truffle Suite requires the following... Node.js v8.9.4 or later NPM v5.0.3 or later Windows, Linux or Mac OS X Truffle also requires that you have a running Ethereum client which supports the standard JSON RPC API (which is nearly all of them). While there are many clients, the Truffle Suite also ships with Ganache, essentially a one-click EVM-based blockchain node for local testing. Once you have the proper Node and npm installed, please run the following command from your terminal to install Truffle: $ npm install -g truffle Once succesful, this will allow you to run Truffle from your command line anywhere on your machine.","title":"Installation"},{"location":"S03-smart-contracts/M2-intro-to-truffle/L1-background/#ganache","text":"Ganache has the same requirements as Truffle (as specified above). In addition, it also comes in two flavors, both a standalone CLI for more intermediate-advanced users and a UI version which is great for users that are just starting out. It\u2019s worth noting that a version of Ganache also ships directly with Truffle which can be instantiated with the truffle develop command. Ganache CLI can be installed via the following: $ npm install -g ganache-cli Ganache UI is available as download here. You can also install the latest beta (at the time of writing) that includes Filecoin support here. Congratulations! You've just successfully installed Truffle and Ganache and are ready to get started developing.","title":"Ganache"},{"location":"S03-smart-contracts/M2-intro-to-truffle/L1-background/#introducing-the-truffle-suite","text":"Now that we have Truffle (and optionally a standalone Ganache version) installed we\u2019re nearly ready to begin diving in and writing our first smart contract. We'd like to cover a few more things before diving in.","title":"Introducing the Truffle Suite"},{"location":"S03-smart-contracts/M2-intro-to-truffle/L1-background/#network-support","text":"As we discussed previously in the course, we are living in an increasingly multi-chain world. There multiple popular public blockchain networks and Truffle Suite aims to serve as many of these as is realistic. At the time of writing, Truffle Suite supports development on the following networks: Ethereum Quorum Hyperledger Fabric Corda Filecoin Tezos Polygon Arbritrum Optimism PBC That said, Truffle\u2019s richest support is for that of EVM (Ethereum Virtual Machine) based blockchains. This is in part due to Truffle\u2019s lineage and the fact that supporting every blockchain would be futile, particularly given the rapid evolution of the space. Given the above, the bulk of this section of the course will be specifically focused on EVM based chains (unless specified otherwise).","title":"Network Support"},{"location":"S03-smart-contracts/M2-intro-to-truffle/L1-background/#language-support","text":"As highlighted earlier, amongst many other things, Truffle handles the compilation of your contracts from that of a higher-level language to Ethereum bytecode, which is the language \u201cspoken\u201d by the nodes on the network. Out of the box, Truffle supports the following: Solidity Vyper Yul (experimental and not for beginners) At the time of writing, Solidity is by far the most popular language for writing smart contracts, although as with everything in the space, things are moving rapidly and it\u2019s recently there\u2019s been some major projects built using Vyper.","title":"Language Support"},{"location":"S03-smart-contracts/M2-intro-to-truffle/L1-background/#core-truffle-commands","text":"Truffle is built around a large collection of commands that you use as part of your contract development workflow. Examples of these include: truffle init truffle compile truffle test truffle debug truffle migrate Note that you can see a complete list of the available commands by running truffle help . As you can likely infer from the commands, they map to key stages of the development lifecycle. More on this in the upcoming section!","title":"Core Truffle Commands"},{"location":"S03-smart-contracts/M2-intro-to-truffle/L1-background/#truffle-boxes","text":"Truffle also provides Boxes, or pre-built templates and Truffle codebases that allow you to focus on either learning more about smart contract development or building quickly. In addition to Truffle code, Truffle Boxes can contain other helpful modules, Solidity contracts & libraries, front-end views and more; all the way up to complete example dapps.","title":"Truffle Boxes"},{"location":"S03-smart-contracts/M2-intro-to-truffle/L1-background/#truffle-boxes_1","text":"Up until now we\u2019ve been writing all the code, scripts, and config ourselves and while this follows the mantra of \u201clearning by doing\u201d, there\u2019s another great resource at your disposal and that is Truffle boxes. Learning with Truffle boxes As per the description, boxes are \u201chelpful boilerplates\u201d that comprise of sample contracts, front-end code (using a variety of different frameworks), and applied boxes that focus on a particular theme or protocol such as L2. From a learning standpoint they\u2019re a useful to way to augment your learning by immediately getting hands-on. At the time of writing some of the boxes that would be a great s Layer 2 (examples targeting Optimism, Arbitrum, and Polygon respectively) Filecoin Aave Flashloan example Oracles with ChainLink Installing a box is simply a case of using the unbox command, for example: $ truffle unbox optimism Beyond this, simply follow along with the readme .. We'll discuss boxes more when we dive deeper into developer tooling and more advanced Truffle, but feel free to explore available boxes. Two popular boxes to for folks new to Truffle are Petshop and Metacoin. In the next section, we'll take a simple smart contract and use it to explore the initial commands for developing using Truffle.","title":"Truffle Boxes"},{"location":"S03-smart-contracts/M3-python/L1-background-and-context/","text":"Python in Ethereum: Background and Context After going through all this Solidity content, (which looks like Javascript but as you now know is very different from Javascript) we may have some Python developers saying, \"What about us?\" Python is, after all, one of the most popular and accessible programming languages. When people want to start programming, they typically feel they have to decide between Javascript or Python. So, what gives? Where's the Python in Ethereum? While it's not as popular as Solidity and Javascript, there are some exciting ways to build on Ethereum using Python. In this section, we're going to go over a few of those projects. In the following section, we'll go over Vyper, the programming language whose syntax is a subset of Python's. Vyper Vyper is an experimental, contract-oriented, pythonic programming language that targets the Ethereum Virtual Machine. Please note: there have been some issues with the Vyper compiler. Like Python, writing clear, understandable code is of primary importance in Vyper. Vyper is not trying to be a replacement for Solidity. It is meant to be a more security focused smart contract programming language and will likely not be able to do everything that Solidity can. Its development is uncertain and not nearly as strong as Solidity. You can read more about Vyper in this section here. Note that you can use Vyper with a number of development frameworks, including Truffle and Brownie. Brownie and Web3.py Brownie is a Python-based development and testing framework for smart contracts running on the EVM. It uses Web3.py as well as Solidity. It is most well-known for being the development framework the Yearn.Finance team uses to build their powerful DeFi platform and CRV. Brownie definitely takes some notes from Truffle (they are both \"sweet\"){target=_blank}, including having a brownie init command and their equivalent of Truffle boxes, \"Brownie Mixes.\" This makes it an easy tool for a lot of Truffle developers familiar with Truffle. Here are some tutorials to introduce you to Brownie: Tutorial: Develop a DeFi Project using Python (Chainlink){target=_blank} Tutorial: Vyper and Brownie Contract Development on EVM Chains An interesting tutorial walking through developing a contract using Vyper and Brownie. Yellow Paper rewrite Piper Merriam is a Python-based Ethereum developer who works for the Ethereum Foundation. They have proposed an interesting project : Replacing Ethereum's yellow paper (which we studied earlier) with executable markdown. This was inspired by Ethereum 2.0's Beacon chain specifications. Rather than writing a document of specifications, the Ethereum 2.0 developer teams built a series of tests based on the spec conditions of a theoretical Beacon chain. Then, Ethereum 2.0 software clients only had to make sure they passed those tests. Merriam is proposing a similar exercise, but for Ethereum Mainnet (the PoW chain), based in Python. The goal would be to replace the yellow paper, which can be very challenging to read. It would rely heavily on py-EVM (a Python implementation of the EVM). The whole project is very new and you can check it's Github repository for updates. Maybe you can even get involved! Fe Fe is \"an emerging smart contract language for the Ethereum blockchain.\" It's influenced both by Python and Rust and was released in January 2021. You can read more about it here. To be honest, it looks more like Rust than Python, but that's just our opinion. Conclusion While more niche, it's possible to be a Python developer and contribute to the Ethereum (and wider blockchain) ecosystem! Solidity is still important to know, but we hope this gets you started finding more Python opportunities.","title":"Python in Ethereum: Background and Context"},{"location":"S03-smart-contracts/M3-python/L1-background-and-context/#python-in-ethereum-background-and-context","text":"After going through all this Solidity content, (which looks like Javascript but as you now know is very different from Javascript) we may have some Python developers saying, \"What about us?\" Python is, after all, one of the most popular and accessible programming languages. When people want to start programming, they typically feel they have to decide between Javascript or Python. So, what gives? Where's the Python in Ethereum? While it's not as popular as Solidity and Javascript, there are some exciting ways to build on Ethereum using Python. In this section, we're going to go over a few of those projects. In the following section, we'll go over Vyper, the programming language whose syntax is a subset of Python's.","title":"Python in Ethereum: Background and Context"},{"location":"S03-smart-contracts/M3-python/L1-background-and-context/#vyper","text":"Vyper is an experimental, contract-oriented, pythonic programming language that targets the Ethereum Virtual Machine. Please note: there have been some issues with the Vyper compiler. Like Python, writing clear, understandable code is of primary importance in Vyper. Vyper is not trying to be a replacement for Solidity. It is meant to be a more security focused smart contract programming language and will likely not be able to do everything that Solidity can. Its development is uncertain and not nearly as strong as Solidity. You can read more about Vyper in this section here. Note that you can use Vyper with a number of development frameworks, including Truffle and Brownie.","title":"Vyper"},{"location":"S03-smart-contracts/M3-python/L1-background-and-context/#brownie-and-web3py","text":"Brownie is a Python-based development and testing framework for smart contracts running on the EVM. It uses Web3.py as well as Solidity. It is most well-known for being the development framework the Yearn.Finance team uses to build their powerful DeFi platform and CRV. Brownie definitely takes some notes from Truffle (they are both \"sweet\"){target=_blank}, including having a brownie init command and their equivalent of Truffle boxes, \"Brownie Mixes.\" This makes it an easy tool for a lot of Truffle developers familiar with Truffle. Here are some tutorials to introduce you to Brownie: Tutorial: Develop a DeFi Project using Python (Chainlink){target=_blank} Tutorial: Vyper and Brownie Contract Development on EVM Chains An interesting tutorial walking through developing a contract using Vyper and Brownie.","title":"Brownie and Web3.py"},{"location":"S03-smart-contracts/M3-python/L1-background-and-context/#yellow-paper-rewrite","text":"Piper Merriam is a Python-based Ethereum developer who works for the Ethereum Foundation. They have proposed an interesting project : Replacing Ethereum's yellow paper (which we studied earlier) with executable markdown. This was inspired by Ethereum 2.0's Beacon chain specifications. Rather than writing a document of specifications, the Ethereum 2.0 developer teams built a series of tests based on the spec conditions of a theoretical Beacon chain. Then, Ethereum 2.0 software clients only had to make sure they passed those tests. Merriam is proposing a similar exercise, but for Ethereum Mainnet (the PoW chain), based in Python. The goal would be to replace the yellow paper, which can be very challenging to read. It would rely heavily on py-EVM (a Python implementation of the EVM). The whole project is very new and you can check it's Github repository for updates. Maybe you can even get involved!","title":"Yellow Paper rewrite"},{"location":"S03-smart-contracts/M3-python/L1-background-and-context/#fe","text":"Fe is \"an emerging smart contract language for the Ethereum blockchain.\" It's influenced both by Python and Rust and was released in January 2021. You can read more about it here. To be honest, it looks more like Rust than Python, but that's just our opinion.","title":"Fe"},{"location":"S03-smart-contracts/M3-python/L1-background-and-context/#conclusion","text":"While more niche, it's possible to be a Python developer and contribute to the Ethereum (and wider blockchain) ecosystem! Solidity is still important to know, but we hope this gets you started finding more Python opportunities.","title":"Conclusion"},{"location":"S03-smart-contracts/M4-design-patterns/L10-proof-of-existence/","text":"Writing a Smart Contract (Proof of Existence Exercise) This walkthrough is based on this Medium post by Manuel Araoz. What is Proof of Existence? Proof of Existence is a service that verifies the existence of a document or file at a specific time via timestamped transactions. PoE utilizes the one-way nature of cryptographic hash functions to reduce the amount of information that needs to be stored on the blockchain. In this walkthrough, we are going to introduce some basic Ethereum smart contract development best practices by creating a simple proof of existence contract and interacting with it on the blockchain. Set up the Environment Connect to a Blockchain To develop Ethereum applications, you will need a client to connect to an Ethereum blockchain. You can use Geth , Parity or a development blockchain such as Ganache . In this walkthrough we will be using the Ganache command line interface, ganache-cli. You can find the documentation on ganache-cli here . You can install ganache-cli with the following command: $ sudo npm install -g ganache-cli And start ganache-cli with $ ganache-cli When ganache-cli starts, you can see that it starts with 10 accounts. Each of those accounts comes prefunded with Ether so we do not need to mine or acquire funds from a faucet. Ganache-cli starts a development blockchain that needs to be running while developing the application so leave it running while we continue working on the application. Truffle Development Framework Solidity is the most popular programming language for writing smart contracts in Ethereum. We will be using Solidity throughout this course. The Truffle development framework is one of the most popular development tools for writing Solidity smart contracts for Ethereum. Truffle will help us compile, deploy and test our smart contracts once they are written. To install truffle $ sudo npm install -g truffle $ sudo npm install -g @truffle/hdwallet-provider Create a project directory and set up truffle $ mkdir proof-of-existence $ cd proof-of-existence/ $ truffle init Truffle sets up a contracts directory where we will write our contracts, a migrations directory where we will write scripts to deploy our contracts along with a test directory where we will write tests to make sure that our contracts work as expected. In truffle-config.js we need to specify the network that we will be using. In the module.exports object, add the following information: Note: This information is cooked in when we initialized truffle, head over the truffle.config.js file, and view the commented out code under networks. module . exports = { networks : { development : { host : 'localhost' , port : 8545 , network_id : '*' } } }; We use this information because ganache-cli is running our development blockchain on localhost:8545, and this is what we want to connect to. Specifying network_id as * means that any truffle will deploy to any network running at localhost:8545. Truffle comes with a Migrations.sol contract that keeps track of our migrations as well as a 1_initial_migrations.js script to deploy the Migrations.sol contract. Run the following command in the terminal in your project directory: $ truffle migrate The terminal should print information about the deployment of the Migrations contract. This means that your environment is set up correctly and we can move on to writing our smart contracts. Writing the Smart Contract Run the following command in the terminal in your project directory: $ truffle create contract ProofOfExistence1 This command will create a Solidity file in the contracts directory called \u201cProofOfExistence1.sol\u201d and set up the boilerplate code for the contract, a contract definition along with a contract constructor. Open ProofOfExistence.sol in your text editor. Update your ProofOfExistence1.sol file so that it looks like this . We are starting with something simple, but incorrect and we are going to work towards a better contract. Our contract has a state and two functions. This contract actually has two different kinds of functions, a transactional function (notarize) and a read-only, or view , function (proofFor). Transactional functions can modify state whereas constant functions can only read the state and return values. Let\u2019s deploy this contract to our test network. Create a new migrations file in the migrations directory called 2_deploy_contracts.js. In this file add the following: const ProofOfExistence1 = artifacts . require ( './ProofOfExistence1.sol' ); module . exports = function ( deployer ) { deployer . deploy ( ProofOfExistence1 ); }; This script tells Truffle to get the contract information from ProofOfExitence.sol and deploy it to the specified network. Now we just need to tell Truffle to run the deployment. Run the following command in the terminal in your project directory: $ truffle migrate In the terminal output you should see that Truffle compiled ProofOfExistence.sol and printed some warning regarding the contract. Then it runs the migrations using the network \u2018development\u2019 that we specified in truffle-config.js. Truffle remembers which contracts it has migrated to the network, so if we want to run the migration again on the same network, we need to use the --reset option like so: $ truffle migrate --reset You can find more information about truffle migrations here . Interacting with your Smart Contract Our contract is now on the development blockchain, so we can interact with it. We can read the contract state from the blockchain and update the state by calling the notarize function. We can do this using the Truffle console. Bring up the truffle console with the command: $ truffle console You should see truffle(development)> On the first line enter: const poe = await ProofOfExistence1 . at ( ProofOfExistence1 . address ) This line says that the variable \u201cpoe\u201d is an instance of ProofOfExistence1.sol found at the address that we just deployed. You can see the address by entering truffle(development)> poe.address '0xc490df1850010ea8146c1dd3e961fedbf6b85bef' To call the notarize function, we call it like any other javascript function. truffle(development)> poe.notarize('Hello World!') { tx: '0x60ae...2643cbea65', receipt: \u2026 } This function causes a state change, so it is a transactional function. Transactional functions return a Promise that resolves to a transaction object. We can get the proof for the string with truffle(development)> poe.proofFor('Hello World!') \u20180x7f83b...126d9069\u2019 And check that the contract\u2019s state was correctly changed truffle(development)> poe.proof() '0x7f83b...126d9069' The hashes match! Iterating the code Our contract works! But it can only store one proof at a time. Let\u2019s change that. Exit the Truffle console and create a new file called ProofOfExistence2.sol: truffle ( development ) > . exit $ truffle create contract ProofOfExistence2 Update the ProofOfExistence2 contract to match this contract . The main changes between the first version and this version are that we changed the \u201cproof\u201d variable to a bytes32 array called \u201cproofs\u201d and made it private. We also added a function called \u201chasProof\u201d to check if a proof has already been stored in the array. Update the migration script 2_deploy_contracts.js to deploy the new contract and deploy it to the development blockchain. $ truffle migrate --reset Launch the console to interact with the new contract. $ truffle console Save the deployed contract truffle ( development ) > const poe = await ProofOfExistence2 . at ( ProofOfExistence2 . address ) We can check for a proof. truffle(development)> poe.checkDocument('Hello World!') false It returns false because we haven\u2019t added anything yet. Let\u2019s do that now. truffle(development)> poe.notarize('Hello World!') { tx: '0xd6f72...10a6e', receipt: { transactionHash: ... logs: [] } truffle(development)> poe.checkDocument('Hello World!') true We can check to make sure that our contract will store multiple proofs. truffle(development)> poe.notarize('Hello Consensys!') { tx: '0x8b566...091ace', receipt: { transactionHash: ... logs: [] } truffle(development)> poe.checkDocument('Hello Consensys!') True Looping over arrays in smart contracts can get expensive as arrays get longer. Using a mapping is a better solution. Let\u2019s create a final version in ProofOfExistence3.sol using mappings. You can use this code for the contract. Modify the deployment script to deploy the new contract and test it in the console to make sure that it behaves just like ProofOfExistence2.sol. Deploying to the Testnet From here, we are going to deviate from the Zeppelin Solutions walkthrough. If you want to learn how to deploy contracts using the geth console (which requires syncing with the testnet), you can consult the Zeppelin Solutions walkthrough. The first step in our alternate deployment method is to get an Ethereum account on Metamask. On the landing page, click \u201cGet Chrome Extension.\u201d Once the extension is installed, accept the terms of use and enter a password for your account. You will be shown 12 words that can be used to restore your wallet. A word of caution, do NOT publish these words anywhere public. Anyone that has these 12 words has access to your wallet. Save these 12 words in a file called '.secret' in your project directory. Note: They do not need to be contained within a string when adding to .secret, line 25 in truffle config assists with this detail. With Truffle version 5, the truffle-config.js file comes populated with a lot of configuration settings that are commented out. // truffle - config . js 21 const HDWallet = require ( '@truffle/hdwallet-provider' ); 22 const infuraURL = 'https://rinkeby.infura.io/v3/<Project_ID>' <-- project id here 23 24 const fs = require ( 'fs' ); 25 const mnemonic = fs . readFileSync ( '.secret' ) . toString () . trim (); Uncomment lines 21 - 25. Line 21 will import the tool to derive a private key and address from a mnemonic. Entering your Infura API key in line 22 will allow you to easily deploy contracts to Ethereum (we will cover how to get an API key shortly). Lines 24 and 25 will import your seed phrase (from the .secret file) into the file. Truffle will use these words to access your wallet and deploy the contract. Deploying the contract requires us to make a transaction on the testnet, so we need some ether to pay for the transaction. You can get free Rinkeby ether by going to this website and following the instructions. Make sure you enter the Ethereum address for your 1st Metamask account. Now that we have a testnet account with Ether, we need to configure Truffle to be able to deploy the contract. To deploy contracts to the testnet using Truffle without having to sync a local node, you can use Infura. Infura allows you to access a fully synced Ethereum node via their API. We will use their API to deploy our contracts to the Rinkeby testnet. Go to the Infura website and sign up for a free account. Save the Rinkeby test network URL that Infura provides in a variable called infuraURL in truffle-config.js. Your Project ID is a unique code provided by Infura. // truffle - config . js const infuraURL = 'https://rinkeby.infura.io/v3/<Project_ID>' <-- project id here For Truffle to derive our ethereum address from the mnemonic, we need to install the Truffle HD wallet provider. In the terminal located in the proof-of-existence project root run: npm install @truffle / hdwallet - provider And import the HD wallet provider into your truffle-config.js file, ensure you have the following. const HDWallet = require ( '@truffle/hdwallet-provider' ) Now we just need to add the rinkeby network configuration to the networks object in truffle-config.js module.exports. Change the module.exports object to resemble this: networks: { development: { host: 'localhost', port: 8545, network_id: '*' }, rinkeby: { provider: () => new HDWalletProvider(mnemonic, infuraURL), network_id: 4, // Rinkeby's network id gas: 5500000, }, } Where the \u201cmnemonic\u201d variable is the 12 word seed phrase that you saved from metamask and the \u201cinfuraKey\u201d variable is your Infura Project ID. You just have to run \u201ctruffle migrate\u201d for the correct network and your contract will be deployed! $ truffle migrate --network rinkeby Starting migrations... ====================== > Network name: 'rinkeby' > Network id: 4 > Block gas limit: 7602728 1_initial_migration.js ====================== Deploying 'Migrations' ---------------------- > transaction hash: 0x6c5cc0090db1147dfadfd4a84280715e8e8bb3bb820401e32ea40b4f6e521178 > Blocks: 0 Seconds: 12 > contract address: 0x8e8786683C33147dA500c3F54A9690A4f81a3D48 > account: 0x196150D99a325f624F7c420Cf80ca6ab9a1BeDBA > balance: 0 .987611032134329654 > gas used: 284908 > gas price: 20 gwei > value sent: 0 ETH > total cost: 0 .00569816 ETH > Saving migration to chain. > Saving artifacts ------------------------------------- > Total cost: 0 .00569816 ETH 2_deploy_contracts.js ===================== Deploying 'ProofOfExistence2' ----------------------------- > transaction hash: 0x4692cc0311571c70f7d425f9a89702a355abe8345961c871980a279c8b16e20a > Blocks: 1 Seconds: 12 > contract address: 0x096c0636F39372Fa83EF498bCC5Bce86C8fd1Fb5 > account: 0x196150D99a325f624F7c420Cf80ca6ab9a1BeDBA > balance: 0 .978713032134329654 > gas used: 402866 > gas price: 20 gwei > value sent: 0 ETH > total cost: 0 .00805732 ETH > Saving migration to chain. > Saving artifacts ------------------------------------- > Total cost: 0 .00805732 ETH Summary ======= > Total deployments: 2 > Final cost: 0 .01375548 ETH The terminal prints the addresses of the deployed contracts as well as the transaction hashes of the deployment transactions. This information can also be referenced in the contract artifacts, which are stored in proof-of-existence/build/contracts/. Deployment information is found at the bottom of each JSON file. You can view your deployed contracts at https://rinkeby.etherscan.io/ and paste in the deployed contract address to have a look you yourself, you can now interact with the deployed contract on the Rinkeby test network!","title":"Writing a Smart Contract (Proof of Existence Exercise)"},{"location":"S03-smart-contracts/M4-design-patterns/L10-proof-of-existence/#writing-a-smart-contract-proof-of-existence-exercise","text":"This walkthrough is based on this Medium post by Manuel Araoz.","title":"Writing a Smart Contract (Proof of Existence Exercise)"},{"location":"S03-smart-contracts/M4-design-patterns/L10-proof-of-existence/#what-is-proof-of-existence","text":"Proof of Existence is a service that verifies the existence of a document or file at a specific time via timestamped transactions. PoE utilizes the one-way nature of cryptographic hash functions to reduce the amount of information that needs to be stored on the blockchain. In this walkthrough, we are going to introduce some basic Ethereum smart contract development best practices by creating a simple proof of existence contract and interacting with it on the blockchain.","title":"What is Proof of Existence?"},{"location":"S03-smart-contracts/M4-design-patterns/L10-proof-of-existence/#set-up-the-environment","text":"","title":"Set up the Environment"},{"location":"S03-smart-contracts/M4-design-patterns/L10-proof-of-existence/#connect-to-a-blockchain","text":"To develop Ethereum applications, you will need a client to connect to an Ethereum blockchain. You can use Geth , Parity or a development blockchain such as Ganache . In this walkthrough we will be using the Ganache command line interface, ganache-cli. You can find the documentation on ganache-cli here . You can install ganache-cli with the following command: $ sudo npm install -g ganache-cli And start ganache-cli with $ ganache-cli When ganache-cli starts, you can see that it starts with 10 accounts. Each of those accounts comes prefunded with Ether so we do not need to mine or acquire funds from a faucet. Ganache-cli starts a development blockchain that needs to be running while developing the application so leave it running while we continue working on the application.","title":"Connect to a Blockchain"},{"location":"S03-smart-contracts/M4-design-patterns/L10-proof-of-existence/#truffle-development-framework","text":"Solidity is the most popular programming language for writing smart contracts in Ethereum. We will be using Solidity throughout this course. The Truffle development framework is one of the most popular development tools for writing Solidity smart contracts for Ethereum. Truffle will help us compile, deploy and test our smart contracts once they are written. To install truffle $ sudo npm install -g truffle $ sudo npm install -g @truffle/hdwallet-provider Create a project directory and set up truffle $ mkdir proof-of-existence $ cd proof-of-existence/ $ truffle init Truffle sets up a contracts directory where we will write our contracts, a migrations directory where we will write scripts to deploy our contracts along with a test directory where we will write tests to make sure that our contracts work as expected. In truffle-config.js we need to specify the network that we will be using. In the module.exports object, add the following information: Note: This information is cooked in when we initialized truffle, head over the truffle.config.js file, and view the commented out code under networks. module . exports = { networks : { development : { host : 'localhost' , port : 8545 , network_id : '*' } } }; We use this information because ganache-cli is running our development blockchain on localhost:8545, and this is what we want to connect to. Specifying network_id as * means that any truffle will deploy to any network running at localhost:8545. Truffle comes with a Migrations.sol contract that keeps track of our migrations as well as a 1_initial_migrations.js script to deploy the Migrations.sol contract. Run the following command in the terminal in your project directory: $ truffle migrate The terminal should print information about the deployment of the Migrations contract. This means that your environment is set up correctly and we can move on to writing our smart contracts.","title":"Truffle Development Framework"},{"location":"S03-smart-contracts/M4-design-patterns/L10-proof-of-existence/#writing-the-smart-contract","text":"Run the following command in the terminal in your project directory: $ truffle create contract ProofOfExistence1 This command will create a Solidity file in the contracts directory called \u201cProofOfExistence1.sol\u201d and set up the boilerplate code for the contract, a contract definition along with a contract constructor. Open ProofOfExistence.sol in your text editor. Update your ProofOfExistence1.sol file so that it looks like this . We are starting with something simple, but incorrect and we are going to work towards a better contract. Our contract has a state and two functions. This contract actually has two different kinds of functions, a transactional function (notarize) and a read-only, or view , function (proofFor). Transactional functions can modify state whereas constant functions can only read the state and return values. Let\u2019s deploy this contract to our test network. Create a new migrations file in the migrations directory called 2_deploy_contracts.js. In this file add the following: const ProofOfExistence1 = artifacts . require ( './ProofOfExistence1.sol' ); module . exports = function ( deployer ) { deployer . deploy ( ProofOfExistence1 ); }; This script tells Truffle to get the contract information from ProofOfExitence.sol and deploy it to the specified network. Now we just need to tell Truffle to run the deployment. Run the following command in the terminal in your project directory: $ truffle migrate In the terminal output you should see that Truffle compiled ProofOfExistence.sol and printed some warning regarding the contract. Then it runs the migrations using the network \u2018development\u2019 that we specified in truffle-config.js. Truffle remembers which contracts it has migrated to the network, so if we want to run the migration again on the same network, we need to use the --reset option like so: $ truffle migrate --reset You can find more information about truffle migrations here .","title":"Writing the Smart Contract"},{"location":"S03-smart-contracts/M4-design-patterns/L10-proof-of-existence/#interacting-with-your-smart-contract","text":"Our contract is now on the development blockchain, so we can interact with it. We can read the contract state from the blockchain and update the state by calling the notarize function. We can do this using the Truffle console. Bring up the truffle console with the command: $ truffle console You should see truffle(development)> On the first line enter: const poe = await ProofOfExistence1 . at ( ProofOfExistence1 . address ) This line says that the variable \u201cpoe\u201d is an instance of ProofOfExistence1.sol found at the address that we just deployed. You can see the address by entering truffle(development)> poe.address '0xc490df1850010ea8146c1dd3e961fedbf6b85bef' To call the notarize function, we call it like any other javascript function. truffle(development)> poe.notarize('Hello World!') { tx: '0x60ae...2643cbea65', receipt: \u2026 } This function causes a state change, so it is a transactional function. Transactional functions return a Promise that resolves to a transaction object. We can get the proof for the string with truffle(development)> poe.proofFor('Hello World!') \u20180x7f83b...126d9069\u2019 And check that the contract\u2019s state was correctly changed truffle(development)> poe.proof() '0x7f83b...126d9069' The hashes match!","title":"Interacting with your Smart Contract"},{"location":"S03-smart-contracts/M4-design-patterns/L10-proof-of-existence/#iterating-the-code","text":"Our contract works! But it can only store one proof at a time. Let\u2019s change that. Exit the Truffle console and create a new file called ProofOfExistence2.sol: truffle ( development ) > . exit $ truffle create contract ProofOfExistence2 Update the ProofOfExistence2 contract to match this contract . The main changes between the first version and this version are that we changed the \u201cproof\u201d variable to a bytes32 array called \u201cproofs\u201d and made it private. We also added a function called \u201chasProof\u201d to check if a proof has already been stored in the array. Update the migration script 2_deploy_contracts.js to deploy the new contract and deploy it to the development blockchain. $ truffle migrate --reset Launch the console to interact with the new contract. $ truffle console Save the deployed contract truffle ( development ) > const poe = await ProofOfExistence2 . at ( ProofOfExistence2 . address ) We can check for a proof. truffle(development)> poe.checkDocument('Hello World!') false It returns false because we haven\u2019t added anything yet. Let\u2019s do that now. truffle(development)> poe.notarize('Hello World!') { tx: '0xd6f72...10a6e', receipt: { transactionHash: ... logs: [] } truffle(development)> poe.checkDocument('Hello World!') true We can check to make sure that our contract will store multiple proofs. truffle(development)> poe.notarize('Hello Consensys!') { tx: '0x8b566...091ace', receipt: { transactionHash: ... logs: [] } truffle(development)> poe.checkDocument('Hello Consensys!') True Looping over arrays in smart contracts can get expensive as arrays get longer. Using a mapping is a better solution. Let\u2019s create a final version in ProofOfExistence3.sol using mappings. You can use this code for the contract. Modify the deployment script to deploy the new contract and test it in the console to make sure that it behaves just like ProofOfExistence2.sol.","title":"Iterating the code"},{"location":"S03-smart-contracts/M4-design-patterns/L10-proof-of-existence/#deploying-to-the-testnet","text":"From here, we are going to deviate from the Zeppelin Solutions walkthrough. If you want to learn how to deploy contracts using the geth console (which requires syncing with the testnet), you can consult the Zeppelin Solutions walkthrough. The first step in our alternate deployment method is to get an Ethereum account on Metamask. On the landing page, click \u201cGet Chrome Extension.\u201d Once the extension is installed, accept the terms of use and enter a password for your account. You will be shown 12 words that can be used to restore your wallet. A word of caution, do NOT publish these words anywhere public. Anyone that has these 12 words has access to your wallet. Save these 12 words in a file called '.secret' in your project directory. Note: They do not need to be contained within a string when adding to .secret, line 25 in truffle config assists with this detail. With Truffle version 5, the truffle-config.js file comes populated with a lot of configuration settings that are commented out. // truffle - config . js 21 const HDWallet = require ( '@truffle/hdwallet-provider' ); 22 const infuraURL = 'https://rinkeby.infura.io/v3/<Project_ID>' <-- project id here 23 24 const fs = require ( 'fs' ); 25 const mnemonic = fs . readFileSync ( '.secret' ) . toString () . trim (); Uncomment lines 21 - 25. Line 21 will import the tool to derive a private key and address from a mnemonic. Entering your Infura API key in line 22 will allow you to easily deploy contracts to Ethereum (we will cover how to get an API key shortly). Lines 24 and 25 will import your seed phrase (from the .secret file) into the file. Truffle will use these words to access your wallet and deploy the contract. Deploying the contract requires us to make a transaction on the testnet, so we need some ether to pay for the transaction. You can get free Rinkeby ether by going to this website and following the instructions. Make sure you enter the Ethereum address for your 1st Metamask account. Now that we have a testnet account with Ether, we need to configure Truffle to be able to deploy the contract. To deploy contracts to the testnet using Truffle without having to sync a local node, you can use Infura. Infura allows you to access a fully synced Ethereum node via their API. We will use their API to deploy our contracts to the Rinkeby testnet. Go to the Infura website and sign up for a free account. Save the Rinkeby test network URL that Infura provides in a variable called infuraURL in truffle-config.js. Your Project ID is a unique code provided by Infura. // truffle - config . js const infuraURL = 'https://rinkeby.infura.io/v3/<Project_ID>' <-- project id here For Truffle to derive our ethereum address from the mnemonic, we need to install the Truffle HD wallet provider. In the terminal located in the proof-of-existence project root run: npm install @truffle / hdwallet - provider And import the HD wallet provider into your truffle-config.js file, ensure you have the following. const HDWallet = require ( '@truffle/hdwallet-provider' ) Now we just need to add the rinkeby network configuration to the networks object in truffle-config.js module.exports. Change the module.exports object to resemble this: networks: { development: { host: 'localhost', port: 8545, network_id: '*' }, rinkeby: { provider: () => new HDWalletProvider(mnemonic, infuraURL), network_id: 4, // Rinkeby's network id gas: 5500000, }, } Where the \u201cmnemonic\u201d variable is the 12 word seed phrase that you saved from metamask and the \u201cinfuraKey\u201d variable is your Infura Project ID. You just have to run \u201ctruffle migrate\u201d for the correct network and your contract will be deployed! $ truffle migrate --network rinkeby Starting migrations... ====================== > Network name: 'rinkeby' > Network id: 4 > Block gas limit: 7602728 1_initial_migration.js ====================== Deploying 'Migrations' ---------------------- > transaction hash: 0x6c5cc0090db1147dfadfd4a84280715e8e8bb3bb820401e32ea40b4f6e521178 > Blocks: 0 Seconds: 12 > contract address: 0x8e8786683C33147dA500c3F54A9690A4f81a3D48 > account: 0x196150D99a325f624F7c420Cf80ca6ab9a1BeDBA > balance: 0 .987611032134329654 > gas used: 284908 > gas price: 20 gwei > value sent: 0 ETH > total cost: 0 .00569816 ETH > Saving migration to chain. > Saving artifacts ------------------------------------- > Total cost: 0 .00569816 ETH 2_deploy_contracts.js ===================== Deploying 'ProofOfExistence2' ----------------------------- > transaction hash: 0x4692cc0311571c70f7d425f9a89702a355abe8345961c871980a279c8b16e20a > Blocks: 1 Seconds: 12 > contract address: 0x096c0636F39372Fa83EF498bCC5Bce86C8fd1Fb5 > account: 0x196150D99a325f624F7c420Cf80ca6ab9a1BeDBA > balance: 0 .978713032134329654 > gas used: 402866 > gas price: 20 gwei > value sent: 0 ETH > total cost: 0 .00805732 ETH > Saving migration to chain. > Saving artifacts ------------------------------------- > Total cost: 0 .00805732 ETH Summary ======= > Total deployments: 2 > Final cost: 0 .01375548 ETH The terminal prints the addresses of the deployed contracts as well as the transaction hashes of the deployment transactions. This information can also be referenced in the contract artifacts, which are stored in proof-of-existence/build/contracts/. Deployment information is found at the bottom of each JSON file. You can view your deployed contracts at https://rinkeby.etherscan.io/ and paste in the deployed contract address to have a look you yourself, you can now interact with the deployed contract on the Rinkeby test network!","title":"Deploying to the Testnet"},{"location":"S03-smart-contracts/M4-design-patterns/L4-oracles/","text":"Oracles In our discussion blockchain primitives, we discussed how public blockchain networks coordinate these powerful tools to create a network state that can be established and maintained by network participants. However, that network state only holds true within the boundaries of the network itself. We can only guarantee the state transitions of the network participants running our blockchain client software. What about data that exists outside this network, specifically what about \"real world\" data, such as weather reports, incident details or stock prices? Additionally, what if we'd like to interact with external computation in the outside world? A machine learning algorithm, a random number generator, or an event-driven task automation would all be examples of external computation that would be essential to building feature rich smart contracts. Such data and external computation exists but they are beyond the trust boundary established by our blockchain protocols. One way blockchain developers have tried to bridge the gap is through what are called off-chain oracles. Off-chain oracles are agents that find and verify real-world information and submit them to the blockchain to be used by smart contracts. They can trigger smart contract executions when the data is obtained or predefined conditions are meet (e.g. time, weather, tracking, payments). Off-chain oracles are provided by organizations like Provable Things, Chainlink Labs, and more. Please note that DeFi has led to on-chain oracles, particularly dealing with token prices, which we will discuss more in the section on DeFi. Such examples of on-chain oracles are Chainlink's Data Feeds , Uniswap's [Observations](https://docs.uniswap.org/protocol/concepts/V3-overview/oracle){target=_blank} , and MakerDAO's Feeds . A smart contract using oracles in some facet is often referred to as a Hybrid Smart Contract . Trust It's extremely important to understand oracles like the ones we're discussing here can require trust. Understanding where the data is coming from is essential to running a decentralized smart contract. When using oracles, it's important to build in smart contract mechanisms that can anticipate errors or possible corruption, as oracles can succumb to many attack vectors. Having a single oracle or a single data source delivering your data or executing your computation creates a single source of failure for your smart contract, and means your decentralized application isn't actually decentralized. You'll want to make sure the oracle network you're working with is executing in as decentralized a context as possible. For example, if you want pricing data, you get the pricing data from many exchanges, and many independent blockchain oracles deliver this data. It's important to know, that just setting up an oracle service yourself can mean your application is centralized, and possibly doesn't have the highest quality data or execution. So we want to avoid setting these up ourselves, but also do our due diligence to make sure the system we are working with is decentralized. Services such as Chainlink have built more decentralized networks to hedge against the centralization of trust, you can read more about how Chainlink does that here and see an overview of decentralized oracles here. Basic Oracle Mechanism At its most basic, a smart contract contract using an oracle needs to implement a method to: Make the request to the oracle, and Receive the oracle's response from a callback method This is often known as the request and receive model of oracles. Unless you've setup the service yourself, external calls to oracles typically require a fee attached to provide the data to your contract. There are multiple ways to achieve this, let's look at one generalized code using Provable below: import \"github.com/oraclize/ethereum-api/provableAPI.sol\"; contract DieselPrice is usingProvable { uint DieselPriceUSD; constructor() public { provable_query(\"URL\", \"xml(https://www.fueleconomy.gov/ws/rest/fuelprices).fuelPrices.diesel\"); } function __callback (bytes32 myid, string result) public { require(msg.sender == provable_cbAddress()); DieselPriceUSD = parseInt(result); } } In this code, we're calling the Provable API for the price of diesel when we create the contract. The first query is free, but we'll have to provide ETH to pay for our requests moving forward. The call triggers an event, which lets the Provable contract pull from its off-chain datafeed and provide our contract the result. The contract stores that value in DieselPriceUSD . The overall model is for your contract to emit an event, either to another contract or simply in the block its created. The oracle service will detect that event, pull the desired data, and respond back to your contract. The oracle services require you to have a standard method, like __callback , that its transaction can target when responding to your oracle query. Decentralized Oracle Mechanism In the above example, we looked at pulling data from a single source and provider. Stringing requests like this together can make us have a more decentralized application, but we would still be routing it though the same organization. Let's look at another example of pulling data in a decentralized context, for example, from a Chainlink data feed. You can see the most up to date version of this code in the Chainlink documentation. pragma solidity ^0.6.7; import \"@chainlink/contracts/src/v0.6/interfaces/AggregatorV3Interface.sol\"; contract PriceConsumerV3 { AggregatorV3Interface internal priceFeed; /** * Network: Kovan * Aggregator: ETH/USD * Address: 0x9326BFA02ADD2366b30bacB125260Af641031331 */ constructor() public { priceFeed = AggregatorV3Interface(0x9326BFA02ADD2366b30bacB125260Af641031331); } /** * Returns the latest price */ function getThePrice() public view returns (int) { ( uint80 roundID, int price, uint startedAt, uint timeStamp, uint80 answeredInRound ) = priceFeed.latestRoundData(); return price; } } In this example, we are pulling the price of Ethereum in terms of the United States Dollar into our smart contract. We do this by querying another contract that has already followed the basic request model among many different oracles and data providers. It's important that the network is using different data providers and oracles so that there is never a single authority point. You can see a visualization of this contract and the different nodes gathering the data for them at data.chain.link. Summary The oracle ecosystem is growing as fast as blockchain, and we'll touch on it more later in the course. For now, it's important to see how oracles allow us to cross the \"trust boundary\" of blockchains, what trust assumptions that requires, and the basic design pattern for incorporating this data stream. Additional Material Wiki: Oracles (Ethereum.org) A great overview of oracles on Ethereum with demo code for Chainlink Wikipedia: Blockchain Oracle Docs: Chainlink A really well-curated series of docs outline how to get started with Chainlink and the different services and networks available. Tutorial: Implementing a Blockchain Oracle on Ethereum * Code: Provable Things Ethereum Examples A collection of examples from Provable Things, a bit dated but can still provide a source of reference. * Academic Article: A Study of Blockchain Oracles A technical examination of blockchain oracles referenced in the blockchain oracles Wikipedia page. * Article: So You Want to Use a Price Oracle Interesting article (albeit about on-chain oracles) discussing attack vectors.","title":"Oracles"},{"location":"S03-smart-contracts/M4-design-patterns/L4-oracles/#oracles","text":"In our discussion blockchain primitives, we discussed how public blockchain networks coordinate these powerful tools to create a network state that can be established and maintained by network participants. However, that network state only holds true within the boundaries of the network itself. We can only guarantee the state transitions of the network participants running our blockchain client software. What about data that exists outside this network, specifically what about \"real world\" data, such as weather reports, incident details or stock prices? Additionally, what if we'd like to interact with external computation in the outside world? A machine learning algorithm, a random number generator, or an event-driven task automation would all be examples of external computation that would be essential to building feature rich smart contracts. Such data and external computation exists but they are beyond the trust boundary established by our blockchain protocols. One way blockchain developers have tried to bridge the gap is through what are called off-chain oracles. Off-chain oracles are agents that find and verify real-world information and submit them to the blockchain to be used by smart contracts. They can trigger smart contract executions when the data is obtained or predefined conditions are meet (e.g. time, weather, tracking, payments). Off-chain oracles are provided by organizations like Provable Things, Chainlink Labs, and more. Please note that DeFi has led to on-chain oracles, particularly dealing with token prices, which we will discuss more in the section on DeFi. Such examples of on-chain oracles are Chainlink's Data Feeds , Uniswap's [Observations](https://docs.uniswap.org/protocol/concepts/V3-overview/oracle){target=_blank} , and MakerDAO's Feeds . A smart contract using oracles in some facet is often referred to as a Hybrid Smart Contract .","title":"Oracles"},{"location":"S03-smart-contracts/M4-design-patterns/L4-oracles/#trust","text":"It's extremely important to understand oracles like the ones we're discussing here can require trust. Understanding where the data is coming from is essential to running a decentralized smart contract. When using oracles, it's important to build in smart contract mechanisms that can anticipate errors or possible corruption, as oracles can succumb to many attack vectors. Having a single oracle or a single data source delivering your data or executing your computation creates a single source of failure for your smart contract, and means your decentralized application isn't actually decentralized. You'll want to make sure the oracle network you're working with is executing in as decentralized a context as possible. For example, if you want pricing data, you get the pricing data from many exchanges, and many independent blockchain oracles deliver this data. It's important to know, that just setting up an oracle service yourself can mean your application is centralized, and possibly doesn't have the highest quality data or execution. So we want to avoid setting these up ourselves, but also do our due diligence to make sure the system we are working with is decentralized. Services such as Chainlink have built more decentralized networks to hedge against the centralization of trust, you can read more about how Chainlink does that here and see an overview of decentralized oracles here.","title":"Trust"},{"location":"S03-smart-contracts/M4-design-patterns/L4-oracles/#basic-oracle-mechanism","text":"At its most basic, a smart contract contract using an oracle needs to implement a method to: Make the request to the oracle, and Receive the oracle's response from a callback method This is often known as the request and receive model of oracles. Unless you've setup the service yourself, external calls to oracles typically require a fee attached to provide the data to your contract. There are multiple ways to achieve this, let's look at one generalized code using Provable below: import \"github.com/oraclize/ethereum-api/provableAPI.sol\"; contract DieselPrice is usingProvable { uint DieselPriceUSD; constructor() public { provable_query(\"URL\", \"xml(https://www.fueleconomy.gov/ws/rest/fuelprices).fuelPrices.diesel\"); } function __callback (bytes32 myid, string result) public { require(msg.sender == provable_cbAddress()); DieselPriceUSD = parseInt(result); } } In this code, we're calling the Provable API for the price of diesel when we create the contract. The first query is free, but we'll have to provide ETH to pay for our requests moving forward. The call triggers an event, which lets the Provable contract pull from its off-chain datafeed and provide our contract the result. The contract stores that value in DieselPriceUSD . The overall model is for your contract to emit an event, either to another contract or simply in the block its created. The oracle service will detect that event, pull the desired data, and respond back to your contract. The oracle services require you to have a standard method, like __callback , that its transaction can target when responding to your oracle query.","title":"Basic Oracle Mechanism"},{"location":"S03-smart-contracts/M4-design-patterns/L4-oracles/#decentralized-oracle-mechanism","text":"In the above example, we looked at pulling data from a single source and provider. Stringing requests like this together can make us have a more decentralized application, but we would still be routing it though the same organization. Let's look at another example of pulling data in a decentralized context, for example, from a Chainlink data feed. You can see the most up to date version of this code in the Chainlink documentation. pragma solidity ^0.6.7; import \"@chainlink/contracts/src/v0.6/interfaces/AggregatorV3Interface.sol\"; contract PriceConsumerV3 { AggregatorV3Interface internal priceFeed; /** * Network: Kovan * Aggregator: ETH/USD * Address: 0x9326BFA02ADD2366b30bacB125260Af641031331 */ constructor() public { priceFeed = AggregatorV3Interface(0x9326BFA02ADD2366b30bacB125260Af641031331); } /** * Returns the latest price */ function getThePrice() public view returns (int) { ( uint80 roundID, int price, uint startedAt, uint timeStamp, uint80 answeredInRound ) = priceFeed.latestRoundData(); return price; } } In this example, we are pulling the price of Ethereum in terms of the United States Dollar into our smart contract. We do this by querying another contract that has already followed the basic request model among many different oracles and data providers. It's important that the network is using different data providers and oracles so that there is never a single authority point. You can see a visualization of this contract and the different nodes gathering the data for them at data.chain.link.","title":"Decentralized Oracle Mechanism"},{"location":"S03-smart-contracts/M4-design-patterns/L4-oracles/#summary","text":"The oracle ecosystem is growing as fast as blockchain, and we'll touch on it more later in the course. For now, it's important to see how oracles allow us to cross the \"trust boundary\" of blockchains, what trust assumptions that requires, and the basic design pattern for incorporating this data stream.","title":"Summary"},{"location":"S03-smart-contracts/M4-design-patterns/L4-oracles/#additional-material","text":"Wiki: Oracles (Ethereum.org) A great overview of oracles on Ethereum with demo code for Chainlink Wikipedia: Blockchain Oracle Docs: Chainlink A really well-curated series of docs outline how to get started with Chainlink and the different services and networks available. Tutorial: Implementing a Blockchain Oracle on Ethereum * Code: Provable Things Ethereum Examples A collection of examples from Provable Things, a bit dated but can still provide a source of reference. * Academic Article: A Study of Blockchain Oracles A technical examination of blockchain oracles referenced in the blockchain oracles Wikipedia page. * Article: So You Want to Use a Price Oracle Interesting article (albeit about on-chain oracles) discussing attack vectors.","title":"Additional Material"},{"location":"S03-smart-contracts/M6-security/L5-intro-to-dili/","text":"Security: Introduction to Diligence As we've drilled into your head many times now, smart contract development is different than traditional software development: Smart contract development is new and it is constantly changing. Smart contracts are immutable. They cannot be modified (only re-deployed). High cost of failure. More similar to hardware and financial services programming. Smart contracts information is public and anyone can call your public functions. We've gone over many of the common attack vectors, discussed smart contract best practices and showed you design patterns to increase security for your distributed applications. Much of this information came from the brilliant minds at Diligence, the auditing and smart contract security arm of ConsenSys. Diligence provides audits for the largest names in the blockchain sector, including Aave, 0x, Covantis, Aragon, Omisego, Horizon and more. Along with audits, Diligence also provides automated security analysis through two main tools: MythX and Scribble . MythX MythX is a smart contract security service for Ethereum built by ConsenSys Diligence. As part of the bootcamp, we have gotten a free month of the developer plan for every student from the ConsenSys Diligence team. Promo code = x9naNrvz Steps to Get Started Go to https://mythx.io/ Click the \u201c sign up \u201dbutton Complete the registration form. You will then have an option to choose a plan. Please select \u201c Try MythX & Buy Scans \u201d This will allow you to set up a MythX account with no charge. Go to the dashboard Billing tab click \" Buy a pack of 3 scans today for $9.99 \" Enter your promo code x9naNrvz in the box provided and proceed through check with no charge Scribble Diligence also provides another analysis tool called Scribble, which is \"Fuzzing as a Service.\" Fuzzing is a general computer security automation method that essentially runs millions of tests against your codebase. There's a bit of structure involved, but the overall concept is applicable to all other computer security industries. In the following lesson, Joran Honig will walk you through the basic concepts behind Scribble and through three exercises to help you learn how to use Scribble! Additional Material Video: Security by Design and Smart Contract Audits (Shayan Eskandari) Series: Diligence YouTube Channel Video: Shift Left and DevSecOps (Joran Honig) Talk from TruffleCon2020 where Joran goes through the concept of \"Shift Left\" as it applies to security best practices. Video: Shift Left and Automated Tooling (Joran Honig) Articles: Collection of Smart Contract Best Practices from Diligence Scribble Article: Introducing Scribble Article: Introducing Scribble Generator Article: Writing Properties \u2014 A New Approach to Testing Article: Four Effective Strategies To Come Up with Scribble Annotations","title":"Security: Introduction to Diligence"},{"location":"S03-smart-contracts/M6-security/L5-intro-to-dili/#security-introduction-to-diligence","text":"As we've drilled into your head many times now, smart contract development is different than traditional software development: Smart contract development is new and it is constantly changing. Smart contracts are immutable. They cannot be modified (only re-deployed). High cost of failure. More similar to hardware and financial services programming. Smart contracts information is public and anyone can call your public functions. We've gone over many of the common attack vectors, discussed smart contract best practices and showed you design patterns to increase security for your distributed applications. Much of this information came from the brilliant minds at Diligence, the auditing and smart contract security arm of ConsenSys. Diligence provides audits for the largest names in the blockchain sector, including Aave, 0x, Covantis, Aragon, Omisego, Horizon and more. Along with audits, Diligence also provides automated security analysis through two main tools: MythX and Scribble .","title":"Security: Introduction to Diligence"},{"location":"S03-smart-contracts/M6-security/L5-intro-to-dili/#mythx","text":"MythX is a smart contract security service for Ethereum built by ConsenSys Diligence. As part of the bootcamp, we have gotten a free month of the developer plan for every student from the ConsenSys Diligence team. Promo code = x9naNrvz","title":"MythX"},{"location":"S03-smart-contracts/M6-security/L5-intro-to-dili/#steps-to-get-started","text":"Go to https://mythx.io/ Click the \u201c sign up \u201dbutton Complete the registration form. You will then have an option to choose a plan. Please select \u201c Try MythX & Buy Scans \u201d This will allow you to set up a MythX account with no charge. Go to the dashboard Billing tab click \" Buy a pack of 3 scans today for $9.99 \" Enter your promo code x9naNrvz in the box provided and proceed through check with no charge","title":"Steps to Get Started"},{"location":"S03-smart-contracts/M6-security/L5-intro-to-dili/#scribble","text":"Diligence also provides another analysis tool called Scribble, which is \"Fuzzing as a Service.\" Fuzzing is a general computer security automation method that essentially runs millions of tests against your codebase. There's a bit of structure involved, but the overall concept is applicable to all other computer security industries. In the following lesson, Joran Honig will walk you through the basic concepts behind Scribble and through three exercises to help you learn how to use Scribble!","title":"Scribble"},{"location":"S03-smart-contracts/M6-security/L5-intro-to-dili/#additional-material","text":"Video: Security by Design and Smart Contract Audits (Shayan Eskandari) Series: Diligence YouTube Channel Video: Shift Left and DevSecOps (Joran Honig) Talk from TruffleCon2020 where Joran goes through the concept of \"Shift Left\" as it applies to security best practices. Video: Shift Left and Automated Tooling (Joran Honig) Articles: Collection of Smart Contract Best Practices from Diligence","title":"Additional Material"},{"location":"S03-smart-contracts/M6-security/L5-intro-to-dili/#scribble_1","text":"Article: Introducing Scribble Article: Introducing Scribble Generator Article: Writing Properties \u2014 A New Approach to Testing Article: Four Effective Strategies To Come Up with Scribble Annotations","title":"Scribble"},{"location":"S03-smart-contracts/M6-security/L6-scribble/","text":"This is currently a video on the LMS","title":"Index"},{"location":"S03-smart-contracts/M6-security/L7-other-security-options/","text":"Other Security Options Outside of the tools we've provided so far, there are other great security analysis tools: Tools OpenZeppelin Defender Less of a security tool per se and more like an operating system or dashboard for your smart contracts. This allows you to monitor your smart contracts, respond to exploits or bugs by adjusting access control, and private transaction relayers. Slither A static analysis framework for Solidity built by the auditing firm Trail of Bits. It is written in Python, is open-source and you can read more about it here. Manticore \"A symbolic execution tool for analysis of smart contracts and binaries\" as well as WASM modules. Also built by Trail of Bits! Read more here. Ethersplay An EVM Disassembler which takes as input raw EVM bytecode (your contract you're deploying) and analyzes it at the Assembly level. It can provide a flow graph of all the functions in the bytecode. Another tool from Trail of Bits, it also can let you know where Manticore has scanned. Echidna A fuzzer like Scribble. It is \"a Haskell program designed for fuzzing/property-based testing of Ethereum smarts contracts. It uses sophisticated grammar-based fuzzing campaigns based on a contract ABI to falsify user-defined predicates or Solidity assertions.\" ( source ) Also from Trail of Bits. Learning Blocksec CTFs An amazing collection of blockchain Capture The Flag (CTF) exercises and write-ups Article: Ethereum Quirks and Vulnerabilities Discussion around attack vectors on Ethereum Ethernaut Phenomenal CTF / Wargame series of exercises for blockchain security. Be sure to checkout the EthernautDAO, organized in part by the designing of Ethernaut. DamnVulnerableDeFi A DeFi-oriented CFT Capture the Ether Another blockchain CTF, written by Steve Marx CipherShastra Another CTF-style learning challenge Diligence Public Audits, OpenZeppelin Audits Another great way to learn to audit is to read reports from the best organizations doing them. Trail of Bits is not blockchain-specific, but you can see their public security \"reviews\" here. Audits Thread: Before an Audit @Tincho, a security researcher at OpenZeppelin, walks through the things you absolutely should do before submitting your code for an audit Code 423n4 \"A community-driven approach to competitive smart contract audits.\" A great way to get into auditing \u2014 no experience necessary. Immunefi A collection of bug bounties for blockchain projects anyone can contribute. Article: Introducing Solidify (Coinbase) We haven't gotten a chance to try this one, but Coinbase offering a new tool for smart contract analysis. This is not an endorsement of this, just letting you know it exists!","title":"Index"},{"location":"S03-smart-contracts/M6-security/L7-other-security-options/#other-security-options","text":"Outside of the tools we've provided so far, there are other great security analysis tools:","title":"Other Security Options"},{"location":"S03-smart-contracts/M6-security/L7-other-security-options/#tools","text":"OpenZeppelin Defender Less of a security tool per se and more like an operating system or dashboard for your smart contracts. This allows you to monitor your smart contracts, respond to exploits or bugs by adjusting access control, and private transaction relayers. Slither A static analysis framework for Solidity built by the auditing firm Trail of Bits. It is written in Python, is open-source and you can read more about it here. Manticore \"A symbolic execution tool for analysis of smart contracts and binaries\" as well as WASM modules. Also built by Trail of Bits! Read more here. Ethersplay An EVM Disassembler which takes as input raw EVM bytecode (your contract you're deploying) and analyzes it at the Assembly level. It can provide a flow graph of all the functions in the bytecode. Another tool from Trail of Bits, it also can let you know where Manticore has scanned. Echidna A fuzzer like Scribble. It is \"a Haskell program designed for fuzzing/property-based testing of Ethereum smarts contracts. It uses sophisticated grammar-based fuzzing campaigns based on a contract ABI to falsify user-defined predicates or Solidity assertions.\" ( source ) Also from Trail of Bits.","title":"Tools"},{"location":"S03-smart-contracts/M6-security/L7-other-security-options/#learning","text":"Blocksec CTFs An amazing collection of blockchain Capture The Flag (CTF) exercises and write-ups Article: Ethereum Quirks and Vulnerabilities Discussion around attack vectors on Ethereum Ethernaut Phenomenal CTF / Wargame series of exercises for blockchain security. Be sure to checkout the EthernautDAO, organized in part by the designing of Ethernaut. DamnVulnerableDeFi A DeFi-oriented CFT Capture the Ether Another blockchain CTF, written by Steve Marx CipherShastra Another CTF-style learning challenge Diligence Public Audits, OpenZeppelin Audits Another great way to learn to audit is to read reports from the best organizations doing them. Trail of Bits is not blockchain-specific, but you can see their public security \"reviews\" here.","title":"Learning"},{"location":"S03-smart-contracts/M6-security/L7-other-security-options/#audits","text":"Thread: Before an Audit @Tincho, a security researcher at OpenZeppelin, walks through the things you absolutely should do before submitting your code for an audit Code 423n4 \"A community-driven approach to competitive smart contract audits.\" A great way to get into auditing \u2014 no experience necessary. Immunefi A collection of bug bounties for blockchain projects anyone can contribute. Article: Introducing Solidify (Coinbase) We haven't gotten a chance to try this one, but Coinbase offering a new tool for smart contract analysis. This is not an endorsement of this, just letting you know it exists!","title":"Audits"},{"location":"S04-developer-tooling/M1-intro/L4-ag-dev-workflow/","text":"Blockchain Agnostic Developer Workflow It's no surprise by now that, in this course, we want to give you frameworks to help you approach the complicated business of blockchain development. In this section, we're hoping to give you a sense of the general workflow you'll be doing as a developer while developing a distributed application (\"dapp\" or smart contract + interface) on any blockchain. Note: This is not the workflow of a protocol designer, which is a more traditional project-based work environment based on the development language Lifecycle of a Project Let's see the overall lifecycle of a dapp project: Perhaps the most important work you can do as a developer is the first step: Determine the scope and goals of your project. This does not require any code at all, but it will save you so much time in the future. It's essentially creating the roadmap for your application and, when things get complicated, you'll be able to refer back to it for clarity. (This step should be familiar as the first exercise in the course was for you to do this step for your final project!) Next comes the Architectural and Technical Design : Sketching out the technical parameters of your project. What will your smart contract function parameters look like? Where will you do storage? How will you divide on-chain and off-chain logic? If your project involves others, how will you implement governance? How do you plan to scale? Next is the Development phase, which is typically what people think the only phase. Please note that this is the third step in building a project (Planning is so important)! We'll drill deeper into this step later in the section, but it's primarily the building and testing of your smart contract and interface. In this step, you'll leverage framework development tools, like Truffle or Hardhat, as well as testing tools, like testing suites and testnets. You'll also be referring to the notes you've made in the previous two steps, being sure to stay within the guidelines you made for yourself. Next comes Security Audit. We've already heard about Diligence and security tools such as MythX, Scribble, Slither or Manticore. For projects that could potentially hold enormous value this step is critical and should not be overlooked. It can sometimes be challenging to find a team or project to audit your code, but it is worth the effort considering the potential downside! Simultaneous with an audit, you might be running the next step, Bug Bounty and Community, meaning somehow starting to ask your community to test-drive your project. Note, this is before an official, version 1.0 release. Last, after all these steps, is the Launch . You may have already stealthily deployed your contract, assuming you've done all the testing and auditing, but this is where you announce the interface and let folks know that it's ready to go! As we'll see next, in some ways your work has just begun, but at least you've gotten your project launched into the world! Developmer Tooling We're now going to drill into the Development phase mentioned above. This is probably what we all think of when we think about developing application for the blockchain. Here's a simplified oveview of what a development flow looks like at the beginning of a project: Here we see the developer (you) working mainly from their code editor, in which they'll have both the smart contracts and whatever frontend interface they're working on. The code editor will be stocked with the most helpful general extensions as well as specific smart contract extensions, such as the Solidity extension for VSCode as well as tools to help with gas estimation or contract sizing. Next, the developer will have a framework that they're using to deploy the smart contract and interface. In the image above, the developer is using Truffle to build their contracts, hold the build artifacts, run tests and deploy to a testnet. There are other options as well, such as Hardhat , Scaffold-Eth , and Brownie , to name some of the more popular ones. The developer will then having some private testnet tool they're using to deploy the smart contract in the early stages. This will be Ganache for us, since we're using Truffle. As we continue to become more confident in our development process, we may want to deploy to a public testnet. For this, we will use either an Ethereum node we have on our machine or we'll use a gateway service like Infura , which will allow us to easily deploy to an Ethereum testnet, Ethereum mainnet, or even networks like Polygon, Arbitrum or Optimism. (If we're starting to lose you, don't worry we'll cover all these things later in the course!) .env File One thing that seems trivial but is incredibly important is the .env file for your project. These are the local environmental variables that allow you to deploy your application to the public blockchain network. It also is the best way to ensure you don't expose your private keys or any other sensitive information when you're developing. Particularly when you're pushing material to a git repository, it's easy for folks to forget they've included sensitive information. There are definitely bots that are continually scanning Github for private keys that will immediately be compromised. Please read this article about how to keep your development environment safe by using a .env file and other essential techniques! Advanced Developer Tooling As you become more familiar with blockchain development and as your project grows, so will your toolset and workflow. Below is a diagram showing a more expansive development lifecycle that includes auditing, scaling, monitoring and advanced onboarding of users: (We're using ConsenSys products here mainly for reference, since we've discussed many of them so far.) We'll get into these tools more but it's a bit beyond the scope of this lesson. In the next few sections, however, you'll become more familiar with the tools and mechanisms shown here. Additional Materials Remix Interface The original IDE for Ethereum! Well, probably not the original, but still the best first place to go to start Solidity development. Basic Training: Code Editors, VSCode Extensions I know, I know, you're super sick of hearing about Basic Training. Fair! But you should be sure to install all the VSCode extensions in this section of Basic Training, since it will help with development! Tutorial: Using an .env file to keep your secrets safe Essential reading! Good Extensions to Know About Hardhat-contract-sizer , hardhat-gas-reporter Replit Still exploring this one, but really good potential for troubleshooting code with friends. Not blockchain specific! Testing in the Twenties Really good general advice about testing! How to Setup a Solidity Project","title":"Blockchain Agnostic Developer Workflow"},{"location":"S04-developer-tooling/M1-intro/L4-ag-dev-workflow/#blockchain-agnostic-developer-workflow","text":"It's no surprise by now that, in this course, we want to give you frameworks to help you approach the complicated business of blockchain development. In this section, we're hoping to give you a sense of the general workflow you'll be doing as a developer while developing a distributed application (\"dapp\" or smart contract + interface) on any blockchain. Note: This is not the workflow of a protocol designer, which is a more traditional project-based work environment based on the development language","title":"Blockchain Agnostic Developer Workflow"},{"location":"S04-developer-tooling/M1-intro/L4-ag-dev-workflow/#lifecycle-of-a-project","text":"Let's see the overall lifecycle of a dapp project: Perhaps the most important work you can do as a developer is the first step: Determine the scope and goals of your project. This does not require any code at all, but it will save you so much time in the future. It's essentially creating the roadmap for your application and, when things get complicated, you'll be able to refer back to it for clarity. (This step should be familiar as the first exercise in the course was for you to do this step for your final project!) Next comes the Architectural and Technical Design : Sketching out the technical parameters of your project. What will your smart contract function parameters look like? Where will you do storage? How will you divide on-chain and off-chain logic? If your project involves others, how will you implement governance? How do you plan to scale? Next is the Development phase, which is typically what people think the only phase. Please note that this is the third step in building a project (Planning is so important)! We'll drill deeper into this step later in the section, but it's primarily the building and testing of your smart contract and interface. In this step, you'll leverage framework development tools, like Truffle or Hardhat, as well as testing tools, like testing suites and testnets. You'll also be referring to the notes you've made in the previous two steps, being sure to stay within the guidelines you made for yourself. Next comes Security Audit. We've already heard about Diligence and security tools such as MythX, Scribble, Slither or Manticore. For projects that could potentially hold enormous value this step is critical and should not be overlooked. It can sometimes be challenging to find a team or project to audit your code, but it is worth the effort considering the potential downside! Simultaneous with an audit, you might be running the next step, Bug Bounty and Community, meaning somehow starting to ask your community to test-drive your project. Note, this is before an official, version 1.0 release. Last, after all these steps, is the Launch . You may have already stealthily deployed your contract, assuming you've done all the testing and auditing, but this is where you announce the interface and let folks know that it's ready to go! As we'll see next, in some ways your work has just begun, but at least you've gotten your project launched into the world!","title":"Lifecycle of a Project"},{"location":"S04-developer-tooling/M1-intro/L4-ag-dev-workflow/#developmer-tooling","text":"We're now going to drill into the Development phase mentioned above. This is probably what we all think of when we think about developing application for the blockchain. Here's a simplified oveview of what a development flow looks like at the beginning of a project: Here we see the developer (you) working mainly from their code editor, in which they'll have both the smart contracts and whatever frontend interface they're working on. The code editor will be stocked with the most helpful general extensions as well as specific smart contract extensions, such as the Solidity extension for VSCode as well as tools to help with gas estimation or contract sizing. Next, the developer will have a framework that they're using to deploy the smart contract and interface. In the image above, the developer is using Truffle to build their contracts, hold the build artifacts, run tests and deploy to a testnet. There are other options as well, such as Hardhat , Scaffold-Eth , and Brownie , to name some of the more popular ones. The developer will then having some private testnet tool they're using to deploy the smart contract in the early stages. This will be Ganache for us, since we're using Truffle. As we continue to become more confident in our development process, we may want to deploy to a public testnet. For this, we will use either an Ethereum node we have on our machine or we'll use a gateway service like Infura , which will allow us to easily deploy to an Ethereum testnet, Ethereum mainnet, or even networks like Polygon, Arbitrum or Optimism. (If we're starting to lose you, don't worry we'll cover all these things later in the course!)","title":"Developmer Tooling"},{"location":"S04-developer-tooling/M1-intro/L4-ag-dev-workflow/#env-file","text":"One thing that seems trivial but is incredibly important is the .env file for your project. These are the local environmental variables that allow you to deploy your application to the public blockchain network. It also is the best way to ensure you don't expose your private keys or any other sensitive information when you're developing. Particularly when you're pushing material to a git repository, it's easy for folks to forget they've included sensitive information. There are definitely bots that are continually scanning Github for private keys that will immediately be compromised. Please read this article about how to keep your development environment safe by using a .env file and other essential techniques!","title":".env File"},{"location":"S04-developer-tooling/M1-intro/L4-ag-dev-workflow/#advanced-developer-tooling","text":"As you become more familiar with blockchain development and as your project grows, so will your toolset and workflow. Below is a diagram showing a more expansive development lifecycle that includes auditing, scaling, monitoring and advanced onboarding of users: (We're using ConsenSys products here mainly for reference, since we've discussed many of them so far.) We'll get into these tools more but it's a bit beyond the scope of this lesson. In the next few sections, however, you'll become more familiar with the tools and mechanisms shown here.","title":"Advanced Developer Tooling"},{"location":"S04-developer-tooling/M1-intro/L4-ag-dev-workflow/#additional-materials","text":"Remix Interface The original IDE for Ethereum! Well, probably not the original, but still the best first place to go to start Solidity development. Basic Training: Code Editors, VSCode Extensions I know, I know, you're super sick of hearing about Basic Training. Fair! But you should be sure to install all the VSCode extensions in this section of Basic Training, since it will help with development! Tutorial: Using an .env file to keep your secrets safe Essential reading! Good Extensions to Know About Hardhat-contract-sizer , hardhat-gas-reporter Replit Still exploring this one, but really good potential for troubleshooting code with friends. Not blockchain specific! Testing in the Twenties Really good general advice about testing! How to Setup a Solidity Project","title":"Additional Materials"},{"location":"S04-developer-tooling/M2-web3-libraries/L1-intro-web3-ethers/","text":"Web 3 Javascript Libraries As we mentioned in \"Where Do Users Fit in Our Mental Model?\" Web 3 Javascript APIs are critical to connecting users to our blockchain applications. There are a variety of common JavaScript libraries that you can use to connect to Ethereum and develop an interface for your users. Many of the libraries serve the same purpose and have the same functionality, but the syntax differs for each. The purpose of this lesson is to show the similarities and differences between the main two libraries, Web3.js and ethers.js, so you gain a better understanding of what these libraries do a general level and how each one does it. If you are using the Brave browser, you may encounter conflicts with the built-in Ethereum wallet and Metamask. If this happens, try using a different browser with Metamask installed. Truffle Truffle is the framework that we have covered in the most depth so far in the course. Truffle will connect to a running blockchain specified in the truffle-config.js file, manage deployments via migration scripts and information stored in the truffle artifacts and abstracts away much of the complexity of interacting with contracts (via contract abstractions ). Other libraries handle these in different ways and have different APIs that are useful to review. Web3.js Web3.js is one of the most popular JavaScript libraries in Ethereum dApp development. Unfortunately there are two versions of web3.js in use right now. Web3.js 1.x is in development, you can visit the documentation for 1.x here . The latest stable version at the time of this writing is web3.js 0.2x.x, and you can see those docs here . As you can see, the APIs for these versions are different, so pay attention to which version you are using. Web3.js is the library that is injected by Metamask. If you have Metamask installed in your browser, you can see the web3 object by opening your browser developer tools (ctrl shift i in Chrome) and typing web3 in the console. As you go through this workshop, if you encounter problems, try refreshing your browser and repeating the necessary steps. You can see the current version is \"0.20.x\", in the \"version\" property of the web3 object. This web3 object is connected to the Ethereum network through a service provided by Metamask. Metamask uses Infura infrastructure as the gateway to Ethereum. We have included a version of the newer web3.js API in the page as well, so by entering web3 = new Web3(web3.currentProvider) in the browser console, you can get an instance of the web3.js version 1.x. For the remainder of this lesson, we will be using the web3.js v1.x beta API . A Side note about Metamask If you are following along in your browser, you will also see that in the \"currentProvider\" property, the \"selectedAddress\" is undefined or null. Metamask does not provide access to the account address by default. If you type ethereum.enable() in the console, Metamask will pop open asking you if you'd like to connect. Connecting will make this account information accessible to the current page. Connect to Ganache GUI Let's connect to Ganache GUI and send a transaction via the API in the console. Start Ganache GUI and connect Metamask (Ganache GUI defaults to port 7545). Use the Ganache GUI \"Quickstart\" option to follow along with the same account addresses that I use in this explanation. To connect to Ganache GUI, click \"Custom RPC\" in the Network drop down and then enter the network information. You can easily import Ganache GUI accounts into Metamask by importing via the private key. Click the key icon on the right side of Ganache GUI to get the associated account private key. To import the account into Metamask, select \"Import Account\" in the Metamask accounts dropdown, and paste in the private key. Checking the account Once you are connected to Ganache GUI through Metamask, you can send transactions on the Ganache GUI network through the injected web3 object in the browser console. Typing web3.currentProvider.selectedAddress should return your current account address. Sending a Transaction Use the following code snippet as your transaction information. var transaction = { from: web3.currentProvider.selectedAddress, to: \"0xce573835eB9ca38454f97D103Ca46b7e8aDF617f\", value: web3.utils.toWei(\"1\", \"ether\") } Th \"to\" account is the second account that is generated by the Quickstart in Ganache GUI. In the console, it look like this: Now sending a transaction is as easy as entering web3.eth.sendTransaction(transaction) in the console and Metamask will pop up, asking you to sign the transaction. If you get an error, you may need to reset the web3 provider. You can do that with this line of code web3.setProvider(web3.currentProvider) . Nonce Mismatch Errors If you use Metamask accounts on different development blockchains, the nonce counts may get out of sync, in which case you will see an error when trying to execute a transaction. Metamask tracks the account nonce independently so this can get out of sync with the account nonce on the blockchain network that you are trying to interact with. If this happens, it is a simple fix. Open Metamask and click the account icon on the upper right and select \"Settings\". In the \"Advanced\" area, select \"Reset Account\". If you were seeing this error, reset your account and try sending the transaction again. Metamask will get the correct account nonce from the blockchain network. When the transaction succeeds, you should see the new account balances reflected on Ganache GUI. You can check the balance of these accounts with the line await web3.eth.getBalance(address) where address is any Ethereum address. Try it with \" await web3.eth.getBalance(web3.currentProvider.selectedAddress) \". This is just a quick intro to sending transaction with web3.js v1.0. You can learn more about how to use it via the docs and specifically about how to connect to a contract via this section. Keep in mind that this library is still in development, so if you run into any bugs, please report them! Ethers.js Let's try connecting to a different library. The ethers.js library is also included on this page and is accessible via the browser JavaScript console. We will continue to use Metamask as the account signer for the accounts on the development blockchain. Connect to the Web 3 provider In the browser console, you can connect ethers.js to the current network by accessing the provider given by Metamask, then set it is as the \"signer\". Metamask injects the provider as \"web3.currentProvider\", but since we changed the web3 object to use web3.js v1.0, the provider is accessible at \"web3.givenProvider\". // MetaMask injects a Web3 Provider as \"web3.currentProvider\", so // we can wrap it up in the ethers.js Web3Provider, which wraps a // Web3 Provider and exposes the ethers.js Provider API. const provider = new ethers.providers.Web3Provider(web3.currentProvider); // There is only ever up to one account in MetaMask exposed const signer = provider.getSigner(); Now that Metamask is set as the signer, you can send a transaction. Check the \"signer\" API of ethers.js to see how to send a transaction. Something like \"signer.sendTransaction(transaction)\" should work, but what does a transaction look like in ethers.js? { // Required unless deploying a contract (in which case omit) to: addressOrName, // the target address or ENS name // These are optional/meaningless for call and estimateGas nonce: 0, // the transaction nonce gasLimit: 0, // the maximum gas this transaction may spend gasPrice: 0, // the price (in wei) per unit of gas // These are always optional (but for call, data is usually specified) data: \"0x\", // extra data for the transaction, or input for call value: 0, // the amount (in wei) this transaction is sending chainId: 3 // the network ID; usually added by a signer } You can check the docs here. For a simple ether transfer, you can get away with: var transaction = { to: \"0xce573835eB9ca38454f97D103Ca46b7e8aDF617f\", value: ethers.utils.parseEther(\"1\") } And to send it, enter signer.sendTransaction(transaction) in the browser console. Confirm the transaction and verify that the account balances have changed in Ganache GUI. Please explore the Ether.js documentation to see all of the things that it can do. Summary As a developer, you can use different JavaScript libraries to interact with Ethereum blockchains. Many of the JavaScript libraries do many of the same things, but they may handle transaction signers or providers differently or have different ways of connecting to contracts and listening to events. The library you choose is primarily a matter of personal preference. We focus on using web3.js in this course, but introduce ethers.js because it is a popular alternative. Additional Material Article: Web3 vs ethers Part I and Part II by Academy's own Tom Hay and Robbie K. Course: Learn Ethers.js (Chainshot) Use Chainshot's excellent interactive interface to learn more about ethers.js","title":"Web 3 Javascript Libraries"},{"location":"S04-developer-tooling/M2-web3-libraries/L1-intro-web3-ethers/#web-3-javascript-libraries","text":"As we mentioned in \"Where Do Users Fit in Our Mental Model?\" Web 3 Javascript APIs are critical to connecting users to our blockchain applications. There are a variety of common JavaScript libraries that you can use to connect to Ethereum and develop an interface for your users. Many of the libraries serve the same purpose and have the same functionality, but the syntax differs for each. The purpose of this lesson is to show the similarities and differences between the main two libraries, Web3.js and ethers.js, so you gain a better understanding of what these libraries do a general level and how each one does it. If you are using the Brave browser, you may encounter conflicts with the built-in Ethereum wallet and Metamask. If this happens, try using a different browser with Metamask installed.","title":"Web 3 Javascript Libraries"},{"location":"S04-developer-tooling/M2-web3-libraries/L1-intro-web3-ethers/#truffle","text":"Truffle is the framework that we have covered in the most depth so far in the course. Truffle will connect to a running blockchain specified in the truffle-config.js file, manage deployments via migration scripts and information stored in the truffle artifacts and abstracts away much of the complexity of interacting with contracts (via contract abstractions ). Other libraries handle these in different ways and have different APIs that are useful to review.","title":"Truffle"},{"location":"S04-developer-tooling/M2-web3-libraries/L1-intro-web3-ethers/#web3js","text":"Web3.js is one of the most popular JavaScript libraries in Ethereum dApp development. Unfortunately there are two versions of web3.js in use right now. Web3.js 1.x is in development, you can visit the documentation for 1.x here . The latest stable version at the time of this writing is web3.js 0.2x.x, and you can see those docs here . As you can see, the APIs for these versions are different, so pay attention to which version you are using. Web3.js is the library that is injected by Metamask. If you have Metamask installed in your browser, you can see the web3 object by opening your browser developer tools (ctrl shift i in Chrome) and typing web3 in the console. As you go through this workshop, if you encounter problems, try refreshing your browser and repeating the necessary steps. You can see the current version is \"0.20.x\", in the \"version\" property of the web3 object. This web3 object is connected to the Ethereum network through a service provided by Metamask. Metamask uses Infura infrastructure as the gateway to Ethereum. We have included a version of the newer web3.js API in the page as well, so by entering web3 = new Web3(web3.currentProvider) in the browser console, you can get an instance of the web3.js version 1.x. For the remainder of this lesson, we will be using the web3.js v1.x beta API .","title":"Web3.js"},{"location":"S04-developer-tooling/M2-web3-libraries/L1-intro-web3-ethers/#a-side-note-about-metamask","text":"If you are following along in your browser, you will also see that in the \"currentProvider\" property, the \"selectedAddress\" is undefined or null. Metamask does not provide access to the account address by default. If you type ethereum.enable() in the console, Metamask will pop open asking you if you'd like to connect. Connecting will make this account information accessible to the current page. Connect to Ganache GUI Let's connect to Ganache GUI and send a transaction via the API in the console. Start Ganache GUI and connect Metamask (Ganache GUI defaults to port 7545). Use the Ganache GUI \"Quickstart\" option to follow along with the same account addresses that I use in this explanation. To connect to Ganache GUI, click \"Custom RPC\" in the Network drop down and then enter the network information. You can easily import Ganache GUI accounts into Metamask by importing via the private key. Click the key icon on the right side of Ganache GUI to get the associated account private key. To import the account into Metamask, select \"Import Account\" in the Metamask accounts dropdown, and paste in the private key.","title":"A Side note about Metamask"},{"location":"S04-developer-tooling/M2-web3-libraries/L1-intro-web3-ethers/#checking-the-account","text":"Once you are connected to Ganache GUI through Metamask, you can send transactions on the Ganache GUI network through the injected web3 object in the browser console. Typing web3.currentProvider.selectedAddress should return your current account address.","title":"Checking the account"},{"location":"S04-developer-tooling/M2-web3-libraries/L1-intro-web3-ethers/#_1","text":"","title":""},{"location":"S04-developer-tooling/M2-web3-libraries/L1-intro-web3-ethers/#sending-a-transaction","text":"Use the following code snippet as your transaction information. var transaction = { from: web3.currentProvider.selectedAddress, to: \"0xce573835eB9ca38454f97D103Ca46b7e8aDF617f\", value: web3.utils.toWei(\"1\", \"ether\") } Th \"to\" account is the second account that is generated by the Quickstart in Ganache GUI. In the console, it look like this: Now sending a transaction is as easy as entering web3.eth.sendTransaction(transaction) in the console and Metamask will pop up, asking you to sign the transaction. If you get an error, you may need to reset the web3 provider. You can do that with this line of code web3.setProvider(web3.currentProvider) .","title":"Sending a Transaction"},{"location":"S04-developer-tooling/M2-web3-libraries/L1-intro-web3-ethers/#nonce-mismatch-errors","text":"If you use Metamask accounts on different development blockchains, the nonce counts may get out of sync, in which case you will see an error when trying to execute a transaction. Metamask tracks the account nonce independently so this can get out of sync with the account nonce on the blockchain network that you are trying to interact with. If this happens, it is a simple fix. Open Metamask and click the account icon on the upper right and select \"Settings\". In the \"Advanced\" area, select \"Reset Account\". If you were seeing this error, reset your account and try sending the transaction again. Metamask will get the correct account nonce from the blockchain network. When the transaction succeeds, you should see the new account balances reflected on Ganache GUI. You can check the balance of these accounts with the line await web3.eth.getBalance(address) where address is any Ethereum address. Try it with \" await web3.eth.getBalance(web3.currentProvider.selectedAddress) \". This is just a quick intro to sending transaction with web3.js v1.0. You can learn more about how to use it via the docs and specifically about how to connect to a contract via this section. Keep in mind that this library is still in development, so if you run into any bugs, please report them!","title":"Nonce Mismatch Errors"},{"location":"S04-developer-tooling/M2-web3-libraries/L1-intro-web3-ethers/#ethersjs","text":"Let's try connecting to a different library. The ethers.js library is also included on this page and is accessible via the browser JavaScript console. We will continue to use Metamask as the account signer for the accounts on the development blockchain.","title":"Ethers.js"},{"location":"S04-developer-tooling/M2-web3-libraries/L1-intro-web3-ethers/#connect-to-the-web-3-provider","text":"In the browser console, you can connect ethers.js to the current network by accessing the provider given by Metamask, then set it is as the \"signer\". Metamask injects the provider as \"web3.currentProvider\", but since we changed the web3 object to use web3.js v1.0, the provider is accessible at \"web3.givenProvider\". // MetaMask injects a Web3 Provider as \"web3.currentProvider\", so // we can wrap it up in the ethers.js Web3Provider, which wraps a // Web3 Provider and exposes the ethers.js Provider API. const provider = new ethers.providers.Web3Provider(web3.currentProvider); // There is only ever up to one account in MetaMask exposed const signer = provider.getSigner(); Now that Metamask is set as the signer, you can send a transaction. Check the \"signer\" API of ethers.js to see how to send a transaction. Something like \"signer.sendTransaction(transaction)\" should work, but what does a transaction look like in ethers.js? { // Required unless deploying a contract (in which case omit) to: addressOrName, // the target address or ENS name // These are optional/meaningless for call and estimateGas nonce: 0, // the transaction nonce gasLimit: 0, // the maximum gas this transaction may spend gasPrice: 0, // the price (in wei) per unit of gas // These are always optional (but for call, data is usually specified) data: \"0x\", // extra data for the transaction, or input for call value: 0, // the amount (in wei) this transaction is sending chainId: 3 // the network ID; usually added by a signer } You can check the docs here. For a simple ether transfer, you can get away with: var transaction = { to: \"0xce573835eB9ca38454f97D103Ca46b7e8aDF617f\", value: ethers.utils.parseEther(\"1\") } And to send it, enter signer.sendTransaction(transaction) in the browser console. Confirm the transaction and verify that the account balances have changed in Ganache GUI. Please explore the Ether.js documentation to see all of the things that it can do.","title":"Connect to the Web 3 provider"},{"location":"S04-developer-tooling/M2-web3-libraries/L1-intro-web3-ethers/#summary","text":"As a developer, you can use different JavaScript libraries to interact with Ethereum blockchains. Many of the JavaScript libraries do many of the same things, but they may handle transaction signers or providers differently or have different ways of connecting to contracts and listening to events. The library you choose is primarily a matter of personal preference. We focus on using web3.js in this course, but introduce ethers.js because it is a popular alternative.","title":"Summary"},{"location":"S04-developer-tooling/M2-web3-libraries/L1-intro-web3-ethers/#additional-material","text":"Article: Web3 vs ethers Part I and Part II by Academy's own Tom Hay and Robbie K. Course: Learn Ethers.js (Chainshot) Use Chainshot's excellent interactive interface to learn more about ethers.js","title":"Additional Material"},{"location":"S04-developer-tooling/M2-web3-libraries/L2-web3-connect-to-contract/","text":"This lesson is meant to directly follow the previous lesson. If you have not gone through that lesson yet, please go back and do that one first to avoid confusion and potential errors. In this lesson we are going to use web3.js in the browser console again. This time to connect to a SimpleStorage.sol contract that is deployed on the Rinkeby testnet. You can view the code for the contract on GitHub here and on etherscan here . Connect to the network First, make sure that Metamask is connected to the Rinkeby network. To use the web3.js 1.x library, we need to initialize it again. We can do that by running web3 = new Web3(web3.givenProvider) in the browser console. Initialize the contract Before we can interact with the contract, we need to initialize an instance of the SimpleStorage contract using web3.js. In this step we are going to provide web3.js with the ABI and the contract address, so it knows what functions are available at the address that we provide. The SimpleStorage.sol contract is deployed at address 0x49Bb098E781eD5C50D85E82d85cbA1a6F03FD3e6. Let's set the address in the console: const SSaddress = \"0x49Bb098E781eD5C50D85E82d85cbA1a6F03FD3e6\" Let's set the ABI in the console with: const ABI = [ { \"constant\": false, \"inputs\": [ { \"internalType\": \"uint256\", \"name\": \"x\", \"type\": \"uint256\" } ], \"name\": \"set\", \"outputs\": [], \"payable\": false, \"stateMutability\": \"nonpayable\", \"type\": \"function\" }, { \"anonymous\": false, \"inputs\": [ { \"indexed\": false, \"internalType\": \"uint256\", \"name\": \"newValue\", \"type\": \"uint256\" }, { \"indexed\": false, \"internalType\": \"address\", \"name\": \"updatedBy\", \"type\": \"address\" } ], \"name\": \"storageUpdate\", \"type\": \"event\" }, { \"constant\": true, \"inputs\": [], \"name\": \"get\", \"outputs\": [ { \"internalType\": \"uint256\", \"name\": \"\", \"type\": \"uint256\" } ], \"payable\": false, \"stateMutability\": \"view\", \"type\": \"function\" } ] To create a new contract instance we run const simpleStorage = new web3.eth.Contract(ABI, SSaddress) . We can see that the simpleStorage contract object now has events, methods and an address that we provided with the ABI and contract address. We just need to set the web3 provider for the contract, which we can do with simpleStorage.setProvider(web3.givenProvider) . Now we can use this contract object to interact with the deployed contract. You will need some Rinkeby ETH to pay for gas to interact with the contract. You can get some via this link: https://www.rinkeby.io/#faucet . Read the contract state Let's read the current value of the storedData. Since our contract object is saved as simpleStorage, run simpleStorage.methods.get().call().then(console.log) . This will print the current storedData value of the contract. Since this is just reading the contract, there is no transaction sent to the network and there is no cost associated with this action. Update the contract state We update the contract by sending it a transaction to the \"set()\" function with the desired parameter. This action does cost gas, since we need to update the state of the contract on all of the network nodes. Feel free to update the value to whatever you want. simpleStorage.methods.set(3).send({from: web3.givenProvider.selectedAddress}) Running this code should trigger Metamask to ask you to sign a transaction. Once the transaction is mined, you can check for the transaction with simpleStorage.methods.get().call().then(console.log) or you can check the contract on a Rinkeby block explorer like rinkeby.etherscan.io. https://rinkeby.etherscan.io/address/0x49bb098e781ed5c50d85e82d85cba1a6f03fd3e6 Watch for events You can easily subscribe to events with simpleStorage. Notice we have a \"storageUpdate\" event in the contract. To listen for that event, run simpleStorage.events.storageUpdate(function(error, event){console.log(event)}) Here is a link to the relevant web3.js documentation for subscribing to events. To trigger this event, you will have to call the \"set()\" function on the contract again. Once the update transaction is mined, the event will fire. This is what it looks like in the browser console. This should give you a good overview of how to connect to a contract and interact with it using web3.js","title":"Index"},{"location":"S04-developer-tooling/M2-web3-libraries/L2-web3-connect-to-contract/#connect-to-the-network","text":"First, make sure that Metamask is connected to the Rinkeby network. To use the web3.js 1.x library, we need to initialize it again. We can do that by running web3 = new Web3(web3.givenProvider) in the browser console.","title":"Connect to the network"},{"location":"S04-developer-tooling/M2-web3-libraries/L2-web3-connect-to-contract/#initialize-the-contract","text":"Before we can interact with the contract, we need to initialize an instance of the SimpleStorage contract using web3.js. In this step we are going to provide web3.js with the ABI and the contract address, so it knows what functions are available at the address that we provide. The SimpleStorage.sol contract is deployed at address 0x49Bb098E781eD5C50D85E82d85cbA1a6F03FD3e6. Let's set the address in the console: const SSaddress = \"0x49Bb098E781eD5C50D85E82d85cbA1a6F03FD3e6\" Let's set the ABI in the console with: const ABI = [ { \"constant\": false, \"inputs\": [ { \"internalType\": \"uint256\", \"name\": \"x\", \"type\": \"uint256\" } ], \"name\": \"set\", \"outputs\": [], \"payable\": false, \"stateMutability\": \"nonpayable\", \"type\": \"function\" }, { \"anonymous\": false, \"inputs\": [ { \"indexed\": false, \"internalType\": \"uint256\", \"name\": \"newValue\", \"type\": \"uint256\" }, { \"indexed\": false, \"internalType\": \"address\", \"name\": \"updatedBy\", \"type\": \"address\" } ], \"name\": \"storageUpdate\", \"type\": \"event\" }, { \"constant\": true, \"inputs\": [], \"name\": \"get\", \"outputs\": [ { \"internalType\": \"uint256\", \"name\": \"\", \"type\": \"uint256\" } ], \"payable\": false, \"stateMutability\": \"view\", \"type\": \"function\" } ] To create a new contract instance we run const simpleStorage = new web3.eth.Contract(ABI, SSaddress) . We can see that the simpleStorage contract object now has events, methods and an address that we provided with the ABI and contract address. We just need to set the web3 provider for the contract, which we can do with simpleStorage.setProvider(web3.givenProvider) . Now we can use this contract object to interact with the deployed contract. You will need some Rinkeby ETH to pay for gas to interact with the contract. You can get some via this link: https://www.rinkeby.io/#faucet .","title":"Initialize the contract"},{"location":"S04-developer-tooling/M2-web3-libraries/L2-web3-connect-to-contract/#read-the-contract-state","text":"Let's read the current value of the storedData. Since our contract object is saved as simpleStorage, run simpleStorage.methods.get().call().then(console.log) . This will print the current storedData value of the contract. Since this is just reading the contract, there is no transaction sent to the network and there is no cost associated with this action.","title":"Read the contract state"},{"location":"S04-developer-tooling/M2-web3-libraries/L2-web3-connect-to-contract/#update-the-contract-state","text":"We update the contract by sending it a transaction to the \"set()\" function with the desired parameter. This action does cost gas, since we need to update the state of the contract on all of the network nodes. Feel free to update the value to whatever you want. simpleStorage.methods.set(3).send({from: web3.givenProvider.selectedAddress}) Running this code should trigger Metamask to ask you to sign a transaction. Once the transaction is mined, you can check for the transaction with simpleStorage.methods.get().call().then(console.log) or you can check the contract on a Rinkeby block explorer like rinkeby.etherscan.io. https://rinkeby.etherscan.io/address/0x49bb098e781ed5c50d85e82d85cba1a6f03fd3e6","title":"Update the contract state"},{"location":"S04-developer-tooling/M2-web3-libraries/L2-web3-connect-to-contract/#watch-for-events","text":"You can easily subscribe to events with simpleStorage. Notice we have a \"storageUpdate\" event in the contract. To listen for that event, run simpleStorage.events.storageUpdate(function(error, event){console.log(event)}) Here is a link to the relevant web3.js documentation for subscribing to events. To trigger this event, you will have to call the \"set()\" function on the contract again. Once the update transaction is mined, the event will fire. This is what it looks like in the browser console. This should give you a good overview of how to connect to a contract and interact with it using web3.js","title":"Watch for events"},{"location":"S04-developer-tooling/M3-infura/L1/","text":"In our section on \"Development Workflow\" and \"Users in the Mental Model,\" we described our interactions with the Ethereum network going through a network gateway. In the \"Ethereum Basics\" of the course, we talked about Ethereum clients, nodes, and how to interact with the network you need to run a full node. However, for developers just starting on Ethereum or users who don't have easy access (or knowledge) of an Ethereum node, you can use an Ethereum gateway service called Infura. Infura provides a simple API access point for not only the Ethereum mainnet and all public testnets, but also for IPFS, the Ethereum 2.0 Beacon Chain, Filecoin, Optimism, Arbitrum and Polygon. Incorporating Infura into your workflow will make deploying much easier. As your project grows, you can absolutely consider other options, but it's a nice, easy onramp to development. Follow the steps below to sign up for Infura (there are also great step-by-step instructions on Infura's website here ): Register First thing, register for a free account on Infura. For smaller development projects (or even medium-sized ones), Infura's free tier is more than capable of handling your requests. Coogan is currently running an Eth2 validator client on Infura's free tier (you'll see it in the screenshots below!) Setting Up Your First Project Once you've registered, you'll click the Ethereum logo on the left-hand side. (see below) You can also see the other available network endpoints listed here, which will be more since we took the above screenshot! Next, click \u201cCreate New Project\u201d in upper right hand corner: Name your project and go to \u201cSettings.\u201d There, you'll be able to access the credentials you'll use in your local environment (either Truffle or some other framework) to help make deployment easy. Again, these are your personal, sensitive credentials so be sure to store them in a .env file that you add to your .gitignore doc! Read more about that here. A .env file for our Infura credentials will look something like: MNEMONIC=\"Your MNEMONICs\" // A wallet with enough ETH INFURA_URL=\"Your Infura URL with API key\" source Incorporating Infura Into Your Development Environment Now let's see how we take our Infura credentials and plug them into a development framework like Truffle. For this, we're going to dive deeper into our truffle-config.js file As you might have gathered by now, the Truffle configuration file ( truffle-config.js ) is the backbone of a Truffle-based project. Thus far we\u2019ve only seen it used to store details of the different networks we\u2019re targeting (e.g. local, testnet, mainet, etc), but it\u2019s actually used for a lot more, such as network configuration. To illustrate this, let's look at the networks section of our truffle-config.js from our SimpleStorage example we've been working on in previous lessons. Here's a sample truffle-config.js file that targets the Ganache endpoint we setup earlier: module . exports = { networks : { development : { host : \"127.0.0.1\" , port : 8545 , network_id : \"*\" // Match any network id } }, compilers : { solc : { version : \"^0.8.0\" } } }; When we run truffle init earlier, though, the truffle-config.js contains a ton of helpful material that's been commented out. For example, here's a section under networks : // Useful for deploying to a public network. // NB: It's important to wrap the provider as a function. // ropsten: { // provider: () => new HDWalletProvider(mnemonic, `https://ropsten.infura.io/v3/YOUR-PROJECT-ID`), // network_id: 3, // Ropsten's id // gas: 5500000, // Ropsten has a lower block limit than mainnet // confirmations: 2, // # of confs to wait between deployments. (default: 0) // timeoutBlocks: 200, // # of blocks before a deployment times out (minimum/default: 50) // skipDryRun: true // Skip dry run before migrations? (default: false for public nets ) // }, To activate this, all we have to do is comment the code back in ( \u2318 / in VSCode) and substitute in the credentials we got from Infura. Note that you need to change the YOUR-PROJECT-ID and well as whatever subdomain network you'd like to work from. Be sure as well to change the network_id to the appropriate one for your network. To grab our Infura credentials safely from our .env file, you'll include this at the top of truffle-config.js (HDWallet is provided, we just need to comment it out): const HDWallet = require ( '@truffle/hdwallet-provider' ); const dotenv = require ( 'dotenv' ); dotenv . config (); const mnemonic = process . env . MNEMONIC ; (For reference, in the above snippet we're also using Truffle\u2019s HDWalletProvider library enabling us to use a custom mnemonic as part of the deployment.) We can change the name of our testnet to any one we'd like. Here's what it would look like for Rinkeby: rinkeby : { provider : () => new HDWalletProvider ( mnemonic , process . env . INFURA_URL ), network_id : \"4\" , gas : 5500000 } To migrate SimpleStorage to Rinkeby, we run the following command: $ truffle migrate --network rinkeby That's it! If you have enough Rinkeby test Eth in the account associated with your mnemonic phrase, your contract is now deployed to a public testnet using Infura! Later in the course, we'll discuss how Truffle and Infura can be used in the truffle-config.js file to deploy to multiple networks, including non-Ethereum networks.","title":"Index"},{"location":"S04-developer-tooling/M3-infura/L1/#register","text":"First thing, register for a free account on Infura. For smaller development projects (or even medium-sized ones), Infura's free tier is more than capable of handling your requests. Coogan is currently running an Eth2 validator client on Infura's free tier (you'll see it in the screenshots below!)","title":"Register"},{"location":"S04-developer-tooling/M3-infura/L1/#setting-up-your-first-project","text":"Once you've registered, you'll click the Ethereum logo on the left-hand side. (see below) You can also see the other available network endpoints listed here, which will be more since we took the above screenshot! Next, click \u201cCreate New Project\u201d in upper right hand corner: Name your project and go to \u201cSettings.\u201d There, you'll be able to access the credentials you'll use in your local environment (either Truffle or some other framework) to help make deployment easy. Again, these are your personal, sensitive credentials so be sure to store them in a .env file that you add to your .gitignore doc! Read more about that here. A .env file for our Infura credentials will look something like: MNEMONIC=\"Your MNEMONICs\" // A wallet with enough ETH INFURA_URL=\"Your Infura URL with API key\" source","title":"Setting Up Your First Project"},{"location":"S04-developer-tooling/M3-infura/L1/#incorporating-infura-into-your-development-environment","text":"Now let's see how we take our Infura credentials and plug them into a development framework like Truffle. For this, we're going to dive deeper into our truffle-config.js file As you might have gathered by now, the Truffle configuration file ( truffle-config.js ) is the backbone of a Truffle-based project. Thus far we\u2019ve only seen it used to store details of the different networks we\u2019re targeting (e.g. local, testnet, mainet, etc), but it\u2019s actually used for a lot more, such as network configuration. To illustrate this, let's look at the networks section of our truffle-config.js from our SimpleStorage example we've been working on in previous lessons. Here's a sample truffle-config.js file that targets the Ganache endpoint we setup earlier: module . exports = { networks : { development : { host : \"127.0.0.1\" , port : 8545 , network_id : \"*\" // Match any network id } }, compilers : { solc : { version : \"^0.8.0\" } } }; When we run truffle init earlier, though, the truffle-config.js contains a ton of helpful material that's been commented out. For example, here's a section under networks : // Useful for deploying to a public network. // NB: It's important to wrap the provider as a function. // ropsten: { // provider: () => new HDWalletProvider(mnemonic, `https://ropsten.infura.io/v3/YOUR-PROJECT-ID`), // network_id: 3, // Ropsten's id // gas: 5500000, // Ropsten has a lower block limit than mainnet // confirmations: 2, // # of confs to wait between deployments. (default: 0) // timeoutBlocks: 200, // # of blocks before a deployment times out (minimum/default: 50) // skipDryRun: true // Skip dry run before migrations? (default: false for public nets ) // }, To activate this, all we have to do is comment the code back in ( \u2318 / in VSCode) and substitute in the credentials we got from Infura. Note that you need to change the YOUR-PROJECT-ID and well as whatever subdomain network you'd like to work from. Be sure as well to change the network_id to the appropriate one for your network. To grab our Infura credentials safely from our .env file, you'll include this at the top of truffle-config.js (HDWallet is provided, we just need to comment it out): const HDWallet = require ( '@truffle/hdwallet-provider' ); const dotenv = require ( 'dotenv' ); dotenv . config (); const mnemonic = process . env . MNEMONIC ; (For reference, in the above snippet we're also using Truffle\u2019s HDWalletProvider library enabling us to use a custom mnemonic as part of the deployment.) We can change the name of our testnet to any one we'd like. Here's what it would look like for Rinkeby: rinkeby : { provider : () => new HDWalletProvider ( mnemonic , process . env . INFURA_URL ), network_id : \"4\" , gas : 5500000 } To migrate SimpleStorage to Rinkeby, we run the following command: $ truffle migrate --network rinkeby That's it! If you have enough Rinkeby test Eth in the account associated with your mnemonic phrase, your contract is now deployed to a public testnet using Infura! Later in the course, we'll discuss how Truffle and Infura can be used in the truffle-config.js file to deploy to multiple networks, including non-Ethereum networks.","title":"Incorporating Infura Into Your Development Environment"},{"location":"S04-developer-tooling/M4-truffle-deep-dive/L1-truffle-tests/","text":"Testing, 1, 2, 3 A comprehensive suite of tests adds robustness to your code as it evolves and Truffle provides an automated testing framework that makes adding this to your project a breeze. In the following examples, we\u2019ll be writing our tests in JavaScript, although Truffle also supports Solidity based tests too. Check out the following for when you might use one over the other. All your tests live in a dedicated tests directory, which is automatically created if you used truffle init to initialize your project, although you can of course create one after the fact. Go back to the SimpleStorage file directory we created in the previous lesson, \u201cIntro to Truffle -- Part II\u201d. To create your first test for SimpleStorage run the following: $ truffle create test SimpleStorage As with the earlier create command this creates a simple scaffold which includes a reference to the actual underlying contract artifact. Following this you can simply run the following to run the test suite. $ truffle test It\u2019s worth noting that the testing framework will temporarily spin up it\u2019s own Ganache with which to run the tests against (which it subsequently tears down), thus ensuring it doesn\u2019t pollute any existing instances you might have running. Let\u2019s try running it again with a more meaningful test. Feel free to copy and paste the following into your own test and try running truffle test again. contract ( \"SimpleStorage\" , function ( /* accounts */ ) { it ( \"should assert true\" , async function () { const simpleStorage = await SimpleStorage . deployed (); await simpleStorage . set ( 42 ); return assert . equal ( await simpleStorage . get (), 42 ); }); }); All going well everything should pass again and you\u2019ll see similar output to the following: Contract : SimpleStorage \u2713 should assert true ( 132 ms ) 1 passing ( 167 ms ) As can be inferred from the above example, tests are typically written using the AAA (Arrange, Act, Assert) pattern. In addition, you can access the accounts array and access to the web3 library. More detail can be found on tests here.","title":"Testing, 1, 2, 3"},{"location":"S04-developer-tooling/M4-truffle-deep-dive/L1-truffle-tests/#testing-1-2-3","text":"A comprehensive suite of tests adds robustness to your code as it evolves and Truffle provides an automated testing framework that makes adding this to your project a breeze. In the following examples, we\u2019ll be writing our tests in JavaScript, although Truffle also supports Solidity based tests too. Check out the following for when you might use one over the other. All your tests live in a dedicated tests directory, which is automatically created if you used truffle init to initialize your project, although you can of course create one after the fact. Go back to the SimpleStorage file directory we created in the previous lesson, \u201cIntro to Truffle -- Part II\u201d. To create your first test for SimpleStorage run the following: $ truffle create test SimpleStorage As with the earlier create command this creates a simple scaffold which includes a reference to the actual underlying contract artifact. Following this you can simply run the following to run the test suite. $ truffle test It\u2019s worth noting that the testing framework will temporarily spin up it\u2019s own Ganache with which to run the tests against (which it subsequently tears down), thus ensuring it doesn\u2019t pollute any existing instances you might have running. Let\u2019s try running it again with a more meaningful test. Feel free to copy and paste the following into your own test and try running truffle test again. contract ( \"SimpleStorage\" , function ( /* accounts */ ) { it ( \"should assert true\" , async function () { const simpleStorage = await SimpleStorage . deployed (); await simpleStorage . set ( 42 ); return assert . equal ( await simpleStorage . get (), 42 ); }); }); All going well everything should pass again and you\u2019ll see similar output to the following: Contract : SimpleStorage \u2713 should assert true ( 132 ms ) 1 passing ( 167 ms ) As can be inferred from the above example, tests are typically written using the AAA (Arrange, Act, Assert) pattern. In addition, you can access the accounts array and access to the web3 library. More detail can be found on tests here.","title":"Testing, 1, 2, 3"},{"location":"S04-developer-tooling/M4-truffle-deep-dive/L2-debug-config-forking/","text":"Squashing Bugs with the Debugger Debugging is an important part of any software development lifecycle and Truffle ships with a full CLI-based, interactive debugger to help you squash those pesky bugs. A debug instance is always instantiated off the back of a transaction (tx) hash (as we saw returned when we invoked storage.set(42) in the earlier example). For example: { tx: '0x46e4bb35108e5ecf7ff656008295fda572a753476d5e04c286fcdb7868447dd6', receipt: { transactionHash: '0x46e4bb35108e5ecf7ff656008295fda572a753476d5e04c286fcdb7868447dd6', transactionIndex: 0, blockHash: '0x85dbdf5d71194cb0d841d58bbac283ccf078ce0ebe1c054c6c2ab76442459894', blockNumber: 9, from: '0x5ca1605d4671669b38f7e37c881ed996ede5ac68', to: '0x524b2860a2489e385c5e12537f58d5a09a9d33ab', ... } Running the Debugger Assuming we have a valid transaction hash the debugger is simply invoked as follows. Note that you\u2019ll need to paste in a hash of a transaction that exists on the chain you\u2019re debugging against. truffle debug 0x4a1dcabb384e6ca1b5091495349603499fc2022e5832efdb53f872b6ff23a1c0 Assuming all is good, you should now see the following output (note that the full list of commands has been truncated for brevity): Starting Truffle Debugger ... Addresses affected : 0x6cA2F11a43b2B8f4DCE7De62f8Dc03f8E12BC48F - SimpleStorage Commands : ( enter ) last command entered ( step next ) ( o ) step over , ( i ) step into , ( u ) step out , ( n ) step next ( c ) continue until breakpoint , ( Y ) reset & continue to previous error ( y ) ( if at end ) reset & continue to final error (;) step instruction ( include number to step multiple ) SimpleStorage . sol : 2 : pragma solidity >= 0 . 4 . 21 < 0 . 7 . 0 ; 3 : 4 : contract SimpleStorage { ^^^^^^^^^^^^^^^^^^^^^^^^ You can now start stepping through your code in a manner similar to that of any traditional debugger. As stated in the Truffle docs though it\u2019s worth noting that \u201cyou're not running the code in real-time; instead, you're stepping over the historical execution of that transaction, and mapping that execution onto its associated code\u201d. In the above example, stepping over a couple of times brings us into our SimpleStorage.sol contract wherein we can see our storedData state variable being assigned its new value. SimpleStorage . sol : 7 : event setEvent ( uint newValue ); 8 : 9 : function set ( uint x ) public { ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ debug( develop : 0 x8bd62b08 ... ) > o SimpleStorage . sol : 8 : 9 : function set ( uint x ) public { 10 : storedData = x ; In-test Debugging Lastly, a feature available as of Truffle v5.1 is that of in-test debugging. This essentially enables you to interrupt your tests by simply wrapping a given line with await debug(). More detail on in-test debugging with a simple example here .","title":"Index"},{"location":"S04-developer-tooling/M4-truffle-deep-dive/L2-debug-config-forking/#squashing-bugs-with-the-debugger","text":"Debugging is an important part of any software development lifecycle and Truffle ships with a full CLI-based, interactive debugger to help you squash those pesky bugs. A debug instance is always instantiated off the back of a transaction (tx) hash (as we saw returned when we invoked storage.set(42) in the earlier example). For example: { tx: '0x46e4bb35108e5ecf7ff656008295fda572a753476d5e04c286fcdb7868447dd6', receipt: { transactionHash: '0x46e4bb35108e5ecf7ff656008295fda572a753476d5e04c286fcdb7868447dd6', transactionIndex: 0, blockHash: '0x85dbdf5d71194cb0d841d58bbac283ccf078ce0ebe1c054c6c2ab76442459894', blockNumber: 9, from: '0x5ca1605d4671669b38f7e37c881ed996ede5ac68', to: '0x524b2860a2489e385c5e12537f58d5a09a9d33ab', ... }","title":"Squashing Bugs with the Debugger"},{"location":"S04-developer-tooling/M4-truffle-deep-dive/L2-debug-config-forking/#running-the-debugger","text":"Assuming we have a valid transaction hash the debugger is simply invoked as follows. Note that you\u2019ll need to paste in a hash of a transaction that exists on the chain you\u2019re debugging against. truffle debug 0x4a1dcabb384e6ca1b5091495349603499fc2022e5832efdb53f872b6ff23a1c0 Assuming all is good, you should now see the following output (note that the full list of commands has been truncated for brevity): Starting Truffle Debugger ... Addresses affected : 0x6cA2F11a43b2B8f4DCE7De62f8Dc03f8E12BC48F - SimpleStorage Commands : ( enter ) last command entered ( step next ) ( o ) step over , ( i ) step into , ( u ) step out , ( n ) step next ( c ) continue until breakpoint , ( Y ) reset & continue to previous error ( y ) ( if at end ) reset & continue to final error (;) step instruction ( include number to step multiple ) SimpleStorage . sol : 2 : pragma solidity >= 0 . 4 . 21 < 0 . 7 . 0 ; 3 : 4 : contract SimpleStorage { ^^^^^^^^^^^^^^^^^^^^^^^^ You can now start stepping through your code in a manner similar to that of any traditional debugger. As stated in the Truffle docs though it\u2019s worth noting that \u201cyou're not running the code in real-time; instead, you're stepping over the historical execution of that transaction, and mapping that execution onto its associated code\u201d. In the above example, stepping over a couple of times brings us into our SimpleStorage.sol contract wherein we can see our storedData state variable being assigned its new value. SimpleStorage . sol : 7 : event setEvent ( uint newValue ); 8 : 9 : function set ( uint x ) public { ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ debug( develop : 0 x8bd62b08 ... ) > o SimpleStorage . sol : 8 : 9 : function set ( uint x ) public { 10 : storedData = x ;","title":"Running the Debugger"},{"location":"S04-developer-tooling/M4-truffle-deep-dive/L2-debug-config-forking/#in-test-debugging","text":"Lastly, a feature available as of Truffle v5.1 is that of in-test debugging. This essentially enables you to interrupt your tests by simply wrapping a given line with await debug(). More detail on in-test debugging with a simple example here .","title":"In-test Debugging"},{"location":"S04-developer-tooling/M4-truffle-deep-dive/L3-drizzle-ui/","text":"","title":"Index"},{"location":"S04-developer-tooling/M5-other-dev-tools/","text":"Other Development Tools Before we dive into these other development frameworks, we wanted to provide an \"offramp\" to folks who may not be comfortable with Javascript Frameworks. We'd encourage you to check out the sections on Node and Javascript Frameworks in the Basic Training course. People uncomfortable with terminal-based development might be interested in JSUI, a GUI-based Javascript framework development environment. You can also checkout some frontend boilerplate projects here. Hardhat Another popular development framework is Hardhat, which actually started as a fork of Truffle. It has since grown to create its own suite of tools and a devoted community. Hardhat divides itself into \"tasks\" and \"plugins\". Running npx hardhat compile is a task, for example. Plugins are extended functionality ported into Hardhat. Gas Reporter and Contract Sizer are two popular plugins for Hardhat. Why choose Hardhat? Some feel as though the command-line experience of Hardhat is faster than Truffle. Others like the extensive plugin features. One feature popular with Hardhat developers is their use of console.log() in smart contracts. When developing locally with Hardhat, you can import the console.sol contract, like so: pragma solidity ^0.6.0; import \"hardhat/console.sol\"; contract Token { //... } You can then add it to your contract when developing it locally: function transfer(address to, uint256 amount) external { console.log(\"Sender balance is %s tokens\", balances[msg.sender]); console.log(\"Trying to send %s tokens to %s\", amount, to); require(balances[msg.sender] >= amount, \"Not enough tokens\"); balances[msg.sender] -= amount; balances[to] += amount; } Which gives you this output when running locally on the Hardhat Network: $ npx hardhat test Token contract Deployment \u2713 Should set the right owner \u2713 Should assign the total supply of tokens to the owner Transactions Sender balance is 1000 tokens Trying to send 50 tokens to 0xead9c93b79ae7c1591b1fb5323bd777e86e150d4 Sender balance is 50 tokens Trying to send 50 tokens to 0xe5904695748fe4a84b40b3fc79de2277660bd1d3 \u2713 Should transfer tokens between accounts (373ms) \u2713 Should fail if sender doesn\u2019t have enough tokens Sender balance is 1000 tokens Trying to send 100 tokens to 0xead9c93b79ae7c1591b1fb5323bd777e86e150d4 Sender balance is 900 tokens Trying to send 100 tokens to 0xe5904695748fe4a84b40b3fc79de2277660bd1d3 \u2713 Should update balances after transfers (187ms) 5 passing (2s) You can learn more about this feature in their documentation here. Here are some easy ways to get started using Hardhat to see how you like it: - Hardhat's Beginner Tutorial and Project Setup - Hardhat Development Network - Index of Reusable Plugins The Hardhat teams recommends \"paying attention to see whether any plugins already solve problems [you] may have.\" Scaffold-ETH Scaffold-ETH ( docs ) is a project from prolific builder Austin Griffith meant to minimize the time between thinking of a decentralized app idea and deploying it to the world. However, Scaffold-ETH requires an advanced comfortability with tools like Yarn, Solidity, Hardhat, React, etc. It's best for folks who already have a very solid Web 2 or Web 3 workflow. For those folks, Scaffold-ETH is jet fuel! Please note, however, that projects and tutorials in Scaffold-ETH have not been audited in any way and may contain bugs or vulnerabilities! In the repo, Austin has provided a number of forks that correspond to different template projects or tutorials. Read more about the tutorials and examples here. As we mentioned, Scaffold-ETH by default serves up a React App, with pre-built components, and hooks. It's also incorporated a third-party UI library called Ant Design to help with the designing of components. It also incorporates Surge, a static-site generator to publish your app. Scaffold-ETH also has significant infrastructure support for the later parts of the development cycle, such as The Graph, Tenderly, Etherscan, and L2/Sidechain Services (deploying to Optimism and Arbitrum). Also, there are examples from some great projects in the space, like Aave, The Graph, Chainlink, Optimism and Uniswap. You can also learn about common design patterns such as commit-reveal, ecrecover, multisigs, DEXes, and more! Last, Austin is an incredibly proflific creator of content, including support videos and walkthroughs of Scaffold-ETH. Below are some recent walkthroughs he's done: Start Building Today with Scaffold-ETH Ethereum Dev Onboarding (ETHGlobal) Austin walksthrough Scaffold-ETH after RicMoo and Paul Berg walkthrough their awesome tools Web 2 dev to Web 3 dev Blockchain at Berkeley: Austin Griffith Developer Walkthrough If you follow Austin on twitter or join the Scaffold-ETH Telegram, you'll get updates and assistance there as well. Brownie As we mentioned earlier, Brownie is a Python-based development and testing framework for smart contracts running on the EVM. It uses Web3.py as well as Solidity. It is most well-known for being the development framework the Yearn.Finance team uses to build their powerful DeFi platform and CRV. Brownie definitely takes some notes from Truffle (they are both \"sweet\"){target=_blank}, including having a brownie init command and their equivalent of Truffle boxes, \"Brownie Mixes.\" This makes it an easy tool for a lot of Truffle developers familiar with Truffle. Here are some tutorials to introduce you to Brownie: Tutorial: Develop a DeFi Project using Python (Chainlink){target=_blank} Tutorial: Vyper and Brownie Contract Development on EVM Chains An interesting tutorial walking through developing a contract using Vyper and Brownie. Tenderly Tenderly provides a way to both be alerted to contract events as well as troubleshoot a contract, including a \"Forking Mainnet\" feature similar to that of Truffle and Hardhat. Frontend Tools For your project, we've also discussed frontend interfaces. There are two services you can use for free to create a frontend instance easily: - Heroku is a Platform-as-a-Service, providing a quick way to deploy apps in a number of popular languages, such as Node.js (Javascript), Python, Ruby and Go. You can connect your Github repo to your project for easy deployment. Heroku's basic plan is free and provides basic resources for getting started. Read more here. - Netlify also has a Git-based workflow allowing you to deploy your project easily from Github. It is also free and has a getting started manual here. There are so many more amazing tools that we can't get into now, but if you check out the links in this section, you'll be able to dive deeper and learn more on your own! Additional Material Article: The Complete Guide to Full Stack Ethereum Development An incredibly comprehensive article from Nader Dabit of The Graph. Ethereum Dev Onboarding (ETHGlobal) Walkthrough of the developer stack from RicMoo, Paul Berg ( create-eth-app ) and Austin Griffith. Great walkthrough of dev environments! Wiki: DeFi Developer Roadmap: Other Frameworks A series of other frameworks within a larger, excellent repo about Development tools and tips Article: Build a Web3 Dapp in React Login with MetaMask Tutorial: React Project Setup Using Hardhat & Truffle Part I From Bootcamp alum and fellow Shih-Yu Hwang Google Doc: Overview of Tools From Vu, a bootcamp alum, these are the notes he made after going through the developer section as a helpful mindmap for others! Other tools: Ankr, Quiknode, Alchemy","title":"Other Development Tools"},{"location":"S04-developer-tooling/M5-other-dev-tools/#other-development-tools","text":"Before we dive into these other development frameworks, we wanted to provide an \"offramp\" to folks who may not be comfortable with Javascript Frameworks. We'd encourage you to check out the sections on Node and Javascript Frameworks in the Basic Training course. People uncomfortable with terminal-based development might be interested in JSUI, a GUI-based Javascript framework development environment. You can also checkout some frontend boilerplate projects here.","title":"Other Development Tools"},{"location":"S04-developer-tooling/M5-other-dev-tools/#hardhat","text":"Another popular development framework is Hardhat, which actually started as a fork of Truffle. It has since grown to create its own suite of tools and a devoted community. Hardhat divides itself into \"tasks\" and \"plugins\". Running npx hardhat compile is a task, for example. Plugins are extended functionality ported into Hardhat. Gas Reporter and Contract Sizer are two popular plugins for Hardhat. Why choose Hardhat? Some feel as though the command-line experience of Hardhat is faster than Truffle. Others like the extensive plugin features. One feature popular with Hardhat developers is their use of console.log() in smart contracts. When developing locally with Hardhat, you can import the console.sol contract, like so: pragma solidity ^0.6.0; import \"hardhat/console.sol\"; contract Token { //... } You can then add it to your contract when developing it locally: function transfer(address to, uint256 amount) external { console.log(\"Sender balance is %s tokens\", balances[msg.sender]); console.log(\"Trying to send %s tokens to %s\", amount, to); require(balances[msg.sender] >= amount, \"Not enough tokens\"); balances[msg.sender] -= amount; balances[to] += amount; } Which gives you this output when running locally on the Hardhat Network: $ npx hardhat test Token contract Deployment \u2713 Should set the right owner \u2713 Should assign the total supply of tokens to the owner Transactions Sender balance is 1000 tokens Trying to send 50 tokens to 0xead9c93b79ae7c1591b1fb5323bd777e86e150d4 Sender balance is 50 tokens Trying to send 50 tokens to 0xe5904695748fe4a84b40b3fc79de2277660bd1d3 \u2713 Should transfer tokens between accounts (373ms) \u2713 Should fail if sender doesn\u2019t have enough tokens Sender balance is 1000 tokens Trying to send 100 tokens to 0xead9c93b79ae7c1591b1fb5323bd777e86e150d4 Sender balance is 900 tokens Trying to send 100 tokens to 0xe5904695748fe4a84b40b3fc79de2277660bd1d3 \u2713 Should update balances after transfers (187ms) 5 passing (2s) You can learn more about this feature in their documentation here. Here are some easy ways to get started using Hardhat to see how you like it: - Hardhat's Beginner Tutorial and Project Setup - Hardhat Development Network - Index of Reusable Plugins The Hardhat teams recommends \"paying attention to see whether any plugins already solve problems [you] may have.\"","title":"Hardhat"},{"location":"S04-developer-tooling/M5-other-dev-tools/#scaffold-eth","text":"Scaffold-ETH ( docs ) is a project from prolific builder Austin Griffith meant to minimize the time between thinking of a decentralized app idea and deploying it to the world. However, Scaffold-ETH requires an advanced comfortability with tools like Yarn, Solidity, Hardhat, React, etc. It's best for folks who already have a very solid Web 2 or Web 3 workflow. For those folks, Scaffold-ETH is jet fuel! Please note, however, that projects and tutorials in Scaffold-ETH have not been audited in any way and may contain bugs or vulnerabilities! In the repo, Austin has provided a number of forks that correspond to different template projects or tutorials. Read more about the tutorials and examples here. As we mentioned, Scaffold-ETH by default serves up a React App, with pre-built components, and hooks. It's also incorporated a third-party UI library called Ant Design to help with the designing of components. It also incorporates Surge, a static-site generator to publish your app. Scaffold-ETH also has significant infrastructure support for the later parts of the development cycle, such as The Graph, Tenderly, Etherscan, and L2/Sidechain Services (deploying to Optimism and Arbitrum). Also, there are examples from some great projects in the space, like Aave, The Graph, Chainlink, Optimism and Uniswap. You can also learn about common design patterns such as commit-reveal, ecrecover, multisigs, DEXes, and more! Last, Austin is an incredibly proflific creator of content, including support videos and walkthroughs of Scaffold-ETH. Below are some recent walkthroughs he's done: Start Building Today with Scaffold-ETH Ethereum Dev Onboarding (ETHGlobal) Austin walksthrough Scaffold-ETH after RicMoo and Paul Berg walkthrough their awesome tools Web 2 dev to Web 3 dev Blockchain at Berkeley: Austin Griffith Developer Walkthrough If you follow Austin on twitter or join the Scaffold-ETH Telegram, you'll get updates and assistance there as well.","title":"Scaffold-ETH"},{"location":"S04-developer-tooling/M5-other-dev-tools/#brownie","text":"As we mentioned earlier, Brownie is a Python-based development and testing framework for smart contracts running on the EVM. It uses Web3.py as well as Solidity. It is most well-known for being the development framework the Yearn.Finance team uses to build their powerful DeFi platform and CRV. Brownie definitely takes some notes from Truffle (they are both \"sweet\"){target=_blank}, including having a brownie init command and their equivalent of Truffle boxes, \"Brownie Mixes.\" This makes it an easy tool for a lot of Truffle developers familiar with Truffle. Here are some tutorials to introduce you to Brownie: Tutorial: Develop a DeFi Project using Python (Chainlink){target=_blank} Tutorial: Vyper and Brownie Contract Development on EVM Chains An interesting tutorial walking through developing a contract using Vyper and Brownie.","title":"Brownie"},{"location":"S04-developer-tooling/M5-other-dev-tools/#tenderly","text":"Tenderly provides a way to both be alerted to contract events as well as troubleshoot a contract, including a \"Forking Mainnet\" feature similar to that of Truffle and Hardhat.","title":"Tenderly"},{"location":"S04-developer-tooling/M5-other-dev-tools/#frontend-tools","text":"For your project, we've also discussed frontend interfaces. There are two services you can use for free to create a frontend instance easily: - Heroku is a Platform-as-a-Service, providing a quick way to deploy apps in a number of popular languages, such as Node.js (Javascript), Python, Ruby and Go. You can connect your Github repo to your project for easy deployment. Heroku's basic plan is free and provides basic resources for getting started. Read more here. - Netlify also has a Git-based workflow allowing you to deploy your project easily from Github. It is also free and has a getting started manual here. There are so many more amazing tools that we can't get into now, but if you check out the links in this section, you'll be able to dive deeper and learn more on your own!","title":"Frontend Tools"},{"location":"S04-developer-tooling/M5-other-dev-tools/#additional-material","text":"Article: The Complete Guide to Full Stack Ethereum Development An incredibly comprehensive article from Nader Dabit of The Graph. Ethereum Dev Onboarding (ETHGlobal) Walkthrough of the developer stack from RicMoo, Paul Berg ( create-eth-app ) and Austin Griffith. Great walkthrough of dev environments! Wiki: DeFi Developer Roadmap: Other Frameworks A series of other frameworks within a larger, excellent repo about Development tools and tips Article: Build a Web3 Dapp in React Login with MetaMask Tutorial: React Project Setup Using Hardhat & Truffle Part I From Bootcamp alum and fellow Shih-Yu Hwang Google Doc: Overview of Tools From Vu, a bootcamp alum, these are the notes he made after going through the developer section as a helpful mindmap for others! Other tools: Ankr, Quiknode, Alchemy","title":"Additional Material"},{"location":"S05a-defi/M1-intro/L1-what-is-defi/","text":"What is DeFi? Finance is defined as the management of money and includes activities like saving, borrowing, lending, investing, budgeting, and forecasting. Finance can be seen as a societal tool to manage resources, risk, and rewards across space and time between entities. Decentralized Finance (DeFi) is a term used for software built on programmable open-source blockchains using smart contracts. The aim is to transform traditional financial products into reliable, permissionless, transparent, and censorship-resistant protocols without centralized intermediaries. In some ways, DeFi protocol devs are writing firmware for scalable social collaboration and resource allocation. They extend the internet of value to more complex activities. DeFi protocols share three interesting characteristics among many: Interoperability: The ability to share data and work with each other Composability: money legos, aka they be used in different ways to create new things Immutability: the inability for the core logic to be changed, allowing for more trust. Permissionless: the ability to interact with a protocol without third party permission Auditable: the history of its activities can be investigated by anyone These properties give developers the ability to string together a series of protocols to create new and exciting applications. DeFi also addresses issues in centralized finance related to: Centralized control Limited access and options Lack of interoperability and portability Lack of Transparency High cost Misaligned incentives DeFi apps aim to increase choice, innovation, access, efficiency, speed, transparency, auditability and autonomy while lowering the costs of doing business and reducing systemic risk from too big to fail entities. Anyone with an internet connection can access DeFi Apps and protocols, extending the internet of information to the \u201cinternet of value\u201d.","title":"What is DeFi?"},{"location":"S05a-defi/M1-intro/L1-what-is-defi/#what-is-defi","text":"Finance is defined as the management of money and includes activities like saving, borrowing, lending, investing, budgeting, and forecasting. Finance can be seen as a societal tool to manage resources, risk, and rewards across space and time between entities. Decentralized Finance (DeFi) is a term used for software built on programmable open-source blockchains using smart contracts. The aim is to transform traditional financial products into reliable, permissionless, transparent, and censorship-resistant protocols without centralized intermediaries. In some ways, DeFi protocol devs are writing firmware for scalable social collaboration and resource allocation. They extend the internet of value to more complex activities. DeFi protocols share three interesting characteristics among many: Interoperability: The ability to share data and work with each other Composability: money legos, aka they be used in different ways to create new things Immutability: the inability for the core logic to be changed, allowing for more trust. Permissionless: the ability to interact with a protocol without third party permission Auditable: the history of its activities can be investigated by anyone These properties give developers the ability to string together a series of protocols to create new and exciting applications. DeFi also addresses issues in centralized finance related to: Centralized control Limited access and options Lack of interoperability and portability Lack of Transparency High cost Misaligned incentives DeFi apps aim to increase choice, innovation, access, efficiency, speed, transparency, auditability and autonomy while lowering the costs of doing business and reducing systemic risk from too big to fail entities. Anyone with an internet connection can access DeFi Apps and protocols, extending the internet of information to the \u201cinternet of value\u201d.","title":"What is DeFi?"},{"location":"S05a-defi/M1-intro/L2-key-terms/","text":"Key Terms To understand decentralized finance, we must first understand some key terms in finance. Then we can look into the essential key terms that are unique to DeFi. As mentioned, finance can be defined as the \"study of systems of money, investments, and other financial instruments\". It can be seen as a societal tool to manage resources nominated in money, risk, and rewards across space and time between entities. DeFi upgrades this to occur on the \"internet of value\" via programmable blockchains like Ethereum with protocols written in smart contracts. Economics in 30 seconds A key foundation of finance is economics which deals with how people interact with value , specifically the production, distribution, and consumption of goods and services. Core to this idea is the concept of scarcity , or simply the gap between limited resources and unlimited wants. To solve this problem, we look towards allocating resources to the most productive uses. We use market-based systems based on property rights to allow individual entities to own and allocate resources to where they best see fit. From this arises supply and demand, another central idea in economics. In finance, scarcity, allocation and supply and demand translate into deploying limited amounts of capital across a range of possible investments. As we will see later, smart contract based systems make it easy to enforce and audit ownership, and what owners can do with their digital assets. Key Financial Terms That Appear in DeFi Finance extends economics to the management of money and investments. Money has certain properties , some of which is the ability to quantify and transfer value. The most common example of money is fiat currency , a type of money a government has declared legal tender that allows for the settlement of private or public debts within a political jurisdiction. Examples of fiat currencies are the United States Dollar $, European Union's Euro \u20ac, The People's Republic of China's Renminbi RMB, Japanese Yen \u00a5, South African Rand R. Once we have money, we want to grow it. This is because inflation slowly eats away at the value of money. A dollar today is worth less than a dollar 20 years ago. So we must invest our money to keep up. In finance, that means investing in financial instruments defined as assets or bundles of capital represented by real or virtual documents. Not all opportunities or financial assets are equal. Out of this emerges a central idea in finance: risk , which is the probability of losing money. Risk for a financial product can be affected by the type of asset, time an asset is held, liquidity for the asset, depreciation, volatility and more. From risk emerges the risk-return trade off that states the potential return rises with an increase in risk. Individuals use this to assess an investment and consider many factors, like their overall risk tolerance, the potential to replace lost funds, the investment's return in relation to other assets and more. In simple terms, the riskier the project, the greater the expected. A little more context can be found here . \ud83d\uddbc Add Risk Reward Line here. In the crypto industry and DeFi, one of the main sources of risk is due to volatility . Volatility is the unsteady spread of an asset's price movements around its average price. The greater the swings, the more volatile, the riskier the asset is perceived. Volatility can be measured in different ways. You can see volatility in action here . \ud83d\uddbc Add Volatility graph. This volatility contributes to exchange rate risk , or the risk of devaluation of one currency compared to another. An example can be holding ETH compared to the US dollar, then ETH's price drops 15% in relation to the dollar. This problem is further compounded by holding two volatile currencies in relation to each other. Volatility emerges from various sources including market liquidity risk , or the lack of buyers and sellers. Liquidity is the ease in which you can buy or sell an asset. The less liquidity an asset has, the greater the volatility of an asset's price when buying or selling. Liquidity is the lifeblood of DeFi , since large amounts of liquidity also helps to create price stability. Illiquid assets have wilder price swings, making them less predictable, risker for network participants, and contributes to higher slippage . Slippage occurs when a trade settles for an average price that is different from what was requested. \ud83d\uddbc Add liquidity Think about a market of lots of buyers and sellers for an asset. A deep pool of buyers and sellers results in a narrow bid-ask spread , since you can find alternative takers if someone quotes a price that deviates from the crowd. In DeFi, slippage is most often seen when trading niche tokens with small networks or limited liquidity on decentralized exchanges, covered in a later section. \ud83d\uddbc Add Bid-Ask Spread picture To help reduce volatility and risk in DeFi, stablecoins were created. Stablecoins are fungible tokens that mimic a fiat currency\u2019s performance through a peg . Fungibility is the ability of a good or asset to be exchanged for another of the same kind, like exchanging two different dollars. A peg is a system by which one currency latches on to another to form a tight correlation. Stablecoins can maintain this peg in a variety of ways discussed in a later section. One method is through the use of collateralization and liquidation. DeFi protocols rely on collateral to allow for permissionless participation. Collateral is an asset a borrower pledges to a lender in case they cannot pay back a loan. In DeFi, collateral is in the form of crypto assets like ETH, other tokens with deep levels of liquidity. Even NFTs, which are non-fungible tokens issued to prove ownership of a unique digital asset, can be staked as collateral. Staking refers to depositing assets into an escrow contract thereby passing custody of the collateral to the protocol. \ud83d\uddbc Add Staking Collateral Visualization Due to the volatility of crypto assets and the lack of decentralized credit systems, most DeFi protocols rely on the overcollateralization pattern to issue assets. This means requiring to cover over 100% of the loan. For example, say you wish to get $100 in one crypto asset. You will need $100 of collateral value in another asset. These assets are in sense collateralized loans and are the backbone of DeFi because they allow open, pseudo-anonymous finance , without credit scores or any sort of formal identity tied to a loan. Through overcollateralization, protocols can mitigate their risk, while providing access and possibilities for returns. This functioning is common in DeFi protocols and is explained here . The loan to collateral ratio expresses the degree to which a protocol is overcollateralized. The total value locked represents the amount of funds in a contract and is a proxy for contract's popularity. Due to volatility, the value of the collateral can drop below the value of the borrowed asset. When this occurs DeFi protocols typically begin to liquidate the users assets to maintain stability. Liquidation is the process of selling/distributing assets to settle debts. This is one of the ways a user can get rekt . Another is via rug pulls , where protocol developers run off with the assets, or DeFi hacks , where malicious actors exploit a bug in the contract code . Many protocol developers aim to achieve deep liquidity, broad usage and network effects . The ideal point culminates in a Schelling Point or a natural place of collaboration, brand awareness, and user evangelism. To achieve this, protocol developers rely on incentives to encourage and discourage certain types of behavior. One method to bootstrap the network is via liquidity mining programs. These programs can be seen as part of a marketing program to onboard users to engage with protocol. Users engage in yield farming to find the best APY by shifting capital across various places. \ud83d\uddbc Add Liquidity Mining Picture There are various protocols like Yearn.Finance which help users automate strategies to maximize APY and the lowest cost through the use of vaults {target=_blank}. There are some indicators that DeFi protocol Devs should know {target=_blank}. Additional Resources DeFi and the Future of Finance by Campbell R. Harvey (Duke University){target=_blank}, Ashwin Ramachandran (Dragonfly Capital){target=_blank} and Joey Santoro (Fei Protocol){target=_blank} page 1 - 12 DeFi Primitives Via Campbell Harvey - Duke University The Future of DeFi by Linda Xie The Network Effects Manual by nfx","title":"Key Terms"},{"location":"S05a-defi/M1-intro/L2-key-terms/#key-terms","text":"To understand decentralized finance, we must first understand some key terms in finance. Then we can look into the essential key terms that are unique to DeFi. As mentioned, finance can be defined as the \"study of systems of money, investments, and other financial instruments\". It can be seen as a societal tool to manage resources nominated in money, risk, and rewards across space and time between entities. DeFi upgrades this to occur on the \"internet of value\" via programmable blockchains like Ethereum with protocols written in smart contracts.","title":"Key Terms"},{"location":"S05a-defi/M1-intro/L2-key-terms/#economics-in-30-seconds","text":"A key foundation of finance is economics which deals with how people interact with value , specifically the production, distribution, and consumption of goods and services. Core to this idea is the concept of scarcity , or simply the gap between limited resources and unlimited wants. To solve this problem, we look towards allocating resources to the most productive uses. We use market-based systems based on property rights to allow individual entities to own and allocate resources to where they best see fit. From this arises supply and demand, another central idea in economics. In finance, scarcity, allocation and supply and demand translate into deploying limited amounts of capital across a range of possible investments. As we will see later, smart contract based systems make it easy to enforce and audit ownership, and what owners can do with their digital assets.","title":"Economics in 30 seconds"},{"location":"S05a-defi/M1-intro/L2-key-terms/#key-financial-terms-that-appear-in-defi","text":"Finance extends economics to the management of money and investments. Money has certain properties , some of which is the ability to quantify and transfer value. The most common example of money is fiat currency , a type of money a government has declared legal tender that allows for the settlement of private or public debts within a political jurisdiction. Examples of fiat currencies are the United States Dollar $, European Union's Euro \u20ac, The People's Republic of China's Renminbi RMB, Japanese Yen \u00a5, South African Rand R. Once we have money, we want to grow it. This is because inflation slowly eats away at the value of money. A dollar today is worth less than a dollar 20 years ago. So we must invest our money to keep up. In finance, that means investing in financial instruments defined as assets or bundles of capital represented by real or virtual documents. Not all opportunities or financial assets are equal. Out of this emerges a central idea in finance: risk , which is the probability of losing money. Risk for a financial product can be affected by the type of asset, time an asset is held, liquidity for the asset, depreciation, volatility and more. From risk emerges the risk-return trade off that states the potential return rises with an increase in risk. Individuals use this to assess an investment and consider many factors, like their overall risk tolerance, the potential to replace lost funds, the investment's return in relation to other assets and more. In simple terms, the riskier the project, the greater the expected. A little more context can be found here . \ud83d\uddbc Add Risk Reward Line here. In the crypto industry and DeFi, one of the main sources of risk is due to volatility . Volatility is the unsteady spread of an asset's price movements around its average price. The greater the swings, the more volatile, the riskier the asset is perceived. Volatility can be measured in different ways. You can see volatility in action here . \ud83d\uddbc Add Volatility graph. This volatility contributes to exchange rate risk , or the risk of devaluation of one currency compared to another. An example can be holding ETH compared to the US dollar, then ETH's price drops 15% in relation to the dollar. This problem is further compounded by holding two volatile currencies in relation to each other. Volatility emerges from various sources including market liquidity risk , or the lack of buyers and sellers. Liquidity is the ease in which you can buy or sell an asset. The less liquidity an asset has, the greater the volatility of an asset's price when buying or selling. Liquidity is the lifeblood of DeFi , since large amounts of liquidity also helps to create price stability. Illiquid assets have wilder price swings, making them less predictable, risker for network participants, and contributes to higher slippage . Slippage occurs when a trade settles for an average price that is different from what was requested. \ud83d\uddbc Add liquidity Think about a market of lots of buyers and sellers for an asset. A deep pool of buyers and sellers results in a narrow bid-ask spread , since you can find alternative takers if someone quotes a price that deviates from the crowd. In DeFi, slippage is most often seen when trading niche tokens with small networks or limited liquidity on decentralized exchanges, covered in a later section. \ud83d\uddbc Add Bid-Ask Spread picture To help reduce volatility and risk in DeFi, stablecoins were created. Stablecoins are fungible tokens that mimic a fiat currency\u2019s performance through a peg . Fungibility is the ability of a good or asset to be exchanged for another of the same kind, like exchanging two different dollars. A peg is a system by which one currency latches on to another to form a tight correlation. Stablecoins can maintain this peg in a variety of ways discussed in a later section. One method is through the use of collateralization and liquidation. DeFi protocols rely on collateral to allow for permissionless participation. Collateral is an asset a borrower pledges to a lender in case they cannot pay back a loan. In DeFi, collateral is in the form of crypto assets like ETH, other tokens with deep levels of liquidity. Even NFTs, which are non-fungible tokens issued to prove ownership of a unique digital asset, can be staked as collateral. Staking refers to depositing assets into an escrow contract thereby passing custody of the collateral to the protocol. \ud83d\uddbc Add Staking Collateral Visualization Due to the volatility of crypto assets and the lack of decentralized credit systems, most DeFi protocols rely on the overcollateralization pattern to issue assets. This means requiring to cover over 100% of the loan. For example, say you wish to get $100 in one crypto asset. You will need $100 of collateral value in another asset. These assets are in sense collateralized loans and are the backbone of DeFi because they allow open, pseudo-anonymous finance , without credit scores or any sort of formal identity tied to a loan. Through overcollateralization, protocols can mitigate their risk, while providing access and possibilities for returns. This functioning is common in DeFi protocols and is explained here . The loan to collateral ratio expresses the degree to which a protocol is overcollateralized. The total value locked represents the amount of funds in a contract and is a proxy for contract's popularity. Due to volatility, the value of the collateral can drop below the value of the borrowed asset. When this occurs DeFi protocols typically begin to liquidate the users assets to maintain stability. Liquidation is the process of selling/distributing assets to settle debts. This is one of the ways a user can get rekt . Another is via rug pulls , where protocol developers run off with the assets, or DeFi hacks , where malicious actors exploit a bug in the contract code . Many protocol developers aim to achieve deep liquidity, broad usage and network effects . The ideal point culminates in a Schelling Point or a natural place of collaboration, brand awareness, and user evangelism. To achieve this, protocol developers rely on incentives to encourage and discourage certain types of behavior. One method to bootstrap the network is via liquidity mining programs. These programs can be seen as part of a marketing program to onboard users to engage with protocol. Users engage in yield farming to find the best APY by shifting capital across various places. \ud83d\uddbc Add Liquidity Mining Picture There are various protocols like Yearn.Finance which help users automate strategies to maximize APY and the lowest cost through the use of vaults {target=_blank}. There are some indicators that DeFi protocol Devs should know {target=_blank}.","title":"Key Financial Terms That Appear in DeFi"},{"location":"S05a-defi/M1-intro/L2-key-terms/#additional-resources","text":"DeFi and the Future of Finance by Campbell R. Harvey (Duke University){target=_blank}, Ashwin Ramachandran (Dragonfly Capital){target=_blank} and Joey Santoro (Fei Protocol){target=_blank} page 1 - 12 DeFi Primitives Via Campbell Harvey - Duke University The Future of DeFi by Linda Xie The Network Effects Manual by nfx","title":"Additional Resources"},{"location":"S05a-defi/M2-stablecoins/L1/","text":"What are Stablecoins A critical issue in cryptocurrencies is extreme volatility, especially when compared to fiat currencies. It\u2019s hard to enter into any profitable financial agreement when prices can fluctuate wildly. Stablecoins were created to fix this issue. Stablecoins are cryptocurrencies built using smart contracts on programmatic blockchains, which significantly reduces or eliminates price volatility by aiming to peg to a fiat currency like the US dollar. This gives users a tool for managing risk and allowing them to engage with DeFi protocols reliably. Stablecoins are one of the cornerstones of the DeFi industry, as a user cannot reliably enter or exit a protocol without a stable currency. They also provide access to stable currencies to unbanked and underbanked people around the world , helping people to save, pay, invest and spend in more reliable ways. In addition, Stablecoins allows people to choose their economic policy based on their interests and objectives. There are four types of stablecoins, each collateralized by different assets and its own complexity/centralization profile. The types are ranked from most centralized/less complex to less centralized/more complex. Fiat backed Commodity backed Crypto backed Algorithmic Fractional Central Bank Digital Currencies* A good mental model for a Stablecoin is the paper money you have in your pocket. Think of it as a programmable IOU that everyone in your community would accept as payment. There are three things to consider when looking at stablecoins: Type of collateral Collateralization ratio Degree of centralization A general mental model could be that the simpler the stablecoin it is to understand, the more centralized it is. The exception would be Central Bank Digital Currencies which could be complex, opaque, and highly centralized due to political issues. Fiat Backed Stablecoins These are backed by US dollars or a government currency in a bank account. These stablecoins are collateralized by US Dollars in bank accounts and are audited periodically to maintain their assurances. Examples include Tether\u2019s USDT and Circle\u2019s USDC . Fiat backed stablecoins are collateralized at 100% with dollars. In the case of Tether, it is collateralized with dollar and dollar equivalents (financial instruments that can be quickly turned to cash without losing much value){target=_blank}. These are the simplest to understand since, from a technical perspective, the issuance is very simple. There is one dollar in a bank account for every token on the network. USDT and USDC are by far the most popular stablecoins for their ease of use. However, with this simplicity comes the centralization and the possibility of censorship. Both USDT and USDC can add users to blocklists and freeze their assets. Commodity backed Stablecoins Backed by gold or another asset. An example includes Paxos Gold . Every PAX Gold token is backed by an ounce of allocated gold. This type functions similar to fiat-backed stablecoins, including the ability for the issuer to place users on a blocklist. Furthermore, there is the additional complexity of the volatility of the price of gold which is higher than the fiat currencies. Crypto Collateralized Backed by crypto assets like ETH or other tokens as collateral. Due to the cryptocurrency\u2019s volatility, these assets need over 150% collateral (250% recommended){target=_blank} to deal with volatility. The prominent example is MakerDAO\u2019s DAI . Best known for introducing the Collateralized Debt Position (CDP){target=_blank} . There are many advantages to crypto collateralized stablecoins. Because the asset lives on-chain, the stablecoin can be audited by anyone at any time without permission . Furthermore, DAI is an actual money lego and can be built upon and easily integrated into any project. It can also generate interest. Along with transparency into its financial status, MakerDAO, the entity behind DAI, has a transparent governance system that allows the community members who hold MAKER tokens to vote on the management of the stablecoin. It\u2019s also a fully decentralized DAO , short for decentralized autonomous organization . Algorithmic Rebase Tokens Self-balancing through incentives and algorithms and non-collateralized. These use smart contract logic to expand and contract the supply of tokens to maintain their peg. Examples include AmpleForth\u2019s AMPL & Base Protocol's BASE . One can get a A Visual Explanation of Algorithmic Stablecoins by Haseeb Qureshi . Fractional Stablecoins Partially backed by collateral at 100% or less and partially stabilized algorithmically. The purpose is to improve capital efficiency. Examples include FRAX and Iron Finance\u2019s IRON . You can get a feeling for these via A Visual Explanation of FRAX by Haseeb Qureshi . Fractional stablecoins are a work in progress and have suffered from \u201crun on the bank\u201d , shakes the market\u2019s confidence. This causes a self-fulfilling prophecy of capital withdrawals at the same time. Like a game of musical chairs, someone will be left standing. Seigniorage Seigniorage typically involves at least two tokens. One representing the stable token. While the other attempts to stabilize the token through actively adjusting the stables supply. The volatility, either through profit or loss, is passed on to the other token(s){target=_blank}. A few examples of Seigniorage Stablecoins are Empty Set Dollar, Basis, and Terra. Out of these, Terra is the only one who has maintained its peg. A typical, and often short-lived, mechanism employed is in the use of Seigniorage Shares . Often recognizable by the Stable Token and Bond Token. Central Bank Digital Currencies - CBDCs These are government-issued fiat currencies that utilize distributed ledger technology like Ethereum. As of 2021, China has begun to pilots for its Digital Yuan and five countries in the Caribbean have already launched them . There are also currently CBDC pilots in 14 countries including Sweden, South Korea and China. You can see up to date information on CBDC\u2019s via the Atlantic Council\u2019s CBDC tracker. There are some interesting DeFi primitives that make this technology possible. The first is the ERC-20 standard , which standardizes fungible tokens. Next, the possibility of escrow contracts allows for collateral to be deposited into a protocol, giving the protocol custody over those funds. Next, the ability to alter the supply is necessary. To expand the supply the protocol needs to mint tokens. To contract the supply, it needs to burn tokens. An interesting DeFi primitive that has evolved from algorithmic stablecoins is the concept of rebasing . The peg adjusts by expanding or contracting the supply of tokens depending on the price. For example, if the price goes up or down 10%, the supply will expand or contract 10%. Rebasing makes possible an elastic supply of tokens that change based on any condition, in this case, it being the market price.","title":"What are Stablecoins"},{"location":"S05a-defi/M2-stablecoins/L1/#what-are-stablecoins","text":"A critical issue in cryptocurrencies is extreme volatility, especially when compared to fiat currencies. It\u2019s hard to enter into any profitable financial agreement when prices can fluctuate wildly. Stablecoins were created to fix this issue. Stablecoins are cryptocurrencies built using smart contracts on programmatic blockchains, which significantly reduces or eliminates price volatility by aiming to peg to a fiat currency like the US dollar. This gives users a tool for managing risk and allowing them to engage with DeFi protocols reliably. Stablecoins are one of the cornerstones of the DeFi industry, as a user cannot reliably enter or exit a protocol without a stable currency. They also provide access to stable currencies to unbanked and underbanked people around the world , helping people to save, pay, invest and spend in more reliable ways. In addition, Stablecoins allows people to choose their economic policy based on their interests and objectives. There are four types of stablecoins, each collateralized by different assets and its own complexity/centralization profile. The types are ranked from most centralized/less complex to less centralized/more complex. Fiat backed Commodity backed Crypto backed Algorithmic Fractional Central Bank Digital Currencies* A good mental model for a Stablecoin is the paper money you have in your pocket. Think of it as a programmable IOU that everyone in your community would accept as payment. There are three things to consider when looking at stablecoins: Type of collateral Collateralization ratio Degree of centralization A general mental model could be that the simpler the stablecoin it is to understand, the more centralized it is. The exception would be Central Bank Digital Currencies which could be complex, opaque, and highly centralized due to political issues.","title":"What are Stablecoins"},{"location":"S05a-defi/M2-stablecoins/L1/#fiat-backed-stablecoins","text":"These are backed by US dollars or a government currency in a bank account. These stablecoins are collateralized by US Dollars in bank accounts and are audited periodically to maintain their assurances. Examples include Tether\u2019s USDT and Circle\u2019s USDC . Fiat backed stablecoins are collateralized at 100% with dollars. In the case of Tether, it is collateralized with dollar and dollar equivalents (financial instruments that can be quickly turned to cash without losing much value){target=_blank}. These are the simplest to understand since, from a technical perspective, the issuance is very simple. There is one dollar in a bank account for every token on the network. USDT and USDC are by far the most popular stablecoins for their ease of use. However, with this simplicity comes the centralization and the possibility of censorship. Both USDT and USDC can add users to blocklists and freeze their assets.","title":"Fiat Backed Stablecoins"},{"location":"S05a-defi/M2-stablecoins/L1/#commodity-backed-stablecoins","text":"Backed by gold or another asset. An example includes Paxos Gold . Every PAX Gold token is backed by an ounce of allocated gold. This type functions similar to fiat-backed stablecoins, including the ability for the issuer to place users on a blocklist. Furthermore, there is the additional complexity of the volatility of the price of gold which is higher than the fiat currencies.","title":"Commodity backed Stablecoins"},{"location":"S05a-defi/M2-stablecoins/L1/#crypto-collateralized","text":"Backed by crypto assets like ETH or other tokens as collateral. Due to the cryptocurrency\u2019s volatility, these assets need over 150% collateral (250% recommended){target=_blank} to deal with volatility. The prominent example is MakerDAO\u2019s DAI . Best known for introducing the Collateralized Debt Position (CDP){target=_blank} . There are many advantages to crypto collateralized stablecoins. Because the asset lives on-chain, the stablecoin can be audited by anyone at any time without permission . Furthermore, DAI is an actual money lego and can be built upon and easily integrated into any project. It can also generate interest. Along with transparency into its financial status, MakerDAO, the entity behind DAI, has a transparent governance system that allows the community members who hold MAKER tokens to vote on the management of the stablecoin. It\u2019s also a fully decentralized DAO , short for decentralized autonomous organization .","title":"Crypto Collateralized"},{"location":"S05a-defi/M2-stablecoins/L1/#algorithmic","text":"","title":"Algorithmic"},{"location":"S05a-defi/M2-stablecoins/L1/#rebase-tokens","text":"Self-balancing through incentives and algorithms and non-collateralized. These use smart contract logic to expand and contract the supply of tokens to maintain their peg. Examples include AmpleForth\u2019s AMPL & Base Protocol's BASE . One can get a A Visual Explanation of Algorithmic Stablecoins by Haseeb Qureshi .","title":"Rebase Tokens"},{"location":"S05a-defi/M2-stablecoins/L1/#fractional-stablecoins","text":"Partially backed by collateral at 100% or less and partially stabilized algorithmically. The purpose is to improve capital efficiency. Examples include FRAX and Iron Finance\u2019s IRON . You can get a feeling for these via A Visual Explanation of FRAX by Haseeb Qureshi . Fractional stablecoins are a work in progress and have suffered from \u201crun on the bank\u201d , shakes the market\u2019s confidence. This causes a self-fulfilling prophecy of capital withdrawals at the same time. Like a game of musical chairs, someone will be left standing.","title":"Fractional Stablecoins"},{"location":"S05a-defi/M2-stablecoins/L1/#seigniorage","text":"Seigniorage typically involves at least two tokens. One representing the stable token. While the other attempts to stabilize the token through actively adjusting the stables supply. The volatility, either through profit or loss, is passed on to the other token(s){target=_blank}. A few examples of Seigniorage Stablecoins are Empty Set Dollar, Basis, and Terra. Out of these, Terra is the only one who has maintained its peg. A typical, and often short-lived, mechanism employed is in the use of Seigniorage Shares . Often recognizable by the Stable Token and Bond Token.","title":"Seigniorage"},{"location":"S05a-defi/M2-stablecoins/L1/#central-bank-digital-currencies-cbdcs","text":"These are government-issued fiat currencies that utilize distributed ledger technology like Ethereum. As of 2021, China has begun to pilots for its Digital Yuan and five countries in the Caribbean have already launched them . There are also currently CBDC pilots in 14 countries including Sweden, South Korea and China. You can see up to date information on CBDC\u2019s via the Atlantic Council\u2019s CBDC tracker. There are some interesting DeFi primitives that make this technology possible. The first is the ERC-20 standard , which standardizes fungible tokens. Next, the possibility of escrow contracts allows for collateral to be deposited into a protocol, giving the protocol custody over those funds. Next, the ability to alter the supply is necessary. To expand the supply the protocol needs to mint tokens. To contract the supply, it needs to burn tokens. An interesting DeFi primitive that has evolved from algorithmic stablecoins is the concept of rebasing . The peg adjusts by expanding or contracting the supply of tokens depending on the price. For example, if the price goes up or down 10%, the supply will expand or contract 10%. Rebasing makes possible an elastic supply of tokens that change based on any condition, in this case, it being the market price.","title":"Central Bank Digital Currencies - CBDCs"},{"location":"S05a-defi/M3-nfts/L1/","text":"","title":"Index"},{"location":"S05a-defi/M4-wrapped/L1/","text":"What are Wrapped Tokens? Wrapped tokens are a way to bridge cryptocurrencies between blockchains. They are used to represent tokens that are not native to a blockchain network. An example is using Bitcoin or Dogecoin on Ethereum. Wrapped tokens exist because different blockchain networks may offer different features and can\u2019t talk directly to each other. These help increase interoperability and liquidity between networks. Wrapped tokens by work by having a token deposited into an account and is digitally represented on a smart contract platform like Ethereum. The tokens are collateralized with the asset, typically at 1:1. The deposit and issuance happen through a custodian, an entity that holds the assets. The custodian can be a company like Circle/Coinbase (via USDC), a merchant, a multi-signature wallet, a DAO or a smart contract. To mint wrapped tokens, the custodian receives the asset and then issues it on the chain. Tokens are burned when the custodian receives a notice to release assets from the reserves. Users would want to have wrapped tokens on Ethereum because the assets could earn a yield within DeFi Apps. Instead of your Bitcoin or Dogecoin only gaining value through price appreciation, you can earn a yield by lending it on DeFi platforms. By putting your idle assets to work, you could earn a stream of funds. Also, by using Wrapped Bitcoin on Ethereum, you can settle transactions faster and access markets with deeper liquidity. Since DeFi platforms rely on over-collateralization to assure stability and what is being lent out is not the asset itself, there is less worry about losing your assets. Popular examples of wrapped tokens include WBTC , RenBTC, RenDoge, RenFIL, WETH. Ren Protocol specializes in wrapped tokens between chains. You can also check out BadgerDAO , which is dedicated to creating tools to onboard Bitcoin liquidity onto Ethereum to earn yield. WETH (wrapped ETH) is a special case of a wrapped token since it takes native ETH and wraps it in the ERC-20 standard. This allows ETH to be used in DeFi applications via a standard set of rules , since ETH was created before the ERC-20 standard emerged. With this, ETH can be used as collateral in the network. Prior to WETH, using ETH in a DeFi app would require swapping ETH into a token. As multiple chains emerge for different use cases, inter-blockchain liquidity will be emphasized across networks. Say moving between Polygon and Ethereum. Wrapped Tokens require some level of trust with the custodian. At some point, trustless or trust minimized solutions with no custodians will appear, allowing anyone to move assets between any chain. Additional Resources Article: What are Wrapped Tokens? Article: Understanding Wrapped Bitcoin and the Wrapped Tokens Framework Article: Wrapped Crypto Tokens, Explained Analysis: Poly Network via Rekt.news","title":"What are Wrapped Tokens?"},{"location":"S05a-defi/M4-wrapped/L1/#what-are-wrapped-tokens","text":"Wrapped tokens are a way to bridge cryptocurrencies between blockchains. They are used to represent tokens that are not native to a blockchain network. An example is using Bitcoin or Dogecoin on Ethereum. Wrapped tokens exist because different blockchain networks may offer different features and can\u2019t talk directly to each other. These help increase interoperability and liquidity between networks. Wrapped tokens by work by having a token deposited into an account and is digitally represented on a smart contract platform like Ethereum. The tokens are collateralized with the asset, typically at 1:1. The deposit and issuance happen through a custodian, an entity that holds the assets. The custodian can be a company like Circle/Coinbase (via USDC), a merchant, a multi-signature wallet, a DAO or a smart contract. To mint wrapped tokens, the custodian receives the asset and then issues it on the chain. Tokens are burned when the custodian receives a notice to release assets from the reserves. Users would want to have wrapped tokens on Ethereum because the assets could earn a yield within DeFi Apps. Instead of your Bitcoin or Dogecoin only gaining value through price appreciation, you can earn a yield by lending it on DeFi platforms. By putting your idle assets to work, you could earn a stream of funds. Also, by using Wrapped Bitcoin on Ethereum, you can settle transactions faster and access markets with deeper liquidity. Since DeFi platforms rely on over-collateralization to assure stability and what is being lent out is not the asset itself, there is less worry about losing your assets. Popular examples of wrapped tokens include WBTC , RenBTC, RenDoge, RenFIL, WETH. Ren Protocol specializes in wrapped tokens between chains. You can also check out BadgerDAO , which is dedicated to creating tools to onboard Bitcoin liquidity onto Ethereum to earn yield. WETH (wrapped ETH) is a special case of a wrapped token since it takes native ETH and wraps it in the ERC-20 standard. This allows ETH to be used in DeFi applications via a standard set of rules , since ETH was created before the ERC-20 standard emerged. With this, ETH can be used as collateral in the network. Prior to WETH, using ETH in a DeFi app would require swapping ETH into a token. As multiple chains emerge for different use cases, inter-blockchain liquidity will be emphasized across networks. Say moving between Polygon and Ethereum. Wrapped Tokens require some level of trust with the custodian. At some point, trustless or trust minimized solutions with no custodians will appear, allowing anyone to move assets between any chain.","title":"What are Wrapped Tokens?"},{"location":"S05a-defi/M4-wrapped/L1/#additional-resources","text":"Article: What are Wrapped Tokens? Article: Understanding Wrapped Bitcoin and the Wrapped Tokens Framework Article: Wrapped Crypto Tokens, Explained Analysis: Poly Network via Rekt.news","title":"Additional Resources"},{"location":"S05a-defi/M5a-dexes/L1/","text":"What are DEXes Decentralized Exchanges (DEX) are DeFi applications that allow users to trade cryptocurrencies peer-to-peer without an intermediary using Ethereum smart contracts. DEXs are non-custodial, provide greater access to tokens, are secure, require no signups and eliminate counterparty risk. However, scalability, gas fees and on/off ramps for assets remain a problem. There are three main types of DEXs: order books, automatic market makers and request for quote. This section will cover the most popular type of DEX called an Automated Market Maker and the Request For Quote Model, which has its distinct advantages. In addition, each has its distinct advantages and disadvantages.","title":"What are DEXes"},{"location":"S05a-defi/M5a-dexes/L1/#what-are-dexes","text":"Decentralized Exchanges (DEX) are DeFi applications that allow users to trade cryptocurrencies peer-to-peer without an intermediary using Ethereum smart contracts. DEXs are non-custodial, provide greater access to tokens, are secure, require no signups and eliminate counterparty risk. However, scalability, gas fees and on/off ramps for assets remain a problem. There are three main types of DEXs: order books, automatic market makers and request for quote. This section will cover the most popular type of DEX called an Automated Market Maker and the Request For Quote Model, which has its distinct advantages. In addition, each has its distinct advantages and disadvantages.","title":"What are DEXes"},{"location":"S05a-defi/M5b-amms/L1/","text":"Automated Market Makers - AMM An automated market maker (AMM){target=_blank} is a smart contract that holds assets and is always willing to quote you a price between two assets. You can trade against the AMM's capital in the smart contract instead of between peers. It uses the trades to update the size of the assets and update their price accordingly. The AMM can always guarantee liquidity by raising the price for an asset according to market demand. There are various ones , however we will focus on the most important features popularized by UniSwap. Haseeb Qureshi explains UniSwap well here. We will cover another type of DEX protocol in a later section. This section will give an overview of two key concepts that make AMMs possible. Liquidity Pools Automated Market Makers rely on liquidity pools to source capital. Liquidity pools are collections of tokens locked into a smart contract. This allows for decentralized capital formation. They are used to facilitate trading by providing liquidity, defined as the ability to convert an asset into cash or its equivalents without greatly affecting its market price. This is important because an asset's value is determined by what others are willing to pay for it and how easily it can be bought or sold. The main ideas behind liquidity pools are: They are used to source capital which is used to provide liquidity. Liquidity is the ability to convert an asset into cash without affecting its price. They are useful because order book models are not feasible on-chain completely since market makers cannot update prices all the time due to gas fees and Ethereum's throughput being too slow. Liquidity pools source capital from anyone and are used to allow anyone to trade against the smart contract. Liquidity providers earn a fee for doing so in the proportion of capital they provide to the pool. To become a liquidity provider, users must deposit equal amounts of token based on their price into a pool. If they don't, they risk being arbitraged by traders who find a good deal. -AMMs like UniSwap hold liquidity pools in token pairs. An example can be the ETH - DAI or CRV - COMP (Curve to Compound Finance){target=_blank}. The downside : Because trades happen on-chain, bots can front-run transactions and attempt other sorts of attacks . This results in a user paying more than they intended. Front running occurs when bots read Ethereum's current set of unprocessed pending transactions called the mempool and find an opportunity to outbid a transaction to be processed by the network miner (or validator after ETH 2.0){target=_blank}. The bots get ahead of the line, which results in a better price for them at the expense of the other traders. -This stems from the Miner Extractable Value , where a miner can dictate when, how and where a transaction will go into an Etheruem block. When a transaction large enough to create slippage is sent to the network, bots will notice and set off a bidding war to capture the stop and front-run. Bonding Curves The other key to Automated Market Makers is the bonding curve. A bonding curve is a mathematical formula used to describe the relationship between the price and the supply of an asset. This can be thought of as a deterministic pricing formula. This formula is called the Constant Product Formula in UniSwap. This curve can be represented in a smart contract that can buy or sell the underlying token. Bonding Curves emerge out of the ability to escrow funds in a smart contract. The main ideas behind the Constant Product Formula are: -To always ensure liquidity for any asset by modelling the demand curve in the smart contract. Users trade against the smart contract, not between each other. k always has to stay the same number, no matter what x or y does. The price quoted is directly dependent on the size of the order. The trade-off for assured liquidity is slippage, also known as getting less for more. x * y = k x = supply of asset 1 y = supply of asset 2 k = Fixed Size of Pool The downside: Note that the further one moves along the curve, the less they get. In UniSwap, the slippage starts to seriously affect orders; around 2% of the liquidity of a token is traded against. A good description of this can be found via this article and most recently here . Different AMMs have variations of this deterministic pricing formula depending on their need. Curve implements a special formula to allow stablecoins trades for assets that are a stable representation of each other, which results in a narrow trading band. An example of this is trading between ETH and sETH (synthetic ETH){target=_blank} or USDC and DAI, which should both be pegged close to the US Dollar. Synching Prices Across Markets With Automated Market Makers similar to UniSwap v2, prices are synched with outside markets via arbitrageurs who spot price differences between exchanges. These exchanges can be other DEXes or centralized exchanges. Arbitrageurs capture the profit, which is the difference in price between the two markets. A consequence of this is impermanent loss for liquidity providers, the difference between holding an asset and providing liquidity. This is similar to an opportunity cost which if realized turns into a real loss. Impermanent loss is also described in this animated video and article and here . Uniswap V3 has other features like Range Orders and Limit Orders which you can explore. Some AMMs can hold more than two assets, namely Balancer, but these are more sophisticated and rely on Bonding Surfaces . Additional Resources DeFi and the Future of Finance (section 4.7.2) DeFi and the Future of Finance (section 6.2) Graphical Guide for Understanding Uniswap via EthHub","title":"Automated Market Makers - AMM"},{"location":"S05a-defi/M5b-amms/L1/#automated-market-makers-amm","text":"An automated market maker (AMM){target=_blank} is a smart contract that holds assets and is always willing to quote you a price between two assets. You can trade against the AMM's capital in the smart contract instead of between peers. It uses the trades to update the size of the assets and update their price accordingly. The AMM can always guarantee liquidity by raising the price for an asset according to market demand. There are various ones , however we will focus on the most important features popularized by UniSwap. Haseeb Qureshi explains UniSwap well here. We will cover another type of DEX protocol in a later section. This section will give an overview of two key concepts that make AMMs possible.","title":"Automated Market Makers - AMM"},{"location":"S05a-defi/M5b-amms/L1/#liquidity-pools","text":"Automated Market Makers rely on liquidity pools to source capital. Liquidity pools are collections of tokens locked into a smart contract. This allows for decentralized capital formation. They are used to facilitate trading by providing liquidity, defined as the ability to convert an asset into cash or its equivalents without greatly affecting its market price. This is important because an asset's value is determined by what others are willing to pay for it and how easily it can be bought or sold. The main ideas behind liquidity pools are: They are used to source capital which is used to provide liquidity. Liquidity is the ability to convert an asset into cash without affecting its price. They are useful because order book models are not feasible on-chain completely since market makers cannot update prices all the time due to gas fees and Ethereum's throughput being too slow. Liquidity pools source capital from anyone and are used to allow anyone to trade against the smart contract. Liquidity providers earn a fee for doing so in the proportion of capital they provide to the pool. To become a liquidity provider, users must deposit equal amounts of token based on their price into a pool. If they don't, they risk being arbitraged by traders who find a good deal. -AMMs like UniSwap hold liquidity pools in token pairs. An example can be the ETH - DAI or CRV - COMP (Curve to Compound Finance){target=_blank}. The downside : Because trades happen on-chain, bots can front-run transactions and attempt other sorts of attacks . This results in a user paying more than they intended. Front running occurs when bots read Ethereum's current set of unprocessed pending transactions called the mempool and find an opportunity to outbid a transaction to be processed by the network miner (or validator after ETH 2.0){target=_blank}. The bots get ahead of the line, which results in a better price for them at the expense of the other traders. -This stems from the Miner Extractable Value , where a miner can dictate when, how and where a transaction will go into an Etheruem block. When a transaction large enough to create slippage is sent to the network, bots will notice and set off a bidding war to capture the stop and front-run.","title":"Liquidity Pools"},{"location":"S05a-defi/M5b-amms/L1/#bonding-curves","text":"The other key to Automated Market Makers is the bonding curve. A bonding curve is a mathematical formula used to describe the relationship between the price and the supply of an asset. This can be thought of as a deterministic pricing formula. This formula is called the Constant Product Formula in UniSwap. This curve can be represented in a smart contract that can buy or sell the underlying token. Bonding Curves emerge out of the ability to escrow funds in a smart contract. The main ideas behind the Constant Product Formula are: -To always ensure liquidity for any asset by modelling the demand curve in the smart contract. Users trade against the smart contract, not between each other. k always has to stay the same number, no matter what x or y does. The price quoted is directly dependent on the size of the order. The trade-off for assured liquidity is slippage, also known as getting less for more. x * y = k x = supply of asset 1 y = supply of asset 2 k = Fixed Size of Pool The downside: Note that the further one moves along the curve, the less they get. In UniSwap, the slippage starts to seriously affect orders; around 2% of the liquidity of a token is traded against. A good description of this can be found via this article and most recently here . Different AMMs have variations of this deterministic pricing formula depending on their need. Curve implements a special formula to allow stablecoins trades for assets that are a stable representation of each other, which results in a narrow trading band. An example of this is trading between ETH and sETH (synthetic ETH){target=_blank} or USDC and DAI, which should both be pegged close to the US Dollar.","title":"Bonding Curves"},{"location":"S05a-defi/M5b-amms/L1/#synching-prices-across-markets","text":"With Automated Market Makers similar to UniSwap v2, prices are synched with outside markets via arbitrageurs who spot price differences between exchanges. These exchanges can be other DEXes or centralized exchanges. Arbitrageurs capture the profit, which is the difference in price between the two markets. A consequence of this is impermanent loss for liquidity providers, the difference between holding an asset and providing liquidity. This is similar to an opportunity cost which if realized turns into a real loss. Impermanent loss is also described in this animated video and article and here . Uniswap V3 has other features like Range Orders and Limit Orders which you can explore. Some AMMs can hold more than two assets, namely Balancer, but these are more sophisticated and rely on Bonding Surfaces .","title":"Synching Prices Across Markets"},{"location":"S05a-defi/M5b-amms/L1/#additional-resources","text":"DeFi and the Future of Finance (section 4.7.2) DeFi and the Future of Finance (section 6.2) Graphical Guide for Understanding Uniswap via EthHub","title":"Additional Resources"},{"location":"S05a-defi/M5c-rfqs/L1/","text":"Request for Quote When to use which tool Have a large order? Use AirSwap and go deep. It\u2019s great for larger trades to protect margins and protect against a kind of front running called \"miner extracted value\" . This is because trades are direct between traders, so there is no slippage or public information to the front-run orders. Need liquidity? Use an Automated Market Maker like UniSwap and go wide. It\u2019s great for variety and availability. But, ideally, aim for smaller trades to anything under 2% of the total supply of an asset. This is due to how the \"Constant Product Formula\" used by AMMs is modelled to ensure liquidity. Key Idea : Use the appropriate tool based on the size of your order. RFQ facilities direct trade between parties which is: Fair - avoids the mempool and issues around Front running, and miner extractable value. Efficient - Eliminates slippage, so you get what you paid for. Scalable - negotiations happen off-chain (not on Ethereum){target=_blank} and scales better than relying solely on Ethereum\u2019s throughput. Summary Request for Quote handles trades with off-chain negotiations and on-chain settlement (trades){target=_blank}. Market makers are called Makers who run servers to fulfil orders. Counterparties to makers are called Takers, who wish to trade tokens. RFQ can be seen as a peer-to-peer system. The protocol\u2019s smart contracts focus on on-chain settlement of trades via \"atomic\" swaps using smart contracts. Price discovery and negotiation are made off-chain via RPC (remote procedure calls){target=_blank}, which is scalable and resistant to AMM issues related to front running and miner extractable value. This model exists because of Ethereum\u2019s constraints and the issues around large orders on AMMs mentioned previously. Supplying Liquidity Liquidity is provided by Makers who run Servers . These servers are discoverable via the Registry Protocol which allows for price discovery. Makers submit their prices to trade. Takers are counterparties who wish to start a trade. Instead of talking to each Maker, the Taker sends a request to the Indexer, who aggregates the different Makers for Takers. This negotiation is done off-chain via RPC (remote procedure calls){target=_blank}, making it resistant to flashbots searching the mempool for frontrunning opportunities, Miner Extractable Value and other shenanigans. Price Discovery and Verification Makers can consult with Oracles to consider a fair price suggestion. Takers can also consult with Oracles to ensure the price given is a good deal. Liquidity via Delegates If standing up a Server is too much, a Maker could use Delegates . Delegates enable any trader to add rules that specify the size and price for a particular token, which can be used as limit orders. The added benefit is that the trader doesn\u2019t lose custody of their tokens when doing so. Orders and Settlement Once discovery and off-chain negotiation are complete, the orders are settled on-chain. This is done via atomic swaps using a Swap contract. They are called atomic because they settle or they don\u2019t. The added benefit of this is that compared to order books, off-chain negotiation makes it more likely that orders will be filled once they are accepted. It helps to eliminate slippage and front running.","title":"Request for Quote"},{"location":"S05a-defi/M5c-rfqs/L1/#request-for-quote","text":"","title":"Request for Quote"},{"location":"S05a-defi/M5c-rfqs/L1/#when-to-use-which-tool","text":"Have a large order? Use AirSwap and go deep. It\u2019s great for larger trades to protect margins and protect against a kind of front running called \"miner extracted value\" . This is because trades are direct between traders, so there is no slippage or public information to the front-run orders. Need liquidity? Use an Automated Market Maker like UniSwap and go wide. It\u2019s great for variety and availability. But, ideally, aim for smaller trades to anything under 2% of the total supply of an asset. This is due to how the \"Constant Product Formula\" used by AMMs is modelled to ensure liquidity. Key Idea : Use the appropriate tool based on the size of your order. RFQ facilities direct trade between parties which is: Fair - avoids the mempool and issues around Front running, and miner extractable value. Efficient - Eliminates slippage, so you get what you paid for. Scalable - negotiations happen off-chain (not on Ethereum){target=_blank} and scales better than relying solely on Ethereum\u2019s throughput.","title":"When to use which tool"},{"location":"S05a-defi/M5c-rfqs/L1/#summary","text":"Request for Quote handles trades with off-chain negotiations and on-chain settlement (trades){target=_blank}. Market makers are called Makers who run servers to fulfil orders. Counterparties to makers are called Takers, who wish to trade tokens. RFQ can be seen as a peer-to-peer system. The protocol\u2019s smart contracts focus on on-chain settlement of trades via \"atomic\" swaps using smart contracts. Price discovery and negotiation are made off-chain via RPC (remote procedure calls){target=_blank}, which is scalable and resistant to AMM issues related to front running and miner extractable value. This model exists because of Ethereum\u2019s constraints and the issues around large orders on AMMs mentioned previously.","title":"Summary"},{"location":"S05a-defi/M5c-rfqs/L1/#supplying-liquidity","text":"Liquidity is provided by Makers who run Servers . These servers are discoverable via the Registry Protocol which allows for price discovery. Makers submit their prices to trade. Takers are counterparties who wish to start a trade. Instead of talking to each Maker, the Taker sends a request to the Indexer, who aggregates the different Makers for Takers. This negotiation is done off-chain via RPC (remote procedure calls){target=_blank}, making it resistant to flashbots searching the mempool for frontrunning opportunities, Miner Extractable Value and other shenanigans.","title":"Supplying Liquidity"},{"location":"S05a-defi/M5c-rfqs/L1/#price-discovery-and-verification","text":"Makers can consult with Oracles to consider a fair price suggestion. Takers can also consult with Oracles to ensure the price given is a good deal.","title":"Price Discovery and Verification"},{"location":"S05a-defi/M5c-rfqs/L1/#liquidity-via-delegates","text":"If standing up a Server is too much, a Maker could use Delegates . Delegates enable any trader to add rules that specify the size and price for a particular token, which can be used as limit orders. The added benefit is that the trader doesn\u2019t lose custody of their tokens when doing so.","title":"Liquidity via Delegates"},{"location":"S05a-defi/M5c-rfqs/L1/#orders-and-settlement","text":"Once discovery and off-chain negotiation are complete, the orders are settled on-chain. This is done via atomic swaps using a Swap contract. They are called atomic because they settle or they don\u2019t. The added benefit of this is that compared to order books, off-chain negotiation makes it more likely that orders will be filled once they are accepted. It helps to eliminate slippage and front running.","title":"Orders and Settlement"},{"location":"S05a-defi/M6-oracles/L1/","text":"Oracles in DeFi Oracles are an essential part of DeFi, since finance cannot exist in a vacuum without information. Oracles are bridges between a blockchain and the real world. They are used as queriable on-chain APIs to get information into smart contracts. The data could be anything from price information, weather reports, random numbers or more. Oracles can also be bi-directional and can \"send\" data out to the real world. They are further described here . Many top DeFi applications by total value locked use oracles in some manner. Some of the most popular use cases are collecting pricing data, event-driven decentralized execution, and random number generation. Some popular oracle sources are Chainlink and Tellor . Chainlink is described here . Protocols like Uniswap {target=_blank and Maker can also act like oracles, since they can provide data from their feeds. In fact Compound Finance uses Uniswap v2 data in conjuction with Chainlink . Oracles in DeFi are used for a wealth of different reasons: To get the value of underlying collateral (such as Aave , Synthetix , Compound ) To generate a random lottery winner (such as PoolTogether Margin Trading (such as SushiSwap Execute decentralized event-driven tasks, so that your contracts react to events ! The oracle can execute your contracts if a certain event happens. Verify a stablecoin's collateral Fetch Cryptocurrency prices And more. For many of these DeFi services to work and not be exploited, they need to be connected to the real world to operate correctly. As a result, oracles are used within the popular lending or swaps protocol infrastructure\u2014for example, Bancor uses Chainlink to mitigate impermanent loss . Oracle Attacks in DeFi When looking to get the price of assets, many DeFi users mistake using their own liquidity pools to price an asset or using a poor oracle source. An architecture where a protocol relies on itself to get the value of an underlying asset is a target for attack. Attackers can use flash loans to manipulate the price in their liquidity pools , and therefore ruin any computation using the liquidity pools for pricing. Flash loans are discussed later in the DeFi Lending lesson. Decentralized exchange data can be manipulated, so they are not good decentralized data sources and don't work as decentralized oracles. However, there are exceptions. For example, Uniswap takes the average price across a time period of around 30 minutes and uses that as the price. This is known as getting the TWAP (Time Weighted Average Price). Using a more robust decentralized solution like Chainlink will protect you from these oracle manipulation / flash loan attacks. A further look into Oracles You can create your own oracles . To understand how to use oracles in general, here is a good starter tutorial . Check out Chainlink's tutorial for a deeper dive and their video walk through . Creating a highly robust oracle requires much thought because oracle data inputs directly determine the consuming smart contracts' outputs. This leads to the Oracle Problem . Hence, oracles need to be as decentralized, secure and reliable as the blockchain networks that run them. If not, it's pointless to use a blockchain network. Single centralized nodes are subject to single points of failure if a node becomes corrupted or goes offline. Decentralization and [having multiple data providers is essential since it's difficult to know if a provider is trustworthy, introducing risk. The data source could go offline or deliver bad data. You can get a pretty comprehensive look into decentralized oracles here . Indexers As mentioned, Oracles can provide data about the real world. Yet smart contracts can create data themselves, which may be important to a protocol. With all this internal chain data, how do you organize and find it? We use Indexers, which are decentralized methods to collect, organize and query information. Think of them as what Google does for information, but for on-chain data. This allows for the easy querying of data. Ethereum is a networked decentralized database. Yet to query data, we have to look at event logs. Wouldn't it be better if we had a language we could query data with easier? We can now use TheGraph Protocol , a decentralized protocol for indexing and querying blockchain data that uses GraphQL as query language. Think of it as the \"Google of blockchains\" since indexing and sorting are what search engines do. TheGraph solves the problem of querying on-chain data as dapps grow in size . TheGraph Protocol is explained very well here . [GraphQL] allows for an easier way of querying of data compared to REST APIs . Instead of getting the whole dataset, GraphQL allows for tailoring. GraphQL enables you to send queries to get precisely the data you're looking for in one request instead of working with strict server-defined endpoints . Luckily, it builds upon knowledge of REST, and the basics picked up quickly . You can learn how to use TheGraph protocol via their developer docs and developer academy . Additional Resources 77+ Smart Contract Use Cases Enabled By Chainlink Aave Oracles The Importance of Data Quality for DeFi Smart Contracts Flash Loans and Tamper Proof Oracles TheGraph Academy - GraphQL tutorial via EggHead.io","title":"Oracles in DeFi"},{"location":"S05a-defi/M6-oracles/L1/#oracles-in-defi","text":"Oracles are an essential part of DeFi, since finance cannot exist in a vacuum without information. Oracles are bridges between a blockchain and the real world. They are used as queriable on-chain APIs to get information into smart contracts. The data could be anything from price information, weather reports, random numbers or more. Oracles can also be bi-directional and can \"send\" data out to the real world. They are further described here . Many top DeFi applications by total value locked use oracles in some manner. Some of the most popular use cases are collecting pricing data, event-driven decentralized execution, and random number generation. Some popular oracle sources are Chainlink and Tellor . Chainlink is described here . Protocols like Uniswap {target=_blank and Maker can also act like oracles, since they can provide data from their feeds. In fact Compound Finance uses Uniswap v2 data in conjuction with Chainlink . Oracles in DeFi are used for a wealth of different reasons: To get the value of underlying collateral (such as Aave , Synthetix , Compound ) To generate a random lottery winner (such as PoolTogether Margin Trading (such as SushiSwap Execute decentralized event-driven tasks, so that your contracts react to events ! The oracle can execute your contracts if a certain event happens. Verify a stablecoin's collateral Fetch Cryptocurrency prices And more. For many of these DeFi services to work and not be exploited, they need to be connected to the real world to operate correctly. As a result, oracles are used within the popular lending or swaps protocol infrastructure\u2014for example, Bancor uses Chainlink to mitigate impermanent loss .","title":"Oracles in DeFi"},{"location":"S05a-defi/M6-oracles/L1/#oracle-attacks-in-defi","text":"When looking to get the price of assets, many DeFi users mistake using their own liquidity pools to price an asset or using a poor oracle source. An architecture where a protocol relies on itself to get the value of an underlying asset is a target for attack. Attackers can use flash loans to manipulate the price in their liquidity pools , and therefore ruin any computation using the liquidity pools for pricing. Flash loans are discussed later in the DeFi Lending lesson. Decentralized exchange data can be manipulated, so they are not good decentralized data sources and don't work as decentralized oracles. However, there are exceptions. For example, Uniswap takes the average price across a time period of around 30 minutes and uses that as the price. This is known as getting the TWAP (Time Weighted Average Price). Using a more robust decentralized solution like Chainlink will protect you from these oracle manipulation / flash loan attacks.","title":"Oracle Attacks in DeFi"},{"location":"S05a-defi/M6-oracles/L1/#a-further-look-into-oracles","text":"You can create your own oracles . To understand how to use oracles in general, here is a good starter tutorial . Check out Chainlink's tutorial for a deeper dive and their video walk through . Creating a highly robust oracle requires much thought because oracle data inputs directly determine the consuming smart contracts' outputs. This leads to the Oracle Problem . Hence, oracles need to be as decentralized, secure and reliable as the blockchain networks that run them. If not, it's pointless to use a blockchain network. Single centralized nodes are subject to single points of failure if a node becomes corrupted or goes offline. Decentralization and [having multiple data providers is essential since it's difficult to know if a provider is trustworthy, introducing risk. The data source could go offline or deliver bad data. You can get a pretty comprehensive look into decentralized oracles here .","title":"A further look into Oracles"},{"location":"S05a-defi/M6-oracles/L1/#indexers","text":"As mentioned, Oracles can provide data about the real world. Yet smart contracts can create data themselves, which may be important to a protocol. With all this internal chain data, how do you organize and find it? We use Indexers, which are decentralized methods to collect, organize and query information. Think of them as what Google does for information, but for on-chain data. This allows for the easy querying of data. Ethereum is a networked decentralized database. Yet to query data, we have to look at event logs. Wouldn't it be better if we had a language we could query data with easier? We can now use TheGraph Protocol , a decentralized protocol for indexing and querying blockchain data that uses GraphQL as query language. Think of it as the \"Google of blockchains\" since indexing and sorting are what search engines do. TheGraph solves the problem of querying on-chain data as dapps grow in size . TheGraph Protocol is explained very well here . [GraphQL] allows for an easier way of querying of data compared to REST APIs . Instead of getting the whole dataset, GraphQL allows for tailoring. GraphQL enables you to send queries to get precisely the data you're looking for in one request instead of working with strict server-defined endpoints . Luckily, it builds upon knowledge of REST, and the basics picked up quickly . You can learn how to use TheGraph protocol via their developer docs and developer academy .","title":"Indexers"},{"location":"S05a-defi/M6-oracles/L1/#additional-resources","text":"77+ Smart Contract Use Cases Enabled By Chainlink Aave Oracles The Importance of Data Quality for DeFi Smart Contracts Flash Loans and Tamper Proof Oracles TheGraph Academy - GraphQL tutorial via EggHead.io","title":"Additional Resources"},{"location":"S05a-defi/M7-defi-lending/L1/","text":"DeFi Lending Markets The Use of Lending Markets Lending is a fundamental financial mechanism in finance and DeFi. Lending helps take excess capital from savers and allocates it to borrowers, putting the funds to productive use. This helps create economic growth while generating a return for savers. It's also used for hedging and mitigating risk. On the other hand, lending creates debt, which, if not properly managed, can cause instability to cascade through connected markets, leading to economic and market contractions. These issues are magnified in DeFi because networks are interdependent and open to anyone. \ud83d\uddbc Flow chart -> expansion -> Debt bubble -> pop -> come down DeFi lending markets allow users to become borrowers and lenders in a decentralized way without giving up custody of their funds. It is more efficient since it enables permissionless and programmatic access to capital without a credit check. Being on Ethereum means that any program can access these markets, providing a money lego around lending. Using a peer to contract pattern, lenders and borrowers can interact with the contract and simplify negotiating loan terms or managing counterparty risk themselves. This lowers frictions costs and while being scalable. Overcollateralization as a DeFi Primitive Since DeFi protocols are open and pseudonymous, uncollateralized loans are not possible. You have to put up assets to get funds. Providing collateral ensures the counterparty cannot steal the funds or default on a loan. Due to the volatility risk of crypto assets used as collateral, over-collateralization is required. Issuing debt via over-collateralization is a typical pattern in DeFi . The pattern can also help limit the amount of leverage , and control risk for a protocol since assets need to be greater than liabilities. Solvency = collateral asset value > liabilities (issued loans aka debt) The level of over-collateralization is represented by the loan-to-collateral ratio stated by the protocol. The loan-to-collateral ratio is the percentage you can borrow of an asset relative to the collateral deposited. Other names for the same idea include collateral factor, collateral ratio, cFactor or cRatio. Since we cannot borrow more than we deposit, the cFactor is always below 100%. Depending on the quality of the asset, the collateralization ratio is set. Borrowed value > collateral value * cFactor = All good! \ud83d\uddbc Open & anonymous -> collateral \ud83d\uddbc Volatility -> over collateralization (cFactor < 100%) Debt The ability to add collateral and adjust a token's supply allows for issuing a debt token backed by collateral. This debt token can represent a utility token like a lending market position from Compound.finance or Aave. For example, in Compound Finance, a DAI debt token would be cDAI, while in Aave aDAI. If DAI's collateral factor is 75%, then with $100 worth of collateral, you can borrow 75 DAI. Other DeFi 101 lending concepts like the importance of price oracles to update price can be found here . \ud83d\uddbc Debt collateral pattern Collateral -> contract lock -> debt token = collateral * cFactor ----Create Debt----> Reclaim <- unlock collateral <- pay back loan = collateral + interest accrued <---Unwind Debt---- Loans and Incentives Why would anyone take out an over collateralized loan ? Why not just sell the assets? One reason is to avoid or delay paying capital gains taxes when selling. If the user has excess capital, the user can earn interest on their stablecoin. Or to increase their liquidity without having to sell their assets and only having to pay the interest. This liquidity can be used as leverage to short an asset or to fund an unexpected expense. A user can profit by leveraging a long position or shorting on an asset through lending markets. Going long means expecting the asset's price to appreciate. If the user expects ETH to go up in value, they can deposit their ETH to borrow USDC. Then use USDC to buy more ETH. The user gets exposure to more ETH minus the interest rate. Say the collateral factor is 50% on a deposit of $1000 worth of ETH. The borrower receives 500 USDC and then can buy more ETH. So they can leverage themselves to $1,500 worth of ETH. To turn a profit, the appreciation of ETH should exceed the interest and gas fees required to pay back the loan. \ud83d\uddbc Add graph with a difference. Going short means expecting the asset will lose value in price. If the user expects ETH to depreciate, they can deposit USDC to borrow ETH. Then sell the ETH and repurchase it later at a lower price, making a profit in the difference. Say ETH is at $1000 when the user sells it. Later, they buy the asset back at $300. They get to pocket $700 minus the interest payment and gas fees. \ud83d\uddbc Add graph with difference. Liquidation and Incentives Liquidation involves a user's position being closed to pay the debt incurred. This happens if the collateral's value drops below the acceptable collateral ratio. Lending protocols often use an Oracle service like ChainLink and the price feed of a major exchange like Uniswap to provide real-time data about a collateral's value. Liquidation = Borrowed value > collateral value * cFactor Since smart contracts cannot act without being called, liquidation occurs by offering incentives to an external entity called a \"keeper\". The keeper can liquidate the position and keep a percentage fee. Then, the collateral is auctioned off or via a decentralized exchange at market price. \ud83d\uddbc Halting problem -> contracts need to be called -> incentive to call -> keeper to liquidate In some protocols, everything is auctioned off. In others, the remaining collateral is left in the original contract. An example can be if the collateralization ratio is 200% and the user only placed the bare minimum. If the asset drops 1%, the protocol will liquidate 2% of the collateral. Since liquidation is costly, some protocols allow users to add additional collateral if needed, similar to a margin call . It is wise to add a margin of safety in addition to the collateral ratio. Margin of Safety = cFactor + extraCapital > volatility \ud83d\uddbc graph with volatility line shorter band than collateralization ratio. Applications: Compound Finance and Aave An application of lending markets are money markets. Within DeFi open-source algorithmic money market protocols allow anyone to borrow or lend cryptocurrency assets by only providing collateral. These lending markets connect lenders who wish to earn interest and borrowers who want to borrow for various reasons and pay interest. Users can provide assets with d liquidity like ETH, WITH, DAI, USDT, USDC, LINK, etc. The two leading platforms are Compound.finance and Aave (ghost in Finnish). Explained in the links, Compound Finance and Aave work by using lending pools . A lending pool locks up capital and generates an algorithmically determined interest rate based on supply and demand. You can learn how to use Aave and Compound. Interest and Lending Lending requires borrowers and lenders. Borrowers pay interest while lenders earn interest. Whether you borrow or lend, you must lock up capital into a lending pool inside a contract. In return, you earn a debt token, in Compound's case a cToken, derived from the asset in question. DAI, you get cDAI, BAT renders cBAT and so on. This token accrues interest over time and can be traded or used as collateral in other protocols. For the lender to unlock their collateral, they must pay the interest accrued plus the collateral. For both Aave and Compound, there is no time period to close the loan. Lending can occur with variable rate interest based on market demand or fixed interest rates. Variable interest rates adjust due to the demand and supply for the asset in question. The key to note is that the borrow interest rate is always higher than the supply rate. Demand > supply = interest rate go up Demand < supply = interest rate go down Borrow APY > Supply APY Lending protocols operate in real-time. Rates are adjusted, and interest is accrued every new Ethereum block. Interest is typically accrued to the debt token or accounts tied to the lending pool. Aave lets users redirect their stream of interest to other contracts. Aave offers a stable interest rate , which means it's fixed for a short term and can change depending on borrowing and lending dynamics of the pool. Other platforms offer fixed interest rates, like Notional , 88mph and Barnbridge . Typically, the longer the loan, the higher the risk, the higher the interest rate. On Ethereum, we measure time in blocks. What if we went in reverse? If a loan and a position could be executed in the same block, what happens? We get flash loans! \ud83d\uddbc interest rate graph - grows over time. At origin 0,0, it can be almost no interest and no collateral, aka a flash loan. Flash Loans as a DeFi Primitive DeFi allows the creation of something called Flash Loans . Introduced by Aave, flash loans are the first uncollateralized loans that can be accessed in a permissionless way. This means that anyone anywhere can access large sums of capital. They are explained here . Flash Loans only have one condition: Payback the loan within 1 Ethereum block. Since all protocols share the same database on Ethereum, flash loans can interact with various protocols in one block to find opportunities. The loan only goes through if the transaction results in the loan being repaid. If not, the entire transaction reverts, like it was never sent! A cool application of Flash Loans is the ability to do Flash Swaps. Learn how to create Flash Swaps with Infura here. This can be used to hedge against liquidation risk by swapping a volatile asset into a stable asset! Risks With DeFi Lending Markets, there is zero counterparty risk. However, smart contract risk always exists. Duration risk comes in borrowers being hit with spiking variable borrowing APY rates if they are not paying attention. This can cause a user to get liquidated by having to repay more than expected. When hedging, there is the risk of the trade going the other way.","title":"DeFi Lending Markets"},{"location":"S05a-defi/M7-defi-lending/L1/#defi-lending-markets","text":"","title":"DeFi Lending Markets"},{"location":"S05a-defi/M7-defi-lending/L1/#the-use-of-lending-markets","text":"Lending is a fundamental financial mechanism in finance and DeFi. Lending helps take excess capital from savers and allocates it to borrowers, putting the funds to productive use. This helps create economic growth while generating a return for savers. It's also used for hedging and mitigating risk. On the other hand, lending creates debt, which, if not properly managed, can cause instability to cascade through connected markets, leading to economic and market contractions. These issues are magnified in DeFi because networks are interdependent and open to anyone. \ud83d\uddbc Flow chart -> expansion -> Debt bubble -> pop -> come down DeFi lending markets allow users to become borrowers and lenders in a decentralized way without giving up custody of their funds. It is more efficient since it enables permissionless and programmatic access to capital without a credit check. Being on Ethereum means that any program can access these markets, providing a money lego around lending. Using a peer to contract pattern, lenders and borrowers can interact with the contract and simplify negotiating loan terms or managing counterparty risk themselves. This lowers frictions costs and while being scalable.","title":"The Use of Lending Markets"},{"location":"S05a-defi/M7-defi-lending/L1/#overcollateralization-as-a-defi-primitive","text":"Since DeFi protocols are open and pseudonymous, uncollateralized loans are not possible. You have to put up assets to get funds. Providing collateral ensures the counterparty cannot steal the funds or default on a loan. Due to the volatility risk of crypto assets used as collateral, over-collateralization is required. Issuing debt via over-collateralization is a typical pattern in DeFi . The pattern can also help limit the amount of leverage , and control risk for a protocol since assets need to be greater than liabilities. Solvency = collateral asset value > liabilities (issued loans aka debt) The level of over-collateralization is represented by the loan-to-collateral ratio stated by the protocol. The loan-to-collateral ratio is the percentage you can borrow of an asset relative to the collateral deposited. Other names for the same idea include collateral factor, collateral ratio, cFactor or cRatio. Since we cannot borrow more than we deposit, the cFactor is always below 100%. Depending on the quality of the asset, the collateralization ratio is set. Borrowed value > collateral value * cFactor = All good! \ud83d\uddbc Open & anonymous -> collateral \ud83d\uddbc Volatility -> over collateralization (cFactor < 100%)","title":"Overcollateralization as a DeFi Primitive"},{"location":"S05a-defi/M7-defi-lending/L1/#debt","text":"The ability to add collateral and adjust a token's supply allows for issuing a debt token backed by collateral. This debt token can represent a utility token like a lending market position from Compound.finance or Aave. For example, in Compound Finance, a DAI debt token would be cDAI, while in Aave aDAI. If DAI's collateral factor is 75%, then with $100 worth of collateral, you can borrow 75 DAI. Other DeFi 101 lending concepts like the importance of price oracles to update price can be found here . \ud83d\uddbc Debt collateral pattern Collateral -> contract lock -> debt token = collateral * cFactor ----Create Debt----> Reclaim <- unlock collateral <- pay back loan = collateral + interest accrued <---Unwind Debt----","title":"Debt"},{"location":"S05a-defi/M7-defi-lending/L1/#loans-and-incentives","text":"Why would anyone take out an over collateralized loan ? Why not just sell the assets? One reason is to avoid or delay paying capital gains taxes when selling. If the user has excess capital, the user can earn interest on their stablecoin. Or to increase their liquidity without having to sell their assets and only having to pay the interest. This liquidity can be used as leverage to short an asset or to fund an unexpected expense. A user can profit by leveraging a long position or shorting on an asset through lending markets. Going long means expecting the asset's price to appreciate. If the user expects ETH to go up in value, they can deposit their ETH to borrow USDC. Then use USDC to buy more ETH. The user gets exposure to more ETH minus the interest rate. Say the collateral factor is 50% on a deposit of $1000 worth of ETH. The borrower receives 500 USDC and then can buy more ETH. So they can leverage themselves to $1,500 worth of ETH. To turn a profit, the appreciation of ETH should exceed the interest and gas fees required to pay back the loan. \ud83d\uddbc Add graph with a difference. Going short means expecting the asset will lose value in price. If the user expects ETH to depreciate, they can deposit USDC to borrow ETH. Then sell the ETH and repurchase it later at a lower price, making a profit in the difference. Say ETH is at $1000 when the user sells it. Later, they buy the asset back at $300. They get to pocket $700 minus the interest payment and gas fees. \ud83d\uddbc Add graph with difference.","title":"Loans and Incentives"},{"location":"S05a-defi/M7-defi-lending/L1/#liquidation-and-incentives","text":"Liquidation involves a user's position being closed to pay the debt incurred. This happens if the collateral's value drops below the acceptable collateral ratio. Lending protocols often use an Oracle service like ChainLink and the price feed of a major exchange like Uniswap to provide real-time data about a collateral's value. Liquidation = Borrowed value > collateral value * cFactor Since smart contracts cannot act without being called, liquidation occurs by offering incentives to an external entity called a \"keeper\". The keeper can liquidate the position and keep a percentage fee. Then, the collateral is auctioned off or via a decentralized exchange at market price. \ud83d\uddbc Halting problem -> contracts need to be called -> incentive to call -> keeper to liquidate In some protocols, everything is auctioned off. In others, the remaining collateral is left in the original contract. An example can be if the collateralization ratio is 200% and the user only placed the bare minimum. If the asset drops 1%, the protocol will liquidate 2% of the collateral. Since liquidation is costly, some protocols allow users to add additional collateral if needed, similar to a margin call . It is wise to add a margin of safety in addition to the collateral ratio. Margin of Safety = cFactor + extraCapital > volatility \ud83d\uddbc graph with volatility line shorter band than collateralization ratio. Applications: Compound Finance and Aave An application of lending markets are money markets. Within DeFi open-source algorithmic money market protocols allow anyone to borrow or lend cryptocurrency assets by only providing collateral. These lending markets connect lenders who wish to earn interest and borrowers who want to borrow for various reasons and pay interest. Users can provide assets with d liquidity like ETH, WITH, DAI, USDT, USDC, LINK, etc. The two leading platforms are Compound.finance and Aave (ghost in Finnish). Explained in the links, Compound Finance and Aave work by using lending pools . A lending pool locks up capital and generates an algorithmically determined interest rate based on supply and demand. You can learn how to use Aave and Compound.","title":"Liquidation and Incentives"},{"location":"S05a-defi/M7-defi-lending/L1/#interest-and-lending","text":"Lending requires borrowers and lenders. Borrowers pay interest while lenders earn interest. Whether you borrow or lend, you must lock up capital into a lending pool inside a contract. In return, you earn a debt token, in Compound's case a cToken, derived from the asset in question. DAI, you get cDAI, BAT renders cBAT and so on. This token accrues interest over time and can be traded or used as collateral in other protocols. For the lender to unlock their collateral, they must pay the interest accrued plus the collateral. For both Aave and Compound, there is no time period to close the loan. Lending can occur with variable rate interest based on market demand or fixed interest rates. Variable interest rates adjust due to the demand and supply for the asset in question. The key to note is that the borrow interest rate is always higher than the supply rate. Demand > supply = interest rate go up Demand < supply = interest rate go down Borrow APY > Supply APY Lending protocols operate in real-time. Rates are adjusted, and interest is accrued every new Ethereum block. Interest is typically accrued to the debt token or accounts tied to the lending pool. Aave lets users redirect their stream of interest to other contracts. Aave offers a stable interest rate , which means it's fixed for a short term and can change depending on borrowing and lending dynamics of the pool. Other platforms offer fixed interest rates, like Notional , 88mph and Barnbridge . Typically, the longer the loan, the higher the risk, the higher the interest rate. On Ethereum, we measure time in blocks. What if we went in reverse? If a loan and a position could be executed in the same block, what happens? We get flash loans! \ud83d\uddbc interest rate graph - grows over time. At origin 0,0, it can be almost no interest and no collateral, aka a flash loan.","title":"Interest and Lending"},{"location":"S05a-defi/M7-defi-lending/L1/#flash-loans-as-a-defi-primitive","text":"DeFi allows the creation of something called Flash Loans . Introduced by Aave, flash loans are the first uncollateralized loans that can be accessed in a permissionless way. This means that anyone anywhere can access large sums of capital. They are explained here . Flash Loans only have one condition: Payback the loan within 1 Ethereum block. Since all protocols share the same database on Ethereum, flash loans can interact with various protocols in one block to find opportunities. The loan only goes through if the transaction results in the loan being repaid. If not, the entire transaction reverts, like it was never sent! A cool application of Flash Loans is the ability to do Flash Swaps. Learn how to create Flash Swaps with Infura here. This can be used to hedge against liquidation risk by swapping a volatile asset into a stable asset!","title":"Flash Loans as a DeFi Primitive"},{"location":"S05a-defi/M7-defi-lending/L1/#risks","text":"With DeFi Lending Markets, there is zero counterparty risk. However, smart contract risk always exists. Duration risk comes in borrowers being hit with spiking variable borrowing APY rates if they are not paying attention. This can cause a user to get liquidated by having to repay more than expected. When hedging, there is the risk of the trade going the other way.","title":"Risks"},{"location":"S05a-defi/M8-governance/L1/","text":"What are Governance Tokens Decentralized networks have no central entity to manage them. Governance tokens were created to help the community steer the network . Governance tokens represent a percentage of voting power, called \u201cpro-rata voting rights\u201d, over a protocol or network. These tokens typically have code that embeds rules related to how the system can change, like adjusting configurations or adding new components. The ability to adjust a network is essential because DeFi protocols need to stay in sync and react to changing market conditions. It also allows the network to evolve as the ecosystem matures. Examples of governance tokens include Yearn.finance YFI, Compound Finance COMP, UniSwap UNI, AirSwap AST. Due to their influence over a network, they can accrue value and are traded on exchanges. Governance tokens rely on incentives to encourage or discourage certain behaviors which maintain network security, solvency and growth. Incentives are powerful tools . Through the use of code to enforce, promote and punish certain behaviors, blockchain-based systems from the base layer to DeFi protocols layer can create stability and promote coordination absent a central authority. Properly designed incentives can reward early supporters and the initial development team . Or they can result in pump and dumps. Designing these incentives requires good Tokenomics. Tokenomics deals with the management of token economies, including token creation, removal, and applied network incentives. These incentives are best seen at work in treasury management, which deals with the network\u2019s supply of tokens. Tokenomics is not to be confused with CryptoEconomics or the use of economic incentives to provide guarantees about applications in open and adversarial networks. Tokenomics is about coordinating participants, while CryptoEconomics focuses on securing the underlying system . A token\u2019s policy could be inflationary, deflationary or static. Inflationary policies increase the number of tokens through minting. Deflation reduces tokens through burning tokens. Static policies keep them the same. The policies can be used to encourage different actions. Inflation can be used to bootstrap a network by compensating users for activities to achieve utility, network participants and liquidity. As long as the network\u2019s utility is greater than the inflation rate, it could make sense to increase the supply. This could be measured by many indicators like network activity and the price for the token. Treasury management is deeply linked to community management since how you incentivize and manage tokens leads to community engagement. An application of this are liquidity mining programs. These provide staking rewards as a positive incentive for users to provide liquidity to a network. Users are credited with a bonus in their token account, based on their stake. They function as a marketing expense to bootstrap networks. Airdrops are another popular form of distribution, the most famous being UniSwap to retain liquidity from being sucked away by competing platforms. Pioneered by Compound.finance , users were rewarded by engaging in certain behaviors and received tokens which could be used for governance. This model was copied by others and led to the DeFi Summer of 2020. Seeking a return from these tokens led to yield farming , the activity of moving assets between protocols to gain a maximum return. There are also incentives to discourage negative behavior like slashing funds for certain actions or liquidations for undercollateralized positions. Further incentives include direct rewards where users can be paid for providing liquidity to a pool and earn a fee in proportion to the amount staked. Not all DeFi protocols or blockchain applications are decentralized. If a protocol\u2019s governance can be centralized if it\u2019s only controlled by the admins. True decentralization implies decentralized infrastructure and governance. Governance token holders exercise their influence via DAOs, short for decentralized autonomous organizations. They can be airdropped to users of a platform to transition to a DAO like Shapeshift did in July 2021 . We will cover DAOs in a later section. Additional Resources Compound Governance Contracts Walkthrough Governance and Voting {target=_blank}","title":"What are Governance Tokens"},{"location":"S05a-defi/M8-governance/L1/#what-are-governance-tokens","text":"Decentralized networks have no central entity to manage them. Governance tokens were created to help the community steer the network . Governance tokens represent a percentage of voting power, called \u201cpro-rata voting rights\u201d, over a protocol or network. These tokens typically have code that embeds rules related to how the system can change, like adjusting configurations or adding new components. The ability to adjust a network is essential because DeFi protocols need to stay in sync and react to changing market conditions. It also allows the network to evolve as the ecosystem matures. Examples of governance tokens include Yearn.finance YFI, Compound Finance COMP, UniSwap UNI, AirSwap AST. Due to their influence over a network, they can accrue value and are traded on exchanges. Governance tokens rely on incentives to encourage or discourage certain behaviors which maintain network security, solvency and growth. Incentives are powerful tools . Through the use of code to enforce, promote and punish certain behaviors, blockchain-based systems from the base layer to DeFi protocols layer can create stability and promote coordination absent a central authority. Properly designed incentives can reward early supporters and the initial development team . Or they can result in pump and dumps. Designing these incentives requires good Tokenomics. Tokenomics deals with the management of token economies, including token creation, removal, and applied network incentives. These incentives are best seen at work in treasury management, which deals with the network\u2019s supply of tokens. Tokenomics is not to be confused with CryptoEconomics or the use of economic incentives to provide guarantees about applications in open and adversarial networks. Tokenomics is about coordinating participants, while CryptoEconomics focuses on securing the underlying system . A token\u2019s policy could be inflationary, deflationary or static. Inflationary policies increase the number of tokens through minting. Deflation reduces tokens through burning tokens. Static policies keep them the same. The policies can be used to encourage different actions. Inflation can be used to bootstrap a network by compensating users for activities to achieve utility, network participants and liquidity. As long as the network\u2019s utility is greater than the inflation rate, it could make sense to increase the supply. This could be measured by many indicators like network activity and the price for the token. Treasury management is deeply linked to community management since how you incentivize and manage tokens leads to community engagement. An application of this are liquidity mining programs. These provide staking rewards as a positive incentive for users to provide liquidity to a network. Users are credited with a bonus in their token account, based on their stake. They function as a marketing expense to bootstrap networks. Airdrops are another popular form of distribution, the most famous being UniSwap to retain liquidity from being sucked away by competing platforms. Pioneered by Compound.finance , users were rewarded by engaging in certain behaviors and received tokens which could be used for governance. This model was copied by others and led to the DeFi Summer of 2020. Seeking a return from these tokens led to yield farming , the activity of moving assets between protocols to gain a maximum return. There are also incentives to discourage negative behavior like slashing funds for certain actions or liquidations for undercollateralized positions. Further incentives include direct rewards where users can be paid for providing liquidity to a pool and earn a fee in proportion to the amount staked. Not all DeFi protocols or blockchain applications are decentralized. If a protocol\u2019s governance can be centralized if it\u2019s only controlled by the admins. True decentralization implies decentralized infrastructure and governance. Governance token holders exercise their influence via DAOs, short for decentralized autonomous organizations. They can be airdropped to users of a platform to transition to a DAO like Shapeshift did in July 2021 . We will cover DAOs in a later section.","title":"What are Governance Tokens"},{"location":"S05a-defi/M8-governance/L1/#additional-resources","text":"Compound Governance Contracts Walkthrough Governance and Voting {target=_blank}","title":"Additional Resources"},{"location":"S05a-defi/M9-swaps/L1/","text":"Introduction to MetaMask Swaps The previous section covered decentralized exchanges and briefly touched upon aggregators, which finds the best price between exchanges. We can go a step further and have aggregators of aggregators. The most popular aggregator of aggregators is MetaMask Swaps. MetaMask Swaps addresses five issues 1 | Barrier to entry is high Before interacting with different DeFi protocols, users must first familiarize themselves with those platforms and understand the pros and cons of each. 2 | Rates vary between protocols Not all liquidity sources (DEX, DEX aggregators, and PMMs) are created equal. The rate that each protocol provides may depend on liquidity depth, pricing mechanism, and type of tokens. 3 | Gas usage differs between sources The route and complexity of each Swap may differ, depending on the liquidity source. Some sources will require less gas, depending on the route. 4 | Liquidity is fragmented Decentralization leads to liquidity fragmentation. For some trades, splitting a Swap between multiple sources can result in the most favorable trade. 5 | Token approvals are messy and expensive When Swapping ERC20 tokens, users must first approve individual tokens on each liquidity source. MetaMask Swaps solves these issues with three things 1 | Best rates across DeFi By requesting prices from all available DEXs, DEX aggregators, and individual market makers, we can guarantee that MetaMask users have access to the deepest liquidity, the largest selection of tokens, and the most competitive prices. 2 | Seamless and standardized UX We\u2019ve abstracted all the complexities, allowing MetaMask users to Swap > 1,000 unique tokens in three clicks. 3 | Approve once, Swap anywhere No need to approve every token on multiple DEXs and aggregators for each trade. With Swaps, users only need to approve each token once, reducing gas costs and shortening the path to executing their trade.","title":"Introduction to MetaMask Swaps"},{"location":"S05a-defi/M9-swaps/L1/#introduction-to-metamask-swaps","text":"The previous section covered decentralized exchanges and briefly touched upon aggregators, which finds the best price between exchanges. We can go a step further and have aggregators of aggregators. The most popular aggregator of aggregators is MetaMask Swaps.","title":"Introduction to MetaMask Swaps"},{"location":"S05a-defi/M9-swaps/L1/#metamask-swaps-addresses-five-issues","text":"","title":"MetaMask Swaps addresses five issues"},{"location":"S05a-defi/M9-swaps/L1/#1-barrier-to-entry-is-high","text":"Before interacting with different DeFi protocols, users must first familiarize themselves with those platforms and understand the pros and cons of each.","title":"1 | Barrier to entry is high"},{"location":"S05a-defi/M9-swaps/L1/#2-rates-vary-between-protocols","text":"Not all liquidity sources (DEX, DEX aggregators, and PMMs) are created equal. The rate that each protocol provides may depend on liquidity depth, pricing mechanism, and type of tokens.","title":"2 | Rates vary between protocols"},{"location":"S05a-defi/M9-swaps/L1/#3-gas-usage-differs-between-sources","text":"The route and complexity of each Swap may differ, depending on the liquidity source. Some sources will require less gas, depending on the route.","title":"3 | Gas usage differs between sources"},{"location":"S05a-defi/M9-swaps/L1/#4-liquidity-is-fragmented","text":"Decentralization leads to liquidity fragmentation. For some trades, splitting a Swap between multiple sources can result in the most favorable trade.","title":"4 | Liquidity is fragmented"},{"location":"S05a-defi/M9-swaps/L1/#5-token-approvals-are-messy-and-expensive","text":"When Swapping ERC20 tokens, users must first approve individual tokens on each liquidity source.","title":"5 | Token approvals are messy and expensive"},{"location":"S05a-defi/M9-swaps/L1/#metamask-swaps-solves-these-issues-with-three-things","text":"","title":"MetaMask Swaps solves these issues with three things"},{"location":"S05a-defi/M9-swaps/L1/#1-best-rates-across-defi","text":"By requesting prices from all available DEXs, DEX aggregators, and individual market makers, we can guarantee that MetaMask users have access to the deepest liquidity, the largest selection of tokens, and the most competitive prices.","title":"1 | Best rates across DeFi"},{"location":"S05a-defi/M9-swaps/L1/#2-seamless-and-standardized-ux","text":"We\u2019ve abstracted all the complexities, allowing MetaMask users to Swap > 1,000 unique tokens in three clicks.","title":"2 | Seamless and standardized UX"},{"location":"S05a-defi/M9-swaps/L1/#3-approve-once-swap-anywhere","text":"No need to approve every token on multiple DEXs and aggregators for each trade. With Swaps, users only need to approve each token once, reducing gas costs and shortening the path to executing their trade.","title":"3 | Approve once, Swap anywhere"},{"location":"S07-additional-topics/L1-ipfs/","text":"This section is currently a video on the LMS","title":"Index"},{"location":"S07-additional-topics/L2-filecoin/","text":"Filecoin Protocol Labs, who built IPFS, have developed an entire network for decentralized filesharing called Filecoin. In this section, we'll go through a Truffle Box which sets up a Filecoin environment on your computer. Using the Filecoin Truffle Box This quick start uses an already-created project to provide the base Truffle project structure and example contracts. In your workspace directory, run the following commands: mkdir filecoin-example cd filecoin-example truffle unbox filecoin Running Filecoin Ganache Once installed, you can run Filecoin Ganache with the following command: npx ganache filecoin This creates 10 accounts, each loaded with 100 FIL, and displays both their account addresses and associated private keys. Available Accounts ================== (0) t3rvcqmc5otc3sh3cngqg2ttzcu7ezpco466lbafzaoygxvnzsw7e7n2zbjwhiv5fdzhs6uxm2qckwt6lp5wga (100 FIL) (1) t3s3la37547tijmoeiep7ktogws3tep2eqrralh7rhi2mpe46q574gceyy467356onblzvwf7ejlelo2rdsg4q (100 FIL) (2) t3wk7a46e2dcqb7qxeuz2zq7wodwycdgtbgdpr37hhvelfilf5yvssg5xbsolgusqsumomtmtqhnobh4carhyq (100 FIL) ... It also starts the Lotus and IPFS daemons running over http and ws respectively: Lotus RPC listening on 127.0.0.1:7777 IPFS RPC listening on 127.0.0.1:5001 Filecoin Ganache GUI An alternative to running Filecoin Ganache via the CLI is to use Filecoin Ganche UI. As per the screenshot below, this exposes all the core Filecoin protocol elements as tabs which is particularly useful if you're just starting out. Running the Filecoin Network Explorer git clone https://github.com/trufflesuite/filecoin-network-inspector npm install git checkout ganache-changes npm run start Running Ethereum Ganache npx ganache ethereum RPC Listening on 127.0.0.1:8545 Creating Storage Deals A storage deal is an agreement between a client and a storage miner to store some data in the network for a given duration. Note that while in the case of Filecoin's mainnet, a deal must be secured with a miner before data is stored, in Filecoin Ganache a deal is reached automatically. Via the Filecoin Network Explorer The simplest way to store data, open the Filecoin Network Explorer and navigate to the \"Market\" tab. From here you can select a file by clicking \"Choose File\" followed by \"Upload to the Filecoin Network\". Truffle Preserve Truffle now has a preserve command which allows for the 'preservation' of files directly from the Truffle CLI. This is currently experimental and thus on specific branch; installation details available at here. Once installed, you'll be able to preserve your assets via the following command. Note that you'll need to include the environments object in your truffle-config.js to point at the respective node (although these are already preconfigured in the box). truffle preserve --environment development ./assets/ --filecoin For broader help with this command run truffle help preserve. Via Curl (or equivalent) Lastly, you can send the following curl request directly to the Lotus RPC. Note that the you'll need to update both the wallet address (t3s3la3754...) and CID (QmZTR5bcpQ...). curl -X POST \\ -H 'Content-Type: application/json' \\ -d '{\"jsonrpc\":\"2.0\",\"id\":0,\"method\":\"Filecoin.ClientStartDeal\",\"params\":[{\"Data\":{\"TransferType\":\"graphsync\",\"Root\":{\"/\":\"QmZTR5bcpQD7cFgTorqxZDYaew1Wqgfbd2ud9QqGPAkK2V\"},\"PieceCid\":null,\"PieceSize\":0},\"Wallet\":\"t3s3la37547tijmoeiep7ktogws3tep2eqrralh7rhi2mpe46q574gceyy467356onblzvwf7ejlelo2rdsg4q\",\"Miner\":\"t01000\",\"EpochPrice\":\"2500\",\"MinBlocksDuration\":300}]}' \\ http://localhost:7777/rpc/v0 Minting an NFT In the example below, we've already created a deal for the 3 assets (metadata, thumbnail, and the original asset respectively) that comprise our NFT. These are as follows, with their corresponding CIDs. metadata QmS4t7rFPxaaNriXvCmALr5GYRAtya5urrDaZgkfHutdCG thumbnail - QmbAAMaGWpiSgmMWYTRtGsru382j6qTVQ4FDKX2cRTRso6 asset - QmbAAMaGWpiSgmMWYTRtGsru382j6qTVQ4FDKX2cRTRso6 Assuming the local Ethereum Ganache node is running, you'll be able to open a console and mint a new NFT with the following steps. As the base URL is set to that of an IPFS gateway, we'll just need to pass in the CID to the asset metadata. truffle console truffle ( development ) > const gallery = await MyGallery . deployed () truffle ( development ) > gallery . mint ( accounts [ 0 ], \"QmS4t7rFPxaaNriXvCmALr5GYRAtya5urrDaZgkfHutdCG\" ) In the above example the owner of the NFT is set (via accounts[0]) to that of the first account generated by the mnemonic. If we want to transfer it to a new owner, we'll be able to do so with the following. Transferring Ownership truffle console truffle(development)> gallery.transferFrom(accounts[0], accounts[1], 1) Gallery UI A sample gallery interface is available here.\u200b\u200b You can use the following steps to run this locally... cd ui npm install npm run start Off-chain Assets Thus far we\u2019ve been exclusively referring to data that will be stored on-chain. While this is logical for certain types of data in your dapp, what about other facets (such an underlying NFT asset or a web-based front-end)? Ideally we can still take advantage of the benefits of decentralization without incurring the cost and performance overhead of trying to store on-chain. This is where services such as IPFS, Arweave, or Textile Buckets come in, along with complimentary storage solutions like Filecoin. Storing with Truffle Preserve Truffle has a built-in command called preserve that supports a number of the aforementioned services right out of the box. An example of it\u2019s usage is follows, with ./path being the path to the directory you want to persist. truffle preserve ./path The above example assumes you have a locally running IPFS node, but if not you can leverage Infura\u2019s gateway by adding the following to your truffle-config.js and then appending --environment production to the command. environments: { production: { ipfs: { address: 'https://ipfs.infura.io:5001' } } } The resultant content identified (CID) will then be returned for you to then persist with a Filecoin storage deal or pinning service. More detail can be found in the Truffle docs .","title":"Filecoin"},{"location":"S07-additional-topics/L2-filecoin/#filecoin","text":"Protocol Labs, who built IPFS, have developed an entire network for decentralized filesharing called Filecoin. In this section, we'll go through a Truffle Box which sets up a Filecoin environment on your computer.","title":"Filecoin"},{"location":"S07-additional-topics/L2-filecoin/#using-the-filecoin-truffle-box","text":"This quick start uses an already-created project to provide the base Truffle project structure and example contracts. In your workspace directory, run the following commands: mkdir filecoin-example cd filecoin-example truffle unbox filecoin","title":"Using the Filecoin Truffle Box"},{"location":"S07-additional-topics/L2-filecoin/#running-filecoin-ganache","text":"Once installed, you can run Filecoin Ganache with the following command: npx ganache filecoin This creates 10 accounts, each loaded with 100 FIL, and displays both their account addresses and associated private keys. Available Accounts ================== (0) t3rvcqmc5otc3sh3cngqg2ttzcu7ezpco466lbafzaoygxvnzsw7e7n2zbjwhiv5fdzhs6uxm2qckwt6lp5wga (100 FIL) (1) t3s3la37547tijmoeiep7ktogws3tep2eqrralh7rhi2mpe46q574gceyy467356onblzvwf7ejlelo2rdsg4q (100 FIL) (2) t3wk7a46e2dcqb7qxeuz2zq7wodwycdgtbgdpr37hhvelfilf5yvssg5xbsolgusqsumomtmtqhnobh4carhyq (100 FIL) ... It also starts the Lotus and IPFS daemons running over http and ws respectively: Lotus RPC listening on 127.0.0.1:7777 IPFS RPC listening on 127.0.0.1:5001","title":"Running Filecoin Ganache"},{"location":"S07-additional-topics/L2-filecoin/#filecoin-ganache-gui","text":"An alternative to running Filecoin Ganache via the CLI is to use Filecoin Ganche UI. As per the screenshot below, this exposes all the core Filecoin protocol elements as tabs which is particularly useful if you're just starting out.","title":"Filecoin Ganache GUI"},{"location":"S07-additional-topics/L2-filecoin/#running-the-filecoin-network-explorer","text":"git clone https://github.com/trufflesuite/filecoin-network-inspector npm install git checkout ganache-changes npm run start","title":"Running the Filecoin Network Explorer"},{"location":"S07-additional-topics/L2-filecoin/#running-ethereum-ganache","text":"npx ganache ethereum RPC Listening on 127.0.0.1:8545","title":"Running Ethereum Ganache"},{"location":"S07-additional-topics/L2-filecoin/#creating-storage-deals","text":"A storage deal is an agreement between a client and a storage miner to store some data in the network for a given duration. Note that while in the case of Filecoin's mainnet, a deal must be secured with a miner before data is stored, in Filecoin Ganache a deal is reached automatically. Via the Filecoin Network Explorer The simplest way to store data, open the Filecoin Network Explorer and navigate to the \"Market\" tab. From here you can select a file by clicking \"Choose File\" followed by \"Upload to the Filecoin Network\".","title":"Creating Storage Deals"},{"location":"S07-additional-topics/L2-filecoin/#truffle-preserve","text":"Truffle now has a preserve command which allows for the 'preservation' of files directly from the Truffle CLI. This is currently experimental and thus on specific branch; installation details available at here. Once installed, you'll be able to preserve your assets via the following command. Note that you'll need to include the environments object in your truffle-config.js to point at the respective node (although these are already preconfigured in the box). truffle preserve --environment development ./assets/ --filecoin For broader help with this command run truffle help preserve. Via Curl (or equivalent) Lastly, you can send the following curl request directly to the Lotus RPC. Note that the you'll need to update both the wallet address (t3s3la3754...) and CID (QmZTR5bcpQ...). curl -X POST \\ -H 'Content-Type: application/json' \\ -d '{\"jsonrpc\":\"2.0\",\"id\":0,\"method\":\"Filecoin.ClientStartDeal\",\"params\":[{\"Data\":{\"TransferType\":\"graphsync\",\"Root\":{\"/\":\"QmZTR5bcpQD7cFgTorqxZDYaew1Wqgfbd2ud9QqGPAkK2V\"},\"PieceCid\":null,\"PieceSize\":0},\"Wallet\":\"t3s3la37547tijmoeiep7ktogws3tep2eqrralh7rhi2mpe46q574gceyy467356onblzvwf7ejlelo2rdsg4q\",\"Miner\":\"t01000\",\"EpochPrice\":\"2500\",\"MinBlocksDuration\":300}]}' \\ http://localhost:7777/rpc/v0","title":"Truffle Preserve"},{"location":"S07-additional-topics/L2-filecoin/#minting-an-nft","text":"In the example below, we've already created a deal for the 3 assets (metadata, thumbnail, and the original asset respectively) that comprise our NFT. These are as follows, with their corresponding CIDs. metadata QmS4t7rFPxaaNriXvCmALr5GYRAtya5urrDaZgkfHutdCG thumbnail - QmbAAMaGWpiSgmMWYTRtGsru382j6qTVQ4FDKX2cRTRso6 asset - QmbAAMaGWpiSgmMWYTRtGsru382j6qTVQ4FDKX2cRTRso6 Assuming the local Ethereum Ganache node is running, you'll be able to open a console and mint a new NFT with the following steps. As the base URL is set to that of an IPFS gateway, we'll just need to pass in the CID to the asset metadata. truffle console truffle ( development ) > const gallery = await MyGallery . deployed () truffle ( development ) > gallery . mint ( accounts [ 0 ], \"QmS4t7rFPxaaNriXvCmALr5GYRAtya5urrDaZgkfHutdCG\" ) In the above example the owner of the NFT is set (via accounts[0]) to that of the first account generated by the mnemonic. If we want to transfer it to a new owner, we'll be able to do so with the following. Transferring Ownership truffle console truffle(development)> gallery.transferFrom(accounts[0], accounts[1], 1)","title":"Minting an NFT"},{"location":"S07-additional-topics/L2-filecoin/#gallery-ui","text":"A sample gallery interface is available here.\u200b\u200b You can use the following steps to run this locally... cd ui npm install npm run start","title":"Gallery UI"},{"location":"S07-additional-topics/L2-filecoin/#off-chain-assets","text":"Thus far we\u2019ve been exclusively referring to data that will be stored on-chain. While this is logical for certain types of data in your dapp, what about other facets (such an underlying NFT asset or a web-based front-end)? Ideally we can still take advantage of the benefits of decentralization without incurring the cost and performance overhead of trying to store on-chain. This is where services such as IPFS, Arweave, or Textile Buckets come in, along with complimentary storage solutions like Filecoin.","title":"Off-chain Assets"},{"location":"S07-additional-topics/L2-filecoin/#storing-with-truffle-preserve","text":"Truffle has a built-in command called preserve that supports a number of the aforementioned services right out of the box. An example of it\u2019s usage is follows, with ./path being the path to the directory you want to persist. truffle preserve ./path The above example assumes you have a locally running IPFS node, but if not you can leverage Infura\u2019s gateway by adding the following to your truffle-config.js and then appending --environment production to the command. environments: { production: { ipfs: { address: 'https://ipfs.infura.io:5001' } } } The resultant content identified (CID) will then be returned for you to then persist with a Filecoin storage deal or pinning service. More detail can be found in the Truffle docs .","title":"Storing with Truffle Preserve"},{"location":"S07-additional-topics/L3-the-graph/","text":"The Graph The Graph is a decentralized protocol for indexing and querying data on public blockchain infrastructure. The Graph organizes and makes it easy to query on-chain data in a way that was not possible in the past, with GraphQL. Querying blockchain data is difficult. Node clients can be inconsistent, application developers are required to write proprietary code and build centralized servers for ingesting the data, and they need to manage their own APIs for uptime and optimal UX. The Graph aims to solve this problem by abstracting the back-end, pre-aggregate data and making it significantly more efficient for developers to retrieve on-chain data. See video below: GRT is the native Graph token that is used to coordinate Indexers, Curators and Delegators around providing on-chain data to applications. Node operators, called Indexers, stake and earn GRT for indexing subgraphs and processing queries for dApps, such as querying for aggregate trade volume on DEXs. Delegators contribute to the security of the network by delegating their GRT stake to Indexers and also earning a portion of query fees. Delegation enables Indexers to grow in proportional size on the network and increase their indexing operation to serve more applications. Anyone can delegate GRT to Indexers to secure the network and earn rewards. Curators organize data on The Graph by signaling GRT on useful APIs, called subgraphs. Indexers look out for subgraph signal to understand which subgraphs should be prioritized based on the collective sentiment and aggregate signal. Signaling on a subgraph is a mechanism similar to leaving Yelp reviews, whereby all participants are curating the quality of the ecosystem. Indexers, Delegators, and Curators work together to organize the data for the crypto economy and maintain a useful global API for DeFi and Web3. What is a Subgraph? In the traditional web stack, databases, servers, and APIs query, filter, sort, paginate, group, and join data before it is returned to some client application usually via some type of http request. These types of data transformations are not possible when reading data directly from Ethereum or other blockchains. In the past, developers were getting around this by building their own centralized indexing servers - pulling data from blockchains, storing it in a database, and exposing it over an API. This required significant engineering and hardware resources that broke the important security properties required for decentralization. Through The Graph\u2019s protocol, developers can deploy subgraphs (open source APIs) to query on-chain data, for dapps like DeFi aggregators, wallets and NFT marketplaces. Alternatively, developers would need to build their own proprietary servers and act as a potential central point of failure for their dapp. See the video below on why subgraphs are important to the dApp ecosystem: Why do we care about Subgraphs (dApp perspective) Subgraphs enable blockchain data to be accessible via performant and open APIs with rich querying capabilities like enabling filtering, relationships, and full text search. They are a way to define which data you want to get indexed and how to store it. Subgraphs can be built to do pre-aggregations or calculations on their mappings, some are just used to organize on-chain event data. Then, an entity queries that data. This entity could be a dApp (company/developer, etc), or could be a telegram bot, or a discord bot or even a simple user doing a query to find information. The value of using a subgraph when building a dApp is that your application data will always be indexed by a network of indexers, so that users can easily query your smart contract. This means that your API cannot be taken down and ensuring a decentralized network of Indexers and Curators will always serve your dapp\u2019s queries. This also limits the risk of central points of failure. Imagine that no one knows what Wikipedia is because Wikipedia has not built a code to interact with Google\u2019s index, therefore anyone querying searching information will not turn up any results from Wikipedia. It is vital and in the best interest of the dApp team to develop a subgraph so that information is easily accessible to other developers in the Ethereum community. For example, the Uniswap subgraph is used by Uniswap.info as well as many other projects that are interested in querying Uniswap data such as trade volumes, prices or assets. Subgraphs are written in TypeScript and GraphQL SDL and queried using GraphQL. To learn more about GraphQL and how it will power the decentralized web as a data interoperability layer, read this blog post written by The Graph Co-founder and former Research Lead, Brandon Ramirez. Feel free to also check out The Graph's network documentation for more details . Video explainer below: How to build a Subgraph For a step by step guide on how to build a Subgraph, check out this guide written by DevRel lead, Nader Dabit at Edge & Node. For a video guide, watch below! For a more general guide on how to get started with web3, check out this guide . Querying from a Frontend Application Follow the guide in Gitpod and video below: API Key Management Regardless of whether you\u2019re a dApp developer or a subgraph developer, you\u2019ll need to manage your API keys. This is important for you to be able to query subgraphs because API keys make sure the connections between application services are valid and authorized. This includes authenticating the end user and the device using the application. The Subgraph Studio will list out existing API keys, which will give you the ability to manage or delete them. For more details, follow along in the video below: The Graph Explorer The Graph Explorer is a developer's decentralized portal into the world of subgraphs and network data. The Graph Explorer consists of multiple parts where developers can interact with other subgraph developers, dApp developers, Curators, Indexers, and Delegators. For a general overview of the Graph Explorer, check out the video below or read The Graph's documentation here .","title":"The Graph"},{"location":"S07-additional-topics/L3-the-graph/#the-graph","text":"The Graph is a decentralized protocol for indexing and querying data on public blockchain infrastructure. The Graph organizes and makes it easy to query on-chain data in a way that was not possible in the past, with GraphQL. Querying blockchain data is difficult. Node clients can be inconsistent, application developers are required to write proprietary code and build centralized servers for ingesting the data, and they need to manage their own APIs for uptime and optimal UX. The Graph aims to solve this problem by abstracting the back-end, pre-aggregate data and making it significantly more efficient for developers to retrieve on-chain data. See video below: GRT is the native Graph token that is used to coordinate Indexers, Curators and Delegators around providing on-chain data to applications. Node operators, called Indexers, stake and earn GRT for indexing subgraphs and processing queries for dApps, such as querying for aggregate trade volume on DEXs. Delegators contribute to the security of the network by delegating their GRT stake to Indexers and also earning a portion of query fees. Delegation enables Indexers to grow in proportional size on the network and increase their indexing operation to serve more applications. Anyone can delegate GRT to Indexers to secure the network and earn rewards. Curators organize data on The Graph by signaling GRT on useful APIs, called subgraphs. Indexers look out for subgraph signal to understand which subgraphs should be prioritized based on the collective sentiment and aggregate signal. Signaling on a subgraph is a mechanism similar to leaving Yelp reviews, whereby all participants are curating the quality of the ecosystem. Indexers, Delegators, and Curators work together to organize the data for the crypto economy and maintain a useful global API for DeFi and Web3.","title":"The Graph"},{"location":"S07-additional-topics/L3-the-graph/#what-is-a-subgraph","text":"In the traditional web stack, databases, servers, and APIs query, filter, sort, paginate, group, and join data before it is returned to some client application usually via some type of http request. These types of data transformations are not possible when reading data directly from Ethereum or other blockchains. In the past, developers were getting around this by building their own centralized indexing servers - pulling data from blockchains, storing it in a database, and exposing it over an API. This required significant engineering and hardware resources that broke the important security properties required for decentralization. Through The Graph\u2019s protocol, developers can deploy subgraphs (open source APIs) to query on-chain data, for dapps like DeFi aggregators, wallets and NFT marketplaces. Alternatively, developers would need to build their own proprietary servers and act as a potential central point of failure for their dapp. See the video below on why subgraphs are important to the dApp ecosystem:","title":"What is a Subgraph?"},{"location":"S07-additional-topics/L3-the-graph/#why-do-we-care-about-subgraphs-dapp-perspective","text":"Subgraphs enable blockchain data to be accessible via performant and open APIs with rich querying capabilities like enabling filtering, relationships, and full text search. They are a way to define which data you want to get indexed and how to store it. Subgraphs can be built to do pre-aggregations or calculations on their mappings, some are just used to organize on-chain event data. Then, an entity queries that data. This entity could be a dApp (company/developer, etc), or could be a telegram bot, or a discord bot or even a simple user doing a query to find information. The value of using a subgraph when building a dApp is that your application data will always be indexed by a network of indexers, so that users can easily query your smart contract. This means that your API cannot be taken down and ensuring a decentralized network of Indexers and Curators will always serve your dapp\u2019s queries. This also limits the risk of central points of failure. Imagine that no one knows what Wikipedia is because Wikipedia has not built a code to interact with Google\u2019s index, therefore anyone querying searching information will not turn up any results from Wikipedia. It is vital and in the best interest of the dApp team to develop a subgraph so that information is easily accessible to other developers in the Ethereum community. For example, the Uniswap subgraph is used by Uniswap.info as well as many other projects that are interested in querying Uniswap data such as trade volumes, prices or assets. Subgraphs are written in TypeScript and GraphQL SDL and queried using GraphQL. To learn more about GraphQL and how it will power the decentralized web as a data interoperability layer, read this blog post written by The Graph Co-founder and former Research Lead, Brandon Ramirez. Feel free to also check out The Graph's network documentation for more details . Video explainer below:","title":"Why do we care about Subgraphs (dApp perspective)"},{"location":"S07-additional-topics/L3-the-graph/#how-to-build-a-subgraph","text":"For a step by step guide on how to build a Subgraph, check out this guide written by DevRel lead, Nader Dabit at Edge & Node. For a video guide, watch below! For a more general guide on how to get started with web3, check out this guide .","title":"How to build a Subgraph"},{"location":"S07-additional-topics/L3-the-graph/#querying-from-a-frontend-application","text":"Follow the guide in Gitpod and video below:","title":"Querying from a Frontend Application"},{"location":"S07-additional-topics/L3-the-graph/#api-key-management","text":"Regardless of whether you\u2019re a dApp developer or a subgraph developer, you\u2019ll need to manage your API keys. This is important for you to be able to query subgraphs because API keys make sure the connections between application services are valid and authorized. This includes authenticating the end user and the device using the application. The Subgraph Studio will list out existing API keys, which will give you the ability to manage or delete them. For more details, follow along in the video below:","title":"API Key Management"},{"location":"S07-additional-topics/L3-the-graph/#the-graph-explorer","text":"The Graph Explorer is a developer's decentralized portal into the world of subgraphs and network data. The Graph Explorer consists of multiple parts where developers can interact with other subgraph developers, dApp developers, Curators, Indexers, and Delegators. For a general overview of the Graph Explorer, check out the video below or read The Graph's documentation here .","title":"The Graph Explorer"},{"location":"S07-additional-topics/L4-zkp/","text":"Zero-Knowledge Proofs In our section on scalability later in the course, we will discuss Zero-Knowledge Proof rollups, but we won't go into detail about what ZKPs are. In this section, we'll discuss the basics of ZKPs and compare two common implementations of ZKPs you'll see in blockchain. Zero-Knowledge Proofs have been researched since the 1980s\u2014the initial researchers of ZKPs received the Turing Award, the highest award in computer science, for their work. Their development stems from the field of computational complexity and, while most of the work of ZKPs is very technical, we'll try to provide a broad overview. The goal of zero-knowledge proofs (ZKPs) is for a verifier to be able to convince themselves that a prover possesses knowledge of a secret parameter, called a witness , satisfying some relation, without revealing the witness to the verifier or anyone else. Zero-knowledge proofs are currently being researched as solutions to two main issues in public blockchains: Privacy: All essential information within a blockchain network must be public. ZKPs allow users to verify / prove certain elements of information (such as the answer to a crossword puzzle) while not revealing the information itself (to not give the answer to someone else). Certain tools, such as AZTEC, promise to use ZKPs to provide channels which shield the entire nature of certain transactions from the public blockchain while still using the public blockchain's security promises.2. Scalability: As blockchain networks grow, the amount of data they require to be maintained grows as well. ZKPs compress large amounts of data into compact, verifiable proofs, a characteristic scalability researchers hope to use to dramatically reduce the data required to maintain security on a public blockchain. From ZCash: \u201cSuccinct\u201d zero-knowledge proofs (SNARKs, STARKs) can be verified within a few milliseconds, with a proof length of only a few hundred bytes even for statements about programs that are very large. A common way Zero-Knowledge Proofs are used in blockchain is as follows. A Verifier will take the piece of data to be verified and put it through a private process (ZK-SNARK or STARK, for example) which produces a program called a Proof Circuit. This Proof Circuit can then be posted openly (such as in a Smart Contract) as it reveals no meaningful information about the nature of the data it represents. A Prover can then use the Proof Circuit to irrefutably prove they know the piece of data by publicly \"solving\" the Proof Circuit, a process that also reveals nothing about the nature of the data. The Ethereum Foundation described the impact of this privacy would have on a blockchain: Imagine, for example, an election or auction conducted on the blockchain via a smart contract such that the results can be verified by any observer of the blockchain, but the individual votes or bids are not revealed. Another possible scenario may involve selective disclosure where users would have the ability to prove they are in a certain city without disclosing their exact location. ( source ) ZKPs in Action zk-SNARKs S uccinct N on-Interactive AR gument of K nowledge, or SNARKs, were first widely implemented on a blockchain by ZCash, a fork of Bitcoin. Ethereum introduced SNARK capacity with the Byzantium fork by including precompiled contracts that allowed efficient SNARK verification on-chain. SNARKs are being discussed in terms of privacy (as in ZCash) and scalability (as in SNARK roll-ups ) Pros: Small, constant proof size Constant-time verification costs ( source ) Cons: Trusted setup required (see ZCash parameter generation ceremony ) Relies on Elliptic Curve Cryptography, which is not quantum-resistant zk-STARKs S calable and T ransparent AR gument of K nowledge, or STARKs, share a similar fundamental ideas with SNARKs but differ in execution. Their development and advocacy is done chiefly by STARKware, a private company led by Eli Ben Sasson, with assistance from the Ethereum Foundation. Pros: No trusted setup required Faster proving time relative to SNARKs ( source ) Cons: STARKs have a larger proof size than SNARKs As of June 2019, STARKs still in development ZKP Tutorials \"Hello World\" of zk-SNARKS with Zokrates Creating a zk-SNARKS version of \"Mastermind\" in the Browser \u2014 Koh Wei Jie Github Repo for Creating a zk-SNARKS version of \"Mastermind\" in the Browser Developing Confidential Tokens on AZTEC(requires access to Rinkeby Testnet) Docs: Cairo \u2014 a Turing-complete STARK generation Cairo, Starkware's ZKP language, has a great doc repository here, including a great introductory tutorial. Reddit Scaling Bake-Off ZKP Submissions: STARKWare , Fuel Labs , AZTEC More Resources Article: NYTimes 1987 article discussing the discovery of ZK Proofs Article: Why and How zk-SNARK Works Article: Introduction to zk-SNARKS with Examples Wiki: Zero Knowledge Starter Pack (S[NT]ARK focused) Wiki: Zero Knowledge Proof Science Resources Article: Not-so-gentle Introduction to the PCP Theorem (PCP Theorem underpins many ZKPs) Series: The Math behind STARKs (abstract but still technical) Article: An Approximate Introduction to how zk-SNARKs are possible Video: Vitalik Buterin: Scalable Blockchains as Data Layers","title":"Zero-Knowledge Proofs"},{"location":"S07-additional-topics/L4-zkp/#zero-knowledge-proofs","text":"In our section on scalability later in the course, we will discuss Zero-Knowledge Proof rollups, but we won't go into detail about what ZKPs are. In this section, we'll discuss the basics of ZKPs and compare two common implementations of ZKPs you'll see in blockchain. Zero-Knowledge Proofs have been researched since the 1980s\u2014the initial researchers of ZKPs received the Turing Award, the highest award in computer science, for their work. Their development stems from the field of computational complexity and, while most of the work of ZKPs is very technical, we'll try to provide a broad overview. The goal of zero-knowledge proofs (ZKPs) is for a verifier to be able to convince themselves that a prover possesses knowledge of a secret parameter, called a witness , satisfying some relation, without revealing the witness to the verifier or anyone else. Zero-knowledge proofs are currently being researched as solutions to two main issues in public blockchains: Privacy: All essential information within a blockchain network must be public. ZKPs allow users to verify / prove certain elements of information (such as the answer to a crossword puzzle) while not revealing the information itself (to not give the answer to someone else). Certain tools, such as AZTEC, promise to use ZKPs to provide channels which shield the entire nature of certain transactions from the public blockchain while still using the public blockchain's security promises.2. Scalability: As blockchain networks grow, the amount of data they require to be maintained grows as well. ZKPs compress large amounts of data into compact, verifiable proofs, a characteristic scalability researchers hope to use to dramatically reduce the data required to maintain security on a public blockchain. From ZCash: \u201cSuccinct\u201d zero-knowledge proofs (SNARKs, STARKs) can be verified within a few milliseconds, with a proof length of only a few hundred bytes even for statements about programs that are very large. A common way Zero-Knowledge Proofs are used in blockchain is as follows. A Verifier will take the piece of data to be verified and put it through a private process (ZK-SNARK or STARK, for example) which produces a program called a Proof Circuit. This Proof Circuit can then be posted openly (such as in a Smart Contract) as it reveals no meaningful information about the nature of the data it represents. A Prover can then use the Proof Circuit to irrefutably prove they know the piece of data by publicly \"solving\" the Proof Circuit, a process that also reveals nothing about the nature of the data. The Ethereum Foundation described the impact of this privacy would have on a blockchain: Imagine, for example, an election or auction conducted on the blockchain via a smart contract such that the results can be verified by any observer of the blockchain, but the individual votes or bids are not revealed. Another possible scenario may involve selective disclosure where users would have the ability to prove they are in a certain city without disclosing their exact location. ( source )","title":"Zero-Knowledge Proofs"},{"location":"S07-additional-topics/L4-zkp/#zkps-in-action","text":"","title":"ZKPs in Action"},{"location":"S07-additional-topics/L4-zkp/#zk-snarks","text":"S uccinct N on-Interactive AR gument of K nowledge, or SNARKs, were first widely implemented on a blockchain by ZCash, a fork of Bitcoin. Ethereum introduced SNARK capacity with the Byzantium fork by including precompiled contracts that allowed efficient SNARK verification on-chain. SNARKs are being discussed in terms of privacy (as in ZCash) and scalability (as in SNARK roll-ups ) Pros: Small, constant proof size Constant-time verification costs ( source ) Cons: Trusted setup required (see ZCash parameter generation ceremony ) Relies on Elliptic Curve Cryptography, which is not quantum-resistant","title":"zk-SNARKs"},{"location":"S07-additional-topics/L4-zkp/#zk-starks","text":"S calable and T ransparent AR gument of K nowledge, or STARKs, share a similar fundamental ideas with SNARKs but differ in execution. Their development and advocacy is done chiefly by STARKware, a private company led by Eli Ben Sasson, with assistance from the Ethereum Foundation. Pros: No trusted setup required Faster proving time relative to SNARKs ( source ) Cons: STARKs have a larger proof size than SNARKs As of June 2019, STARKs still in development","title":"zk-STARKs"},{"location":"S07-additional-topics/L4-zkp/#zkp-tutorials","text":"\"Hello World\" of zk-SNARKS with Zokrates Creating a zk-SNARKS version of \"Mastermind\" in the Browser \u2014 Koh Wei Jie Github Repo for Creating a zk-SNARKS version of \"Mastermind\" in the Browser Developing Confidential Tokens on AZTEC(requires access to Rinkeby Testnet) Docs: Cairo \u2014 a Turing-complete STARK generation Cairo, Starkware's ZKP language, has a great doc repository here, including a great introductory tutorial. Reddit Scaling Bake-Off ZKP Submissions: STARKWare , Fuel Labs , AZTEC","title":"ZKP Tutorials"},{"location":"S07-additional-topics/L4-zkp/#more-resources","text":"Article: NYTimes 1987 article discussing the discovery of ZK Proofs Article: Why and How zk-SNARK Works Article: Introduction to zk-SNARKS with Examples Wiki: Zero Knowledge Starter Pack (S[NT]ARK focused) Wiki: Zero Knowledge Proof Science Resources Article: Not-so-gentle Introduction to the PCP Theorem (PCP Theorem underpins many ZKPs) Series: The Math behind STARKs (abstract but still technical) Article: An Approximate Introduction to how zk-SNARKs are possible Video: Vitalik Buterin: Scalable Blockchains as Data Layers","title":"More Resources"},{"location":"S08-scalability/M1-intro/L1-overview/","text":"Intro to Scalability Ethereum gas fees have risen astronomically as the network becomes more popular. The ability of retail investors to participate in new Ethereum-based offerings like DeFi is circumscribed by the ever-increasing cost of entry into the space. Three main challenges presented by the success of mainnet Ethereum: 1. Spiking gas costs price-out market share and impact user experience. Best case is loss of low-value transactions and poor UX due to stuck transactions. Worst case is compromising healthy market operation like Black Thursday. 2. Block Latency is problematic in some applications 3. Pushing gas costs to users creates friction and impacts user experience. User has to navigate rather complex effects. As a result, some users and products are migrating to other chains that offer lower costs. Off-chain scaling is capable of solving these problems by providing higher throughput, faster state advancement and gas cost abstraction. It can be transformative for application UX State advancement (propagating new network state) is 10-100x faster (the rate at which data can be updated on-chain). Reducing user feedback loop from 12s to <1s. High frequency transactions possible (e.g. high-fidelity price oracles, rapid orderbook management) Transactions are a fraction of the cost. Low or no-value transactions viable Support for native gas cost abstraction, meta-transactions, account abstraction. Subsidised gas, or gas cost is paid in relevant tokens. Better native support for smart contract wallets and features like social recovery. If Ethereum is to maintain its dominance and network effect, it must scale. Terminology In the following sections, we're first going to go through some of the types of Layer 2 solutions. Then walk through examples of popular Layer 2 solutions. We're then going to talk about crosschain and blockchain interoperability. Finally, we're going to have live sessions and presentations based around L2 platforms such as Polygon, Optimism and more. In this section, we'll typically refer to mainnet Ethereum as Layer 1 or L1 and whatever the scaling solution is as a Layer 2 solution (L2), a sidechain, sidechannel or a bridge. Conclusion These are exciting and very new topics, please be aware that they are considered very cutting edge and you should be extremely careful when using them. Also, please know that the documentation changes very regularly and perhaps not perceptibly. If you're currently building on a Layer 2 solution, we recommend joining the project's Discord, or wherever the community gathers, and making sure to follow their developer updates. Let's dive in!","title":"Intro to Scalability"},{"location":"S08-scalability/M1-intro/L1-overview/#intro-to-scalability","text":"Ethereum gas fees have risen astronomically as the network becomes more popular. The ability of retail investors to participate in new Ethereum-based offerings like DeFi is circumscribed by the ever-increasing cost of entry into the space. Three main challenges presented by the success of mainnet Ethereum: 1. Spiking gas costs price-out market share and impact user experience. Best case is loss of low-value transactions and poor UX due to stuck transactions. Worst case is compromising healthy market operation like Black Thursday. 2. Block Latency is problematic in some applications 3. Pushing gas costs to users creates friction and impacts user experience. User has to navigate rather complex effects. As a result, some users and products are migrating to other chains that offer lower costs. Off-chain scaling is capable of solving these problems by providing higher throughput, faster state advancement and gas cost abstraction. It can be transformative for application UX State advancement (propagating new network state) is 10-100x faster (the rate at which data can be updated on-chain). Reducing user feedback loop from 12s to <1s. High frequency transactions possible (e.g. high-fidelity price oracles, rapid orderbook management) Transactions are a fraction of the cost. Low or no-value transactions viable Support for native gas cost abstraction, meta-transactions, account abstraction. Subsidised gas, or gas cost is paid in relevant tokens. Better native support for smart contract wallets and features like social recovery. If Ethereum is to maintain its dominance and network effect, it must scale.","title":"Intro to Scalability"},{"location":"S08-scalability/M1-intro/L1-overview/#terminology","text":"In the following sections, we're first going to go through some of the types of Layer 2 solutions. Then walk through examples of popular Layer 2 solutions. We're then going to talk about crosschain and blockchain interoperability. Finally, we're going to have live sessions and presentations based around L2 platforms such as Polygon, Optimism and more. In this section, we'll typically refer to mainnet Ethereum as Layer 1 or L1 and whatever the scaling solution is as a Layer 2 solution (L2), a sidechain, sidechannel or a bridge.","title":"Terminology"},{"location":"S08-scalability/M1-intro/L1-overview/#conclusion","text":"These are exciting and very new topics, please be aware that they are considered very cutting edge and you should be extremely careful when using them. Also, please know that the documentation changes very regularly and perhaps not perceptibly. If you're currently building on a Layer 2 solution, we recommend joining the project's Discord, or wherever the community gathers, and making sure to follow their developer updates. Let's dive in!","title":"Conclusion"},{"location":"S08-scalability/M2-types/L1/","text":"Types of Scaling Solutions Several options have been proposed and worked on for scaling Ethereum on a shorter time frame. Some of these efforts are coming to fruition now and are worth considering. The main tradeoffs for choosing a scaling solution involve considerations of throughput vs. security vs. usability . The following are some solutions currently in the works at various stages: Note: This section draws heavily on the work on Faina Shalts, engineer at Truffle (and Bootcamp alumni!) as well as from Ethereum.org Rollups In general, on Rollup Layer 2 solutions, transactions are submitted to L2 nodes instead of L1, and batched. Eventually they are put on L1 and no longer mutable. L2 nodes are Ethereum compatible, independent blockchains. All state and execution is handled in L2: Signature verification, Contract execution, etc. The L1 only stores transaction data Note: the terminology here can be challenging but Pranay Mohan of Celo Network proposes we think of rollups as shard clients and the rollup contracts as on-chain light clients. There are two major kinds of Rollups: ZK-Rollups and Optimistic Rollups. Zero-Knowledge / ZK-Rollups As we mentioned earlier in the section on Zero-Knowledge proofs, ZKPs can compress a larger amount of computation or verificatio into a single operation. ZK-Rollups bundle hundreds of transfers that occur on the ZKP Rollup L2 into a single L1, mainnet transaction via a smart contract located on L1. From the data submitted the smart contract can verify all the transfers that are included. Critically, you don\u2019t need all the data to verify the transactions, just the zero-knowledge proof. Transactions are written to Ethereum as calldata, to reduce gas. Pros No delay, less vulnerable to economic attacks Cons Limited to simple transfers and ZK-Rollup chains not compatible with EVM as validity proofs are intense to compute and have to build their own language to process. However, there is some work on building Solidity to ZKP language compilers, like this one for Cairo, Starknet's ZKP language. ZK-Rollups are not worth it for applications with little on-chain activity but are attractive for simple, high-volume exchanges. Currently using this sort of rollup: Loopring , Starkware, Matter Labs' zkSync, Aztec's ZK.Money network Optimistic Rollups Optimistic Rollups use a sidechain that sits in parallel to the mainnet Ethereum chain. They don\u2019t do any computation by default: after a transaction, the Optimistic Rollup L2s proposes a new state to the L1 mainnet, or \u201cnotarizes\u201d the transaction. L2 Transactions written to L1 mainnet as calldata . The main mechanism that makes this work are fraud proofs: If someone notices a fraudulent transaction the Optimistic Rollup network will execute a fraud-proof and run the transaction\u2019s computation using the available state data; the gas you need to run a fraud proof is reimbursed. Pros Anything you can do on L1 you can do with Optimistic Rollups because it is EVM and Solidity compatible. All transaction data is stored on the L1 chain, meaning it remains secure and decentralized. Cons Long wait times for on-chain transactions due to potential fraud challenges. Potentially vulnerable to attacks if the value in an optimistic rollup exceeds the amount in an operator\u2019s bond. Optimistic Roll-ups are currently being built by Optimistic PBC, Arbitrum, Fuel Network, ImmutableX, Deversifi and Cartesi Channels Channels, also called Side Channels or State Channels, allow participants to transact a certain number of times off-chain (on the channel) while only submitting 2 transactions to the network on chain (basically, the start and stop of the channel). Fundamentally for a channel to exist, participants must lock a portion of Ethereum\u2019s state, like an ETH deposit, in a multisig contract. Locking state in this way opens up the channel, allowing for the off-chain transactions to occur. When the interaction is finished, one final L1 transaction is submitted, updating the network state based on the activity that occurred on the channel (mainly the rebalancing of funds between the participants). Sidenote: State channels on Ethereum can be enforced through a concept known as counterfactual instantiation. Here's a technical, but concise, overview of counterfactual instantiation: Counterfactual instantiation is achieved by making users sign and share commitments to the multisig wallet. These commitments say that if the counterfactually instantiated contract were to be instantiated on-chain, the multisig wallet (which holds the state deposit) will look at the instantiated contract and transfer the appropriate state deposits based on the state of that contract. On Ethereum, you can use the CREATE2 opcode to predetermine the address of a contract. This means you can make these commitments in a channel and, if you need to dispute something, either party can deploy that contract with the chain of valid commitments. State Channel pros and cons (from Ethereum.org ): - Pros Instant withdrawal/settling on mainnet, high throughput, lower cost per transaction - Cons Time and cost to set up and settle a channel. Funds must be locked up, participants need to periodically watch the network. Channels don\u2019t support open participation. Examples of state channels are Connext, Raiden Network, and Bitcoin's Lightning Network. Sidechains The terminology here can get a little tricky, so bear with us. Sidechains are essentially blockchain networks separate from your Layer 1 (for us, Ethereum). They are connected through a bridge, which allows state to be conveyed from one chain to the other. We'll discuss this more in the crosschain and interoperability section, but essentially you'd use a chain that either has a consensus mechanism with a higher trust assumption (such as Proof of Authority) or some increased transaction throughput relative to your Layer 1. You would then be able to conduct transactions on that sidechain and, when you need to update the state (perhaps a user wishes to exit your network but wants to take their tokens), you can release it on your Layer 1. Examples of sidechains are SKALE and xDai. Conclusion This concludes our overview of the kinds of scaling solutions available to us. It is by no means comprehensive, since the field is rapidly evolving. In the next section, we'll provide a basic rubric by which you can judge any Layer 2 or general scaling solution. Additional Material Wiki: Scaling (Ethereum.org) Great overview of the topic, including the \"pros and cons\" of different solutions Video: Layer 2 Scaling Explained (Finematics) Article: Off-chain protocols: Sidechains and Rollups (Infura) Slide Deck: Scaling Ethereum using Rollups and Sidechains From the Engineering Ethereum meetup and presented by Peter Robinson. Article: Maker's roadmap for L2s Discusses one major application's understanding and strategy for Layer 2 solutions. Article: A Note on Bridges & Layer 2 Protocols (Patrick McCorry) A discussion around different sorts of bridge technologies and considerations we should have when using them. Video: How Layer 2 Addresses Barriers for Enterprise Building on Mainnet Rollups Article: An Incomplete Guide to Rollups (Vitalik Buterin) A follow-up article to Buterin's post on eth.research entitled, \"What would a rollup-centric ethereum roadmap look like?\" and here's a video version of the article. Research: Compressing Data Using Rollups A technical discussions around optimizing data compression for Rollups Thread: Rollup verification A great walkthrough about how rollups conduct verification and how that verification can make it to Layer 1 Article: (Almost) Everything You Need to Know About Optimistic Rollup (Paradigm) Really good technical overview of optimistic rollup tech Artilce: Arbitrum in under 10 minutes An explainer of Arbitrum, an optimistic rollup. Video: Scaling Ethereum with Rollups John Adler from Fuel Labs discusses the concepts behind optimistic rollups Article: Warp Your Way to Starknet Early example of a Solidity to Cairo compiler State Channels Article: State Channel Basics Article: Generalized State Channels on Ethereum Goes into more detail about counterfactual instantiation. Article: State Channels Basic overview of the technology from 2015. Good evergreen concepts here. Article Series: A State Channels Adventure A potentially NSFW walkthrough of the physics behind counterfactual instantiation. Tutorial: Precompute Contract Addresses with CREATE2 A great code tutorial from Solidity by Example showing how to find a predetermined address for a contract, the backbone of counterfactual instantiation. Code Demo: Web3 Torrent A Proof of Concept from StateChannels.org, a torrenting network built using state channels (or a subset of state channels they call virtual channels ).","title":"Types of Scaling Solutions"},{"location":"S08-scalability/M2-types/L1/#types-of-scaling-solutions","text":"Several options have been proposed and worked on for scaling Ethereum on a shorter time frame. Some of these efforts are coming to fruition now and are worth considering. The main tradeoffs for choosing a scaling solution involve considerations of throughput vs. security vs. usability . The following are some solutions currently in the works at various stages: Note: This section draws heavily on the work on Faina Shalts, engineer at Truffle (and Bootcamp alumni!) as well as from Ethereum.org","title":"Types of Scaling Solutions"},{"location":"S08-scalability/M2-types/L1/#rollups","text":"In general, on Rollup Layer 2 solutions, transactions are submitted to L2 nodes instead of L1, and batched. Eventually they are put on L1 and no longer mutable. L2 nodes are Ethereum compatible, independent blockchains. All state and execution is handled in L2: Signature verification, Contract execution, etc. The L1 only stores transaction data Note: the terminology here can be challenging but Pranay Mohan of Celo Network proposes we think of rollups as shard clients and the rollup contracts as on-chain light clients. There are two major kinds of Rollups: ZK-Rollups and Optimistic Rollups.","title":"Rollups"},{"location":"S08-scalability/M2-types/L1/#zero-knowledge-zk-rollups","text":"As we mentioned earlier in the section on Zero-Knowledge proofs, ZKPs can compress a larger amount of computation or verificatio into a single operation. ZK-Rollups bundle hundreds of transfers that occur on the ZKP Rollup L2 into a single L1, mainnet transaction via a smart contract located on L1. From the data submitted the smart contract can verify all the transfers that are included. Critically, you don\u2019t need all the data to verify the transactions, just the zero-knowledge proof. Transactions are written to Ethereum as calldata, to reduce gas. Pros No delay, less vulnerable to economic attacks Cons Limited to simple transfers and ZK-Rollup chains not compatible with EVM as validity proofs are intense to compute and have to build their own language to process. However, there is some work on building Solidity to ZKP language compilers, like this one for Cairo, Starknet's ZKP language. ZK-Rollups are not worth it for applications with little on-chain activity but are attractive for simple, high-volume exchanges. Currently using this sort of rollup: Loopring , Starkware, Matter Labs' zkSync, Aztec's ZK.Money network","title":"Zero-Knowledge / ZK-Rollups"},{"location":"S08-scalability/M2-types/L1/#optimistic-rollups","text":"Optimistic Rollups use a sidechain that sits in parallel to the mainnet Ethereum chain. They don\u2019t do any computation by default: after a transaction, the Optimistic Rollup L2s proposes a new state to the L1 mainnet, or \u201cnotarizes\u201d the transaction. L2 Transactions written to L1 mainnet as calldata . The main mechanism that makes this work are fraud proofs: If someone notices a fraudulent transaction the Optimistic Rollup network will execute a fraud-proof and run the transaction\u2019s computation using the available state data; the gas you need to run a fraud proof is reimbursed. Pros Anything you can do on L1 you can do with Optimistic Rollups because it is EVM and Solidity compatible. All transaction data is stored on the L1 chain, meaning it remains secure and decentralized. Cons Long wait times for on-chain transactions due to potential fraud challenges. Potentially vulnerable to attacks if the value in an optimistic rollup exceeds the amount in an operator\u2019s bond. Optimistic Roll-ups are currently being built by Optimistic PBC, Arbitrum, Fuel Network, ImmutableX, Deversifi and Cartesi","title":"Optimistic Rollups"},{"location":"S08-scalability/M2-types/L1/#channels","text":"Channels, also called Side Channels or State Channels, allow participants to transact a certain number of times off-chain (on the channel) while only submitting 2 transactions to the network on chain (basically, the start and stop of the channel). Fundamentally for a channel to exist, participants must lock a portion of Ethereum\u2019s state, like an ETH deposit, in a multisig contract. Locking state in this way opens up the channel, allowing for the off-chain transactions to occur. When the interaction is finished, one final L1 transaction is submitted, updating the network state based on the activity that occurred on the channel (mainly the rebalancing of funds between the participants). Sidenote: State channels on Ethereum can be enforced through a concept known as counterfactual instantiation. Here's a technical, but concise, overview of counterfactual instantiation: Counterfactual instantiation is achieved by making users sign and share commitments to the multisig wallet. These commitments say that if the counterfactually instantiated contract were to be instantiated on-chain, the multisig wallet (which holds the state deposit) will look at the instantiated contract and transfer the appropriate state deposits based on the state of that contract. On Ethereum, you can use the CREATE2 opcode to predetermine the address of a contract. This means you can make these commitments in a channel and, if you need to dispute something, either party can deploy that contract with the chain of valid commitments. State Channel pros and cons (from Ethereum.org ): - Pros Instant withdrawal/settling on mainnet, high throughput, lower cost per transaction - Cons Time and cost to set up and settle a channel. Funds must be locked up, participants need to periodically watch the network. Channels don\u2019t support open participation. Examples of state channels are Connext, Raiden Network, and Bitcoin's Lightning Network.","title":"Channels"},{"location":"S08-scalability/M2-types/L1/#sidechains","text":"The terminology here can get a little tricky, so bear with us. Sidechains are essentially blockchain networks separate from your Layer 1 (for us, Ethereum). They are connected through a bridge, which allows state to be conveyed from one chain to the other. We'll discuss this more in the crosschain and interoperability section, but essentially you'd use a chain that either has a consensus mechanism with a higher trust assumption (such as Proof of Authority) or some increased transaction throughput relative to your Layer 1. You would then be able to conduct transactions on that sidechain and, when you need to update the state (perhaps a user wishes to exit your network but wants to take their tokens), you can release it on your Layer 1. Examples of sidechains are SKALE and xDai.","title":"Sidechains"},{"location":"S08-scalability/M2-types/L1/#conclusion","text":"This concludes our overview of the kinds of scaling solutions available to us. It is by no means comprehensive, since the field is rapidly evolving. In the next section, we'll provide a basic rubric by which you can judge any Layer 2 or general scaling solution.","title":"Conclusion"},{"location":"S08-scalability/M2-types/L1/#additional-material","text":"Wiki: Scaling (Ethereum.org) Great overview of the topic, including the \"pros and cons\" of different solutions Video: Layer 2 Scaling Explained (Finematics) Article: Off-chain protocols: Sidechains and Rollups (Infura) Slide Deck: Scaling Ethereum using Rollups and Sidechains From the Engineering Ethereum meetup and presented by Peter Robinson. Article: Maker's roadmap for L2s Discusses one major application's understanding and strategy for Layer 2 solutions. Article: A Note on Bridges & Layer 2 Protocols (Patrick McCorry) A discussion around different sorts of bridge technologies and considerations we should have when using them. Video: How Layer 2 Addresses Barriers for Enterprise Building on Mainnet","title":"Additional Material"},{"location":"S08-scalability/M2-types/L1/#rollups_1","text":"Article: An Incomplete Guide to Rollups (Vitalik Buterin) A follow-up article to Buterin's post on eth.research entitled, \"What would a rollup-centric ethereum roadmap look like?\" and here's a video version of the article. Research: Compressing Data Using Rollups A technical discussions around optimizing data compression for Rollups Thread: Rollup verification A great walkthrough about how rollups conduct verification and how that verification can make it to Layer 1 Article: (Almost) Everything You Need to Know About Optimistic Rollup (Paradigm) Really good technical overview of optimistic rollup tech Artilce: Arbitrum in under 10 minutes An explainer of Arbitrum, an optimistic rollup. Video: Scaling Ethereum with Rollups John Adler from Fuel Labs discusses the concepts behind optimistic rollups Article: Warp Your Way to Starknet Early example of a Solidity to Cairo compiler","title":"Rollups"},{"location":"S08-scalability/M2-types/L1/#state-channels","text":"Article: State Channel Basics Article: Generalized State Channels on Ethereum Goes into more detail about counterfactual instantiation. Article: State Channels Basic overview of the technology from 2015. Good evergreen concepts here. Article Series: A State Channels Adventure A potentially NSFW walkthrough of the physics behind counterfactual instantiation. Tutorial: Precompute Contract Addresses with CREATE2 A great code tutorial from Solidity by Example showing how to find a predetermined address for a contract, the backbone of counterfactual instantiation. Code Demo: Web3 Torrent A Proof of Concept from StateChannels.org, a torrenting network built using state channels (or a subset of state channels they call virtual channels ).","title":"State Channels"},{"location":"S08-scalability/M3-rubric/L1/","text":"Rubric for Analyzing Scalability Solutions The challenge with understanding Layer 2 and scaling solutions is a common challenge for The explosion in demand for sophisticated, bleeding-edge cryptographic products leads to a common predicament in blockchain. For those not deeply familiar with these projects and their associated tech, it\u2019s extremely difficult\u2014if not intimidating\u2014to decide which is best and safest to use. Not to mention the rate of change of L2 documentation and the L2 frameworks themselves. However, there are trade-offs to using scalability solutions. Trade-offs to the blockchain primitives we discussed earlier in the course. With this section, we will continue propose a framework to assist the analysis of L2 projects. It\u2019s meant to help the reader develop an intuition for approaching a possible L2 solution for their various needs and circumstances. But it's also meant to highlight the trade-offs a scalability approach requires against our main blockchain network layer. This framework has eight assessment variables ( Operator , Data Availability , Computational Integrity , Economic Security , Stack Security , Capital Efficiency , Transaction Finality , and Programmability ). We use these assessment variables to score a project in three areas: Throughput , Security and Usability . See how the variables contribute to each area below: To be clear, these variables are not exhaustive. We haven\u2019t included network costs, users costs or the downstream network effects a Layer 2 inherits from the underlying Layer 1 blockchain. Again, the idea of the framework is not necessarily to be comprehensive but rather help develop an intuition for the benefits and trade-offs of different L2 solutions. Be sure to keep in mind the mental model around distributed consensus we learned earlier, it will come in handy. You can understand this lesson as a deeper development of the roles and features we discussed there. L2 Operator An Operator describes an actor within the L2 system responsible for executing state change within the L2 network. This includes facilitating entrances and exits to the L2 system as well as processing and authorizing transactions within the L2 system. Some considerations when analyzing an operator: - Is an Operator required in the L2 network? - Understanding Their Role: Who or what is the operator? What is the Operator responsible for? What is the motivation to become an Operator? - Trust Assumptions: Who can become an Operator? What power does an Operator have? What consensus rules do they abide by? What trust assumptions must users make about the Operator? - Risk Exposure: After answering the above questions, what risks do we need to consider? For example, how does the network respond when an Operator disappears, misbehaves. How does the system respond to L1 attacks or mass exit? Computational Integrity Computational Integrity (CI) is a key property of network security. CI seeks to ensure that the code for a given computation runs exactly as intended and therefore the state transition outputs are correct. Let\u2019s unpack this. The operation of each state advancement in a network can be simplified to: CI ensures that the data in the network is correct, because the code that advances the state of the network has run correctly and without manipulation by the operator actually performing the computation. It is enforced through two primary methods: Crypto-Economic Incentives This could take the form of incentivised consensus where the network can only advance if all network participants run and post the same computation resulting in a critical mass of agreement. It could also take the form of incentivised watchtowers where the network trusts third-parties to verify CI from operators and, in the case of misbehavior, trigger a network failsafe. Zero-Knowledge Cryptography Building mathematical assurances into the network\u2019s code execution model making integrity provable and verifiable Data Availability This refers to the availability and immutability of intermediary state transition data. To go back to our state advancement of the network diagram: State transition data is a list of all intermediary state changes, alongside proof they are valid (e.g. aggregate signatures, ZKPs). Once a user\u2019s assets enter a L2, they are subject to the security properties of the L2, and therefore data availability is key to being able to verify that what happens to the user\u2019s assets within the L2 is valid and correct. Economic Security Economic Security refers to the operational security that upholds the financial integrity of the L2 system. It can be rooted in two different areas: Inherited Economic Security from L1 This can be a strong inheritance where L2 assets are ultimately secured in contracts on L1 and L2 state transitions are checkpointed on L1. It can also be a weak inheritance, where L2 assets are ultimately secured on L1, but computational integrity and data availability are not secured by L1 Security Inherent to the Operator The characteristics of the L2 Operators may lend additional security, for example in Proof of Stake. Keep in mind, consensus methods such as Proof of Authority lend no inherent security beyond basic trust in the Operators. Stack Security Stack Security is more of a passive security that comes from having a battle-hardened software stack. It includes aspects like: Shared Cryptographic Primitives Or primitives developed and hardened over decades of research and application. Compare this with newer cryptographic features that may not currently enjoy widespread adoption. For example, Elliptic Curve Cryptography versus newer ZKP methods. Shared Technology Stack Shared Investments into core technology stack and open-source network effects. Examples include the Go-Ethereum implementation, Solidity compiler and EVM runtime maintained by hundreds of contributors over time. Shared Ancillary Tooling Mature development environments and workflows, such as Truffle as well as security tooling, such as EVM bytecode formal verification, Solidity static analysis and fuzzers. Shared ecosystem and aligned security objectives The powerful network effects delivered by open-source development and the community that forms around it. Includes security gathered from audits, formal verification development, and institutional knowledge of attack vectors and history. Capital Efficiency This refers to the cost and utility of capital held in an L2 network over time. L2 capital efficiency points that should be considered include: Capital must be locked into an L1 contract in order to move into the L2 Locking capital can come at high opportunity cost depending on the lockup times. Some systems require a lockup time of minutes, others require weeks. The real cost of capital can be discounted depending on the utility available within the network If the user can access great utility, the opportunity costs of lockup can be lower even if their capital is locked up. Fees for rapid exits, facilitated by liquidity providers (LPs), will be determined by real cost of capital. We'll discuss this more in our live session, \"Problems Still to Be Solved in Scalability.\" Transaction Finality Transaction finality on L1s is essentially full redeemability of assets. For L2s, there are multiple phases of finality: Operator finality, Checkpoint finality and L1 finality. The relationship of these three within a generic L2 is shown below: A user submitting a transaction within an L2 gets near-instantaneous finality when their transaction has been processed by an Operator. The user does not necessarily have to wait for L1 redeemability, rather they can use the Operator Finality as proof to exit via a Liquidity Provider. The Operator periodically submits a batch of L2 transactions and the subsequent state changes to L1. These submissions represent Checkpoint finality. The assets are not redeemable on L1 yet, but the data proving ownership of those assets are now on L1. This should give the user a much higher confidence in their assets\u2019 eventual redemption. At some point, the computational integrity of all those submitted L2 state transitions is verified on L1. Once that occurs, we have L1 finality and the user\u2019s assets are redeemable on L1. Programmability Programmability at the L2 layer is important. Programmability is the key feature of Ethereum. Transactions become a canvas for innovation compared to the more limited execution environment like Bitcoin\u2019s Script. L2 projects that reuse the Ethereum stack benefit from the collective investment into security, developer tools, community support and knowledge of stack, languages, gotchas and failure modes. Zero-knowledge proof (ZKP)-powered L2s are forcing innovation in the programmability arena. ZKP execution environments have traditionally been limited in their capacity due to the nature of their circuits. New zk-circuit-friendly languages are emerging (Gnark, Cairo, Zinc) and bringing greater programmability; however, they require new virtual machines with little proven track record and nascent community support. Extending programmability at the L2 layer will provide an opportunity to experiment with low-level innovation and EIP protocol-level changes like account abstraction, native meta-transaction, and different computation models or runtimes. Conclusion This was a very technical discussion of the rubric for analyzing L2 or scalability solutions. We hope it gives you a sense of the trade-offs each solution provides. If you'd like a less-technical approach to this same rubric, you can see the article below. Additional Material Article: For Questions to Judge Any Layer 2 Scaling Solution A less-technical, more general-audience article discussing the same paradigm outlined in this lesson Article: Evaluating Ethereum L2 Scaling Solutions: A Comparison Framework Another framework proposal from Matter Labs Article: Layer-2 for Beginners (Ali Atiia) Or How to Spot a Sidechain Charlatan and Keep Your Penny Safe","title":"Index"},{"location":"S08-scalability/M3-rubric/L1/#rubric-for-analyzing-scalability-solutions","text":"The challenge with understanding Layer 2 and scaling solutions is a common challenge for The explosion in demand for sophisticated, bleeding-edge cryptographic products leads to a common predicament in blockchain. For those not deeply familiar with these projects and their associated tech, it\u2019s extremely difficult\u2014if not intimidating\u2014to decide which is best and safest to use. Not to mention the rate of change of L2 documentation and the L2 frameworks themselves. However, there are trade-offs to using scalability solutions. Trade-offs to the blockchain primitives we discussed earlier in the course. With this section, we will continue propose a framework to assist the analysis of L2 projects. It\u2019s meant to help the reader develop an intuition for approaching a possible L2 solution for their various needs and circumstances. But it's also meant to highlight the trade-offs a scalability approach requires against our main blockchain network layer. This framework has eight assessment variables ( Operator , Data Availability , Computational Integrity , Economic Security , Stack Security , Capital Efficiency , Transaction Finality , and Programmability ). We use these assessment variables to score a project in three areas: Throughput , Security and Usability . See how the variables contribute to each area below: To be clear, these variables are not exhaustive. We haven\u2019t included network costs, users costs or the downstream network effects a Layer 2 inherits from the underlying Layer 1 blockchain. Again, the idea of the framework is not necessarily to be comprehensive but rather help develop an intuition for the benefits and trade-offs of different L2 solutions. Be sure to keep in mind the mental model around distributed consensus we learned earlier, it will come in handy. You can understand this lesson as a deeper development of the roles and features we discussed there.","title":"Rubric for Analyzing Scalability Solutions"},{"location":"S08-scalability/M3-rubric/L1/#l2-operator","text":"An Operator describes an actor within the L2 system responsible for executing state change within the L2 network. This includes facilitating entrances and exits to the L2 system as well as processing and authorizing transactions within the L2 system. Some considerations when analyzing an operator: - Is an Operator required in the L2 network? - Understanding Their Role: Who or what is the operator? What is the Operator responsible for? What is the motivation to become an Operator? - Trust Assumptions: Who can become an Operator? What power does an Operator have? What consensus rules do they abide by? What trust assumptions must users make about the Operator? - Risk Exposure: After answering the above questions, what risks do we need to consider? For example, how does the network respond when an Operator disappears, misbehaves. How does the system respond to L1 attacks or mass exit?","title":"L2 Operator"},{"location":"S08-scalability/M3-rubric/L1/#computational-integrity","text":"Computational Integrity (CI) is a key property of network security. CI seeks to ensure that the code for a given computation runs exactly as intended and therefore the state transition outputs are correct. Let\u2019s unpack this. The operation of each state advancement in a network can be simplified to: CI ensures that the data in the network is correct, because the code that advances the state of the network has run correctly and without manipulation by the operator actually performing the computation. It is enforced through two primary methods: Crypto-Economic Incentives This could take the form of incentivised consensus where the network can only advance if all network participants run and post the same computation resulting in a critical mass of agreement. It could also take the form of incentivised watchtowers where the network trusts third-parties to verify CI from operators and, in the case of misbehavior, trigger a network failsafe. Zero-Knowledge Cryptography Building mathematical assurances into the network\u2019s code execution model making integrity provable and verifiable","title":"Computational Integrity"},{"location":"S08-scalability/M3-rubric/L1/#data-availability","text":"This refers to the availability and immutability of intermediary state transition data. To go back to our state advancement of the network diagram: State transition data is a list of all intermediary state changes, alongside proof they are valid (e.g. aggregate signatures, ZKPs). Once a user\u2019s assets enter a L2, they are subject to the security properties of the L2, and therefore data availability is key to being able to verify that what happens to the user\u2019s assets within the L2 is valid and correct.","title":"Data Availability"},{"location":"S08-scalability/M3-rubric/L1/#economic-security","text":"Economic Security refers to the operational security that upholds the financial integrity of the L2 system. It can be rooted in two different areas: Inherited Economic Security from L1 This can be a strong inheritance where L2 assets are ultimately secured in contracts on L1 and L2 state transitions are checkpointed on L1. It can also be a weak inheritance, where L2 assets are ultimately secured on L1, but computational integrity and data availability are not secured by L1 Security Inherent to the Operator The characteristics of the L2 Operators may lend additional security, for example in Proof of Stake. Keep in mind, consensus methods such as Proof of Authority lend no inherent security beyond basic trust in the Operators.","title":"Economic Security"},{"location":"S08-scalability/M3-rubric/L1/#stack-security","text":"Stack Security is more of a passive security that comes from having a battle-hardened software stack. It includes aspects like: Shared Cryptographic Primitives Or primitives developed and hardened over decades of research and application. Compare this with newer cryptographic features that may not currently enjoy widespread adoption. For example, Elliptic Curve Cryptography versus newer ZKP methods. Shared Technology Stack Shared Investments into core technology stack and open-source network effects. Examples include the Go-Ethereum implementation, Solidity compiler and EVM runtime maintained by hundreds of contributors over time. Shared Ancillary Tooling Mature development environments and workflows, such as Truffle as well as security tooling, such as EVM bytecode formal verification, Solidity static analysis and fuzzers. Shared ecosystem and aligned security objectives The powerful network effects delivered by open-source development and the community that forms around it. Includes security gathered from audits, formal verification development, and institutional knowledge of attack vectors and history.","title":"Stack Security"},{"location":"S08-scalability/M3-rubric/L1/#capital-efficiency","text":"This refers to the cost and utility of capital held in an L2 network over time. L2 capital efficiency points that should be considered include: Capital must be locked into an L1 contract in order to move into the L2 Locking capital can come at high opportunity cost depending on the lockup times. Some systems require a lockup time of minutes, others require weeks. The real cost of capital can be discounted depending on the utility available within the network If the user can access great utility, the opportunity costs of lockup can be lower even if their capital is locked up. Fees for rapid exits, facilitated by liquidity providers (LPs), will be determined by real cost of capital. We'll discuss this more in our live session, \"Problems Still to Be Solved in Scalability.\"","title":"Capital Efficiency"},{"location":"S08-scalability/M3-rubric/L1/#transaction-finality","text":"Transaction finality on L1s is essentially full redeemability of assets. For L2s, there are multiple phases of finality: Operator finality, Checkpoint finality and L1 finality. The relationship of these three within a generic L2 is shown below: A user submitting a transaction within an L2 gets near-instantaneous finality when their transaction has been processed by an Operator. The user does not necessarily have to wait for L1 redeemability, rather they can use the Operator Finality as proof to exit via a Liquidity Provider. The Operator periodically submits a batch of L2 transactions and the subsequent state changes to L1. These submissions represent Checkpoint finality. The assets are not redeemable on L1 yet, but the data proving ownership of those assets are now on L1. This should give the user a much higher confidence in their assets\u2019 eventual redemption. At some point, the computational integrity of all those submitted L2 state transitions is verified on L1. Once that occurs, we have L1 finality and the user\u2019s assets are redeemable on L1.","title":"Transaction Finality"},{"location":"S08-scalability/M3-rubric/L1/#programmability","text":"Programmability at the L2 layer is important. Programmability is the key feature of Ethereum. Transactions become a canvas for innovation compared to the more limited execution environment like Bitcoin\u2019s Script. L2 projects that reuse the Ethereum stack benefit from the collective investment into security, developer tools, community support and knowledge of stack, languages, gotchas and failure modes. Zero-knowledge proof (ZKP)-powered L2s are forcing innovation in the programmability arena. ZKP execution environments have traditionally been limited in their capacity due to the nature of their circuits. New zk-circuit-friendly languages are emerging (Gnark, Cairo, Zinc) and bringing greater programmability; however, they require new virtual machines with little proven track record and nascent community support. Extending programmability at the L2 layer will provide an opportunity to experiment with low-level innovation and EIP protocol-level changes like account abstraction, native meta-transaction, and different computation models or runtimes.","title":"Programmability"},{"location":"S08-scalability/M3-rubric/L1/#conclusion","text":"This was a very technical discussion of the rubric for analyzing L2 or scalability solutions. We hope it gives you a sense of the trade-offs each solution provides. If you'd like a less-technical approach to this same rubric, you can see the article below.","title":"Conclusion"},{"location":"S08-scalability/M3-rubric/L1/#additional-material","text":"Article: For Questions to Judge Any Layer 2 Scaling Solution A less-technical, more general-audience article discussing the same paradigm outlined in this lesson Article: Evaluating Ethereum L2 Scaling Solutions: A Comparison Framework Another framework proposal from Matter Labs Article: Layer-2 for Beginners (Ali Atiia) Or How to Spot a Sidechain Charlatan and Keep Your Penny Safe","title":"Additional Material"},{"location":"S08-scalability/M4-examples/L1-optimism/","text":"Optimism Optimistic rollups are a great way to leverage all the tooling you've learned so far because they are generally compatible with the Ethereum Virtual Machine. In the next few lessons, we're going to walk through some examples of how to actually use this new technology. Again, we want to stress how new this technology is. You should be extremely cautious when working with it and be aware that documentation may not be up to date. Basic Mechanics To create a sandboxed environment which guarantees deterministic smart contract execution between L1 and L2, Optimism uses an Optimism Virtual Machine, which replaces context-dependent EVM opcodes with their OVM counterparts. See a complete comparison of the OVM vs EVM here. To accommodate this change, Optimism had to have their own compiler. They ended up forking solc and changed ~500 lines. Beward a potential gotcha: a contract compiled with the Optimism Solidity Compiler ends up bigger than it was, meaning that contracts near the 24KB limit must be refactored since they need to be executable on the mainnet as well as Optimism. Accounts on the Optimism Chain \"are redeployable contracts layer-2 contracts which can represent a user and provide a form of 'account abstraction'.\" ( source ) To put it all together, the Optimism Rollup chain: - Uses the OVM as its runtime/state transition function - Uses Optimistic Geth as the L2 client with single sequencer - Has solidity smart contracts deployed on Ethereum mainnet for data availability and dispute resolution/fraud proofs (you can read more about these \"bridge\" contracts here ) Top-level summary of how fraud proofs work: 1. Somebody will dispute a transaction if they disagree 2. They\u2019ll publish all related state on Ethereum including a Merkle proofs for each piece of state 3. They will re-execute the state transition on-chain 4. They will be rewarded for correctly disputing, the malicious sequencer will get slashed, and the invalid state roots will be pruned guaranteeing safety This is all implemented in Optimism\u2019s Fraud Prover service which is packaged with an optimistic-geth instance in a docker compose image. You can read more about the Transaction Challenge contracts here. Optimism Example We'll be walking through the Optimism Truffle Box to show you how to deploy our SimpleStorage smart contract from previous lessons to Optimism! After this example, you will be able to compile, migrate, and test Optimistic Solidity code against a variety of Optimism test networks. Requirements Node.js 10.x or later NPM version 5.2 or later docker , version 19.03.12 or later docker-compose , version 1.27.3 or later Recommended Docker memory allocation of >=8 GB. You'll also need to setup an Optimism project from your Infura account. You don't have to update your account, right now access is being offered at the \"core\" level for free up to 100,000 daily requests. When setting up your project, be sure to select the \"Ethereum\" network. Then, under settings, select the \"Optimism Kovan\" testnet, as shown below: You'll also need to have Kovan test-eth for the project if you'd like to run it on a public testnet. Follow the steps on the Kovan testnet faucet here to get some. You can also request some from MyCrypto here. Once you have Kovan ETH, you'll need to bridge it to Optimism. After getting Kovan ETH, follow these steps: Add Optimism Ethereum as a Custom RPC to your Metamask wallet, using the steps here, except set the RPC URL to https://optimism-kovan.infura.io/v3/\" + infuraKey Go to this site and bridge your Kovan ETH to Optimism Kovan ETH Ensure that your optimistic_kovan network in truffle-config.ovm.js is connected to your Optimism Kovan wallet. Note: You may get an error about your fee being too low when attempting to deploy to Optimistic Kovan. To bypass this error, you may need to increase the gas value in the optimistic_kovan network configuration in truffle-config.ovm.js to the value the error indicates. Gas price should be set at the transaction level, like so: { gasPrice: 15000000 } . Note, we'll also be building it locally so if you're having trouble finding Kovan ETH, you can start by running it locally. Let's get started! (For more detail, you can find the tutorial this lesson is based on here. ) Setup From a new directory, unbox the Optimism box: truffle unbox optimism You will need at least one mnemonic to use with the network. The .dotenv npm package has been installed for you, and you will need to create a .env file for storing your mnemonic and any other needed private information. The .env file is ignored by git in this project, to help protect your private data. In general, it is good security practice to avoid committing information about your private keys to github. The truffle-config.ovm.js file expects a GANACHE_MNEMONIC and a KOVAN_MNEMONIC value to exist in .env for running commands on each of these networks, as well as a default MNEMONIC for the optimism network we will run locally. If you are unfamiliar with using .env for managing your mnemonics and other keys, the basic steps for doing so are below: Use touch .env in the command line to create a .env file at the root of your project. Open the .env file in your preferred IDE Add the following, filling in your own Infura project key and mnemonics: MNEMONIC=\"candy maple cake sugar pudding cream honey rich smooth crumble sweet treat\" INFURA_KEY=\" \" GANACHE_MNEMONIC=\" \" KOVAN_MNEMONIC=\" \" Note: the value for the MNEMONIC above is the one you should use, as it is expected within the local Optimism Ethereum network we will run in this Truffle Box. As you develop your project, you can put any other sensitive information in this file. You can access it from other files with require('dotenv').config() and refer to the variable you need with process.env[' '] . Some Differences You may notice some differences in the workflow from our typical Truffle environment. For example, a new configuration file exists in this project: truffle-config.ovm.js . This file contains a reference to the new file location of the contracts_build_directory and contracts_directory for Optimism contracts and lists several networks for running the Optimism Layer 2 network instance. Another difference: When you compile or migrate, the resulting json files will be at build/optimism-contracts/ . This is to distinguish them from any Ethereum contracts you build, which will live in build/ethereum-contracts . As we have included the appropriate contracts_build_directory in each configuration file, Truffle will know which set of built files to reference. Compiling The SimpleStorage.sol contract code is already in both the ethereum and optimism directories. To compile using the Optimism Virtual Machine compiler, run: npm run compile:ovm As we mentioned earlier, the OVM and EVM compiler are slightly different, so keep an eye out for any issues or errors. Migration Now that we've compiled the contract for Optimism, we can migrate it to an Optimism Layer 2 network. First, let's just try to our local Ganache, which will be almost similar to a normal Ethereum ganache instance: npm run migrate:ovm --network=ganache This may be a bit underwhelming! However, if we have loaded in our Infura Optimism Kovan network endpoint and have enough Optimism Kovan eth in the wallet tied to the .env mnemonic, we can also run: npm run migrate:ovm --network=optimistic_kovan Like standard Truffle, if you would like to migrate previously migrated contracts on the same network, you can run truffle migrate --config truffle-config.ovm.js --network=(ganache | optimistic_ethereum | optimistic_kovan) and add the --reset flag. Following the above steps should allow you to deploy to the Optimism Layer 2 chain. This is only the first step! Once you are ready to deploy your own contracts to function on Layer 1 Ethereum using Layer 2 Optimism, you will need to be aware of the ways in which Layer 1 and Layer 2 interact in the Optimism ecosystem. Furthermore, keep an eye out for new developments in Truffle tooling to assist with bridging L1-L2 data and execution. Please note that, at this moment, Optimism has a whitelist of applications that are allowed to go from Optimism to Ethereum mainnet. You can learn more about that here. Additional Material Optimism website Docs: Optimism Great documentation for different audiences, whether you want to learn about the infrastructure or deploy dapps. Tutorial: Optimism Beginner Tutorial Docs: Whitelisting on Optimism","title":"Optimism"},{"location":"S08-scalability/M4-examples/L1-optimism/#optimism","text":"Optimistic rollups are a great way to leverage all the tooling you've learned so far because they are generally compatible with the Ethereum Virtual Machine. In the next few lessons, we're going to walk through some examples of how to actually use this new technology. Again, we want to stress how new this technology is. You should be extremely cautious when working with it and be aware that documentation may not be up to date.","title":"Optimism"},{"location":"S08-scalability/M4-examples/L1-optimism/#basic-mechanics","text":"To create a sandboxed environment which guarantees deterministic smart contract execution between L1 and L2, Optimism uses an Optimism Virtual Machine, which replaces context-dependent EVM opcodes with their OVM counterparts. See a complete comparison of the OVM vs EVM here. To accommodate this change, Optimism had to have their own compiler. They ended up forking solc and changed ~500 lines. Beward a potential gotcha: a contract compiled with the Optimism Solidity Compiler ends up bigger than it was, meaning that contracts near the 24KB limit must be refactored since they need to be executable on the mainnet as well as Optimism. Accounts on the Optimism Chain \"are redeployable contracts layer-2 contracts which can represent a user and provide a form of 'account abstraction'.\" ( source ) To put it all together, the Optimism Rollup chain: - Uses the OVM as its runtime/state transition function - Uses Optimistic Geth as the L2 client with single sequencer - Has solidity smart contracts deployed on Ethereum mainnet for data availability and dispute resolution/fraud proofs (you can read more about these \"bridge\" contracts here ) Top-level summary of how fraud proofs work: 1. Somebody will dispute a transaction if they disagree 2. They\u2019ll publish all related state on Ethereum including a Merkle proofs for each piece of state 3. They will re-execute the state transition on-chain 4. They will be rewarded for correctly disputing, the malicious sequencer will get slashed, and the invalid state roots will be pruned guaranteeing safety This is all implemented in Optimism\u2019s Fraud Prover service which is packaged with an optimistic-geth instance in a docker compose image. You can read more about the Transaction Challenge contracts here.","title":"Basic Mechanics"},{"location":"S08-scalability/M4-examples/L1-optimism/#optimism-example","text":"We'll be walking through the Optimism Truffle Box to show you how to deploy our SimpleStorage smart contract from previous lessons to Optimism! After this example, you will be able to compile, migrate, and test Optimistic Solidity code against a variety of Optimism test networks.","title":"Optimism Example"},{"location":"S08-scalability/M4-examples/L1-optimism/#requirements","text":"Node.js 10.x or later NPM version 5.2 or later docker , version 19.03.12 or later docker-compose , version 1.27.3 or later Recommended Docker memory allocation of >=8 GB. You'll also need to setup an Optimism project from your Infura account. You don't have to update your account, right now access is being offered at the \"core\" level for free up to 100,000 daily requests. When setting up your project, be sure to select the \"Ethereum\" network. Then, under settings, select the \"Optimism Kovan\" testnet, as shown below: You'll also need to have Kovan test-eth for the project if you'd like to run it on a public testnet. Follow the steps on the Kovan testnet faucet here to get some. You can also request some from MyCrypto here. Once you have Kovan ETH, you'll need to bridge it to Optimism. After getting Kovan ETH, follow these steps: Add Optimism Ethereum as a Custom RPC to your Metamask wallet, using the steps here, except set the RPC URL to https://optimism-kovan.infura.io/v3/\" + infuraKey Go to this site and bridge your Kovan ETH to Optimism Kovan ETH Ensure that your optimistic_kovan network in truffle-config.ovm.js is connected to your Optimism Kovan wallet. Note: You may get an error about your fee being too low when attempting to deploy to Optimistic Kovan. To bypass this error, you may need to increase the gas value in the optimistic_kovan network configuration in truffle-config.ovm.js to the value the error indicates. Gas price should be set at the transaction level, like so: { gasPrice: 15000000 } . Note, we'll also be building it locally so if you're having trouble finding Kovan ETH, you can start by running it locally. Let's get started! (For more detail, you can find the tutorial this lesson is based on here. )","title":"Requirements"},{"location":"S08-scalability/M4-examples/L1-optimism/#setup","text":"From a new directory, unbox the Optimism box: truffle unbox optimism You will need at least one mnemonic to use with the network. The .dotenv npm package has been installed for you, and you will need to create a .env file for storing your mnemonic and any other needed private information. The .env file is ignored by git in this project, to help protect your private data. In general, it is good security practice to avoid committing information about your private keys to github. The truffle-config.ovm.js file expects a GANACHE_MNEMONIC and a KOVAN_MNEMONIC value to exist in .env for running commands on each of these networks, as well as a default MNEMONIC for the optimism network we will run locally. If you are unfamiliar with using .env for managing your mnemonics and other keys, the basic steps for doing so are below: Use touch .env in the command line to create a .env file at the root of your project. Open the .env file in your preferred IDE Add the following, filling in your own Infura project key and mnemonics: MNEMONIC=\"candy maple cake sugar pudding cream honey rich smooth crumble sweet treat\" INFURA_KEY=\" \" GANACHE_MNEMONIC=\" \" KOVAN_MNEMONIC=\" \" Note: the value for the MNEMONIC above is the one you should use, as it is expected within the local Optimism Ethereum network we will run in this Truffle Box. As you develop your project, you can put any other sensitive information in this file. You can access it from other files with require('dotenv').config() and refer to the variable you need with process.env[' '] .","title":"Setup"},{"location":"S08-scalability/M4-examples/L1-optimism/#some-differences","text":"You may notice some differences in the workflow from our typical Truffle environment. For example, a new configuration file exists in this project: truffle-config.ovm.js . This file contains a reference to the new file location of the contracts_build_directory and contracts_directory for Optimism contracts and lists several networks for running the Optimism Layer 2 network instance. Another difference: When you compile or migrate, the resulting json files will be at build/optimism-contracts/ . This is to distinguish them from any Ethereum contracts you build, which will live in build/ethereum-contracts . As we have included the appropriate contracts_build_directory in each configuration file, Truffle will know which set of built files to reference.","title":"Some Differences"},{"location":"S08-scalability/M4-examples/L1-optimism/#compiling","text":"The SimpleStorage.sol contract code is already in both the ethereum and optimism directories. To compile using the Optimism Virtual Machine compiler, run: npm run compile:ovm As we mentioned earlier, the OVM and EVM compiler are slightly different, so keep an eye out for any issues or errors.","title":"Compiling"},{"location":"S08-scalability/M4-examples/L1-optimism/#migration","text":"Now that we've compiled the contract for Optimism, we can migrate it to an Optimism Layer 2 network. First, let's just try to our local Ganache, which will be almost similar to a normal Ethereum ganache instance: npm run migrate:ovm --network=ganache This may be a bit underwhelming! However, if we have loaded in our Infura Optimism Kovan network endpoint and have enough Optimism Kovan eth in the wallet tied to the .env mnemonic, we can also run: npm run migrate:ovm --network=optimistic_kovan Like standard Truffle, if you would like to migrate previously migrated contracts on the same network, you can run truffle migrate --config truffle-config.ovm.js --network=(ganache | optimistic_ethereum | optimistic_kovan) and add the --reset flag. Following the above steps should allow you to deploy to the Optimism Layer 2 chain. This is only the first step! Once you are ready to deploy your own contracts to function on Layer 1 Ethereum using Layer 2 Optimism, you will need to be aware of the ways in which Layer 1 and Layer 2 interact in the Optimism ecosystem. Furthermore, keep an eye out for new developments in Truffle tooling to assist with bridging L1-L2 data and execution. Please note that, at this moment, Optimism has a whitelist of applications that are allowed to go from Optimism to Ethereum mainnet. You can learn more about that here.","title":"Migration"},{"location":"S08-scalability/M4-examples/L1-optimism/#additional-material","text":"Optimism website Docs: Optimism Great documentation for different audiences, whether you want to learn about the infrastructure or deploy dapps. Tutorial: Optimism Beginner Tutorial Docs: Whitelisting on Optimism","title":"Additional Material"},{"location":"S08-scalability/M4-examples/L2-arbitrum/","text":"Arbitrum Example In this lesson, we're going to walkthrough setting up a development environment for the optimistic rollup protocol Arbitrum, built by Offchain Labs. As we've discussed before, optimistic rollups offer an easier path to scaling in certain situations since they leverage much of the same toolkit as EVM development. Out of the box, Arbitrum supports Solidity as well as Truffle, Hardhat, The Graph, etc. You can see the differences between Ethereum and Arbitrum here. Like Optimism, Arbitrum's consensus layer is a dispute-based one, called a Multi-Round Interactive Optimistic Rollup protocol. Anyone can monitor or submit disputes on the Arbitrum chain, Read more about the protocol here and how to run a validator here. As of now, Arbitrum has limited functionality in terms of porting assets from L1 >> L2 and vice versa. See its capabilities in the tutorials listed here. Again, we want to stress how new this technology is. You should be extremely cautious when working with it and be aware that documentation may not be up to date. The requirements and setup for Arbitrum are very similar to the previous lesson on Optimism, except that Arbitrum's testnet is running on Rinkeby while Optimism's is on Kovan. We'll also be using our SimpleStorage.sol contract as a generic stand-in for your dapp's contract. Feel free to swap it out! Requirements Node.js 10.x or later NPM version 5.2 or later docker , version 19.03.12 or later docker-compose , version 1.27.3 or later Recommended Docker memory allocation of >=8 GB. You'll also need to setup an Arbitrum project on your Infura account. You don't have to update your account, right now access is being offered at the \"core\" level for free up to 100,000 daily requests. When setting up your project, be sure to select the \"Ethereum\" network. Then, under settings, select the \"Arbitrum Rinkeby\" testnet, as shown below: You'll also need to have Rinkeby test-ETH for the project if you'd like to run it on a public testnet. Follow the steps on the Rinkeby testnet faucet here to get some. Once you have Rinkeby test-ETH, you'll need to bridge it to Arbitrum testnet. Follow these steps: Add Arbitrum Ethereum as a Custom RPC to your Metamask wallet, using the steps here, except set the RPC URL to https://arbitrum-rinkeby.infura.io/v3/\" + infuraKey Go to this site and bridge your Rinkeby ETH to Arbitrum Rinkeby ETH Ensure that your arbitrum_testnet network in truffle-config.arbitrum.js is the mnemonic associated with your Arbitrum Rinkeby MetaMask wallet we just bridged. Let's get started! (For more detail, you can find the tutorial this lesson is based on here. ) Setup From a new directory, unbox the Arbitrum box: truffle unbox arbitrum You will need at least one mnemonic to use with the network. The .dotenv npm package has been installed for you, and you will need to create a .env file for storing your mnemonic and any other needed private information. The .env file is ignored by git in this project, to help protect your private data. In general, it is good security practice to avoid committing information about your private keys to github. The truffle-config.arbitrum.js file expects a MNEMONIC value to exist in .env for running commands on each of these networks, as well as a default MNEMONIC for the Arbitrum network we will run locally. If you are unfamiliar with using .env for managing your mnemonics and other keys, the basic steps for doing so are below: Use touch .env in the command line to create a .env file at the root of your project. Open the .env file in your preferred IDE Add the following, filling in your own Infura project key and mnemonics: MNEMONIC=\"jar deny prosper gasp flush glass core corn alarm treat leg smart\" INFURA_KEY=\"<Your Infura Project ID>\" RINKEBY_MNEMONIC=\"<Your Rinkeby Mnemonic>\" MAINNET_MNEMONIC=\"<Your Arbitrum Mainnet Mnemonic>\" Note: the value for the MNEMONIC above is the one you should use, as it is expected within the local arbitrum network we will run in this Truffle Box. As you develop your project, you can put any other sensitive information in this file. You can access it from other files with require('dotenv').config() and refer to the variable you need with process.env['<YOUR_VARIABLE>'] . Some Differences You may notice some differences in the workflow from our typical Truffle environment. For example,a new configuration file exists in this project: truffle-config.arbitrum.js . This file contains a reference to the new file location of the contracts_build_directory and contracts_directory for Arbitrum contracts and lists several networks for running the Arbitrum Layer 2 network instance. Please note, the classic truffle-config.js configuration file is included here as well, because you will eventually want to deploy contracts to Ethereum as well. All normal truffle commands ( truffle compile , truffle migrate , etc.) will use this config file and save built files to build/ethereum-contracts . You can save Solidity contracts that you wish to deploy to Ethereum in the contracts/ethereum folder. Another difference: When you compile or migrate, the resulting json files will be at build/arbitrum-contracts/ . This is to distinguish them from any Ethereum contracts you build, which will live in build/ethereum-contracts . As we have included the appropriate contracts_build_directory in each configuration file, Truffle will know which set of built files to reference. Compiling To compile your Arbitrum contracts, run the following in your terminal: npm run compile:arbitrum This script lets Truffle know to use the truffle-config.arbitrum.js configuration file, which tells Truffle where to store your build artifacts. When adding new contracts to compile, you may find some discrepancies and errors, so please remember to keep an eye on differences between ethereum and Arbitrum . Migration Now that we've compiled the contract for Arbitrum, we can migrate it to an Arbitrum Layer 2 network. First, let's just try to our local Ganache, which will be almost similar to a normal Ethereum ganache instance: npm run migrate:ovm --network=ganache This may be a bit underwhelming! However, if we have loaded in our Infura Arbitrum Rinkeby network endpoint and have enough Arbitrum Rinkeby ETH in the wallet tied to the .env mnemonic, we can also run: npm run migrate:ovm --network=arbitrum_testnet Like standard Truffle, if you would like to migrate previously migrated contracts on the same network, you can run truffle migrate --config truffle-config.arbitrum.js --network=(arbitrum_local | arbitrum_testnet | arbitrum_mainnet) and add the --reset flag. Following the above steps should allow you to deploy to the Optimism Layer 2 chain. This is only the first step! Once you are ready to deploy your own contracts to function on Layer 1 Ethereum using Layer 2 Arbitrum, you will need to be aware of the ways in which Layer 1 and Layer 2 interact in the Arbitrum ecosystem. Furthermore, keep an eye out for new developments in Truffle tooling to assist with bridging L1-L2 data and execution. Additional Material Docs: Arbitrum Quickstart Tutorial: Truffle's Pet Shop Box on Arbitrum","title":"Arbitrum Example"},{"location":"S08-scalability/M4-examples/L2-arbitrum/#arbitrum-example","text":"In this lesson, we're going to walkthrough setting up a development environment for the optimistic rollup protocol Arbitrum, built by Offchain Labs. As we've discussed before, optimistic rollups offer an easier path to scaling in certain situations since they leverage much of the same toolkit as EVM development. Out of the box, Arbitrum supports Solidity as well as Truffle, Hardhat, The Graph, etc. You can see the differences between Ethereum and Arbitrum here. Like Optimism, Arbitrum's consensus layer is a dispute-based one, called a Multi-Round Interactive Optimistic Rollup protocol. Anyone can monitor or submit disputes on the Arbitrum chain, Read more about the protocol here and how to run a validator here. As of now, Arbitrum has limited functionality in terms of porting assets from L1 >> L2 and vice versa. See its capabilities in the tutorials listed here. Again, we want to stress how new this technology is. You should be extremely cautious when working with it and be aware that documentation may not be up to date. The requirements and setup for Arbitrum are very similar to the previous lesson on Optimism, except that Arbitrum's testnet is running on Rinkeby while Optimism's is on Kovan. We'll also be using our SimpleStorage.sol contract as a generic stand-in for your dapp's contract. Feel free to swap it out!","title":"Arbitrum Example"},{"location":"S08-scalability/M4-examples/L2-arbitrum/#requirements","text":"Node.js 10.x or later NPM version 5.2 or later docker , version 19.03.12 or later docker-compose , version 1.27.3 or later Recommended Docker memory allocation of >=8 GB. You'll also need to setup an Arbitrum project on your Infura account. You don't have to update your account, right now access is being offered at the \"core\" level for free up to 100,000 daily requests. When setting up your project, be sure to select the \"Ethereum\" network. Then, under settings, select the \"Arbitrum Rinkeby\" testnet, as shown below: You'll also need to have Rinkeby test-ETH for the project if you'd like to run it on a public testnet. Follow the steps on the Rinkeby testnet faucet here to get some. Once you have Rinkeby test-ETH, you'll need to bridge it to Arbitrum testnet. Follow these steps: Add Arbitrum Ethereum as a Custom RPC to your Metamask wallet, using the steps here, except set the RPC URL to https://arbitrum-rinkeby.infura.io/v3/\" + infuraKey Go to this site and bridge your Rinkeby ETH to Arbitrum Rinkeby ETH Ensure that your arbitrum_testnet network in truffle-config.arbitrum.js is the mnemonic associated with your Arbitrum Rinkeby MetaMask wallet we just bridged. Let's get started! (For more detail, you can find the tutorial this lesson is based on here. )","title":"Requirements"},{"location":"S08-scalability/M4-examples/L2-arbitrum/#setup","text":"From a new directory, unbox the Arbitrum box: truffle unbox arbitrum You will need at least one mnemonic to use with the network. The .dotenv npm package has been installed for you, and you will need to create a .env file for storing your mnemonic and any other needed private information. The .env file is ignored by git in this project, to help protect your private data. In general, it is good security practice to avoid committing information about your private keys to github. The truffle-config.arbitrum.js file expects a MNEMONIC value to exist in .env for running commands on each of these networks, as well as a default MNEMONIC for the Arbitrum network we will run locally. If you are unfamiliar with using .env for managing your mnemonics and other keys, the basic steps for doing so are below: Use touch .env in the command line to create a .env file at the root of your project. Open the .env file in your preferred IDE Add the following, filling in your own Infura project key and mnemonics: MNEMONIC=\"jar deny prosper gasp flush glass core corn alarm treat leg smart\" INFURA_KEY=\"<Your Infura Project ID>\" RINKEBY_MNEMONIC=\"<Your Rinkeby Mnemonic>\" MAINNET_MNEMONIC=\"<Your Arbitrum Mainnet Mnemonic>\" Note: the value for the MNEMONIC above is the one you should use, as it is expected within the local arbitrum network we will run in this Truffle Box. As you develop your project, you can put any other sensitive information in this file. You can access it from other files with require('dotenv').config() and refer to the variable you need with process.env['<YOUR_VARIABLE>'] .","title":"Setup"},{"location":"S08-scalability/M4-examples/L2-arbitrum/#some-differences","text":"You may notice some differences in the workflow from our typical Truffle environment. For example,a new configuration file exists in this project: truffle-config.arbitrum.js . This file contains a reference to the new file location of the contracts_build_directory and contracts_directory for Arbitrum contracts and lists several networks for running the Arbitrum Layer 2 network instance. Please note, the classic truffle-config.js configuration file is included here as well, because you will eventually want to deploy contracts to Ethereum as well. All normal truffle commands ( truffle compile , truffle migrate , etc.) will use this config file and save built files to build/ethereum-contracts . You can save Solidity contracts that you wish to deploy to Ethereum in the contracts/ethereum folder. Another difference: When you compile or migrate, the resulting json files will be at build/arbitrum-contracts/ . This is to distinguish them from any Ethereum contracts you build, which will live in build/ethereum-contracts . As we have included the appropriate contracts_build_directory in each configuration file, Truffle will know which set of built files to reference.","title":"Some Differences"},{"location":"S08-scalability/M4-examples/L2-arbitrum/#compiling","text":"To compile your Arbitrum contracts, run the following in your terminal: npm run compile:arbitrum This script lets Truffle know to use the truffle-config.arbitrum.js configuration file, which tells Truffle where to store your build artifacts. When adding new contracts to compile, you may find some discrepancies and errors, so please remember to keep an eye on differences between ethereum and Arbitrum .","title":"Compiling"},{"location":"S08-scalability/M4-examples/L2-arbitrum/#migration","text":"Now that we've compiled the contract for Arbitrum, we can migrate it to an Arbitrum Layer 2 network. First, let's just try to our local Ganache, which will be almost similar to a normal Ethereum ganache instance: npm run migrate:ovm --network=ganache This may be a bit underwhelming! However, if we have loaded in our Infura Arbitrum Rinkeby network endpoint and have enough Arbitrum Rinkeby ETH in the wallet tied to the .env mnemonic, we can also run: npm run migrate:ovm --network=arbitrum_testnet Like standard Truffle, if you would like to migrate previously migrated contracts on the same network, you can run truffle migrate --config truffle-config.arbitrum.js --network=(arbitrum_local | arbitrum_testnet | arbitrum_mainnet) and add the --reset flag. Following the above steps should allow you to deploy to the Optimism Layer 2 chain. This is only the first step! Once you are ready to deploy your own contracts to function on Layer 1 Ethereum using Layer 2 Arbitrum, you will need to be aware of the ways in which Layer 1 and Layer 2 interact in the Arbitrum ecosystem. Furthermore, keep an eye out for new developments in Truffle tooling to assist with bridging L1-L2 data and execution.","title":"Migration"},{"location":"S08-scalability/M4-examples/L2-arbitrum/#additional-material","text":"Docs: Arbitrum Quickstart Tutorial: Truffle's Pet Shop Box on Arbitrum","title":"Additional Material"},{"location":"S08-scalability/M5-crosschain/L1/","text":"Crosschain Communication and Blockchain Interoperability In this section, we're going to discuss about why blockchain networks might want to communicate to each other and how that occurs. We mentioned way back in the \"Blockchain Fundamentals: Network Configurations\" section about the compatibility of certain blockchains and how they might link together. We're going to spend this lesson discussing this in more detail Why? From Peter Robinson: [T]here is no one blockchain to rule them all. There are public permissionless blockchains such as Ethereum Mainnet, Bitcoin, and Filecoin. There are many instances of permissioned, consortium blockchains such as Enterprise Ethereum / Quorum, Corda and Hyperledger Fabric. Within the public permissionless Ethereum space, Sidechains and Roll-ups (L2s) are emerging to improve scalability. All of these technologies need Crosschain Communications to allow them to communicate. Even if the world settled on one blockchain technology platform, all data and all functionality is unlikely to reside on just one instance of the blockchain platform. Crosschain communications allows data (state) and functionality (execution) that resides on one blockchain to be accessible from another blockchain. The seeds of crosschain communication can be found in the Bitcoin whitepaper. Under the section, \"Simplified Payment Verification,\" Nakamoto writes: It is possible to verify payments without running a full network node. A user only needs to keep a copy of the block headers of the longest proof-of-work chain, which he can get by querying network nodes until he's convinced he has the longest chain, and obtain the Merkle branch linking the transaction to the block it's timestamped in. He can't check the transaction for himself, but by linking it to a place in the chain, he can see that a network node has accepted it, and blocks added after it further confirm the network has accepted it. This \"light\" process proves the validity of a transaction by including the verified (mined) block header hash it appeared in as well as the Merkle Proof showing how the individual transaction is included in that hash. Developers realized they could use this implementation to build something that would validate Bitcoin payments on another chain entirely, and used it to build one of the first blockchain bridges, BTC Relay. Simplified Diagram of BTC Relay using transactions mined on Blockchain A to validate action on Blockchain B BTC Relay used the Simplified Payment Verification to move Ether from one account to the other using a Bitcoin transaction. Transactions could only be validated if the block header they relate to is on the longest chain and if at least six block headers have been posted on top of the block header that the transaction relates to. As attackers can not produce a longer chain than the main Bitcoin blockchain due to the mining difficulty, they are unable to confirm transactions based on a malicious fork. Other mechanisms of doing similar crosschain action include Hash Time Locked Contracts. These simple constructions allow two parties to exchange tokens on two separate blockchains without trusting each other. How? Crosschain communication relies on the concept of Atomicity. We previously spoke about atomicity in \"Trustless Consensus.\" An atomic message is accepted or rejected by a network, there is no in-between or halfway. Let's take a look at what that means: In the image above, a user submits a transaction that calls the crossSwap function on the Cons contract on the Source Blockchain . The function uses the message sender as the account to transfer from. The account to transfer to and the amount are specified by the to and the val parameters. If the user\u2019s account doesn\u2019t have a large enough balance then the transaction terminates with a revert error. Otherwise, the to and from account balances are updated and a crosschain function call is executed to execute the destination blockchain part of the crosschain transaction. Along with Atomicity, crosschain communication also requires Liveness of the networks involved. This means that the network is progressing in a stable, safe way. For example, the BTC Relay only works if the Ethereum network keeps up with the blocks being mined on Bitcoin. Only a series of uninterrupted Bitcoin blocks transmitted to the Ethereum smart contract (\"liveness\") will allow the Simplified Payment Verification to work. Without liveness, the networks involved cannot capture the crosschain communication and incorporate it into their respective global states. Events Many EVM-based protocols rely on transferring information between blockchains using Events. Transactions that trigger events programmatically generate log events that are stored in transaction receipts. The transaction receipts for all transactions in a block are stored in a modified Merkle Patricia Tree. The root hash of this tree is stored in the Ethereum block header. The log event information includes: the address of the contract that emitted the event, an identifier known as a topic that specifies the type of event that is emitted, and a data blob containing the encoded event parameters. This ability to programmatically produce events can be used to produce information on a source blockchain that can be consumed on a destination blockchain. Examples of Crosschain and Blockchain Interoperability Celo Optics by cLabs Validators sign and transfer state roots to destination blockchains. Validators can be slashed on the source chain for signing invalid state roots. Cross-Chain Interoperability Protocol by Chainlink Connext's Noncustodial Xchain Transfer Protocol (NXTP) Uses HTLCs as the underlying crosschain consensus mechanism. Cosmos Inter-Blockchain Communication (IBC) A multi-blockchain system in which blockchains called Zones communicate transactions via a central blockchain called a Hub. The Zones and the Hub typically use Tendermint a type of Practical Byzantine Fault Tolerance consensus algorithm. Rainbow Bridge by NEAR Protocol Uses NEAR's Rust execution environment to power crosschain Simplified Payment Verification transactions with a threshold number of validators to attest. Polkadot Uses a relay chain to coordinate information across many \"parachains\" . The parachains can also \"connect with external networks via bridges.\" BSC's Binance Bridge This is a two-way bridge between Binance's Smart Contract network (which is EVM-compatible) and Ethereum mainnet. Additional Material Video: Building Bridges, Not Walled Gardens An argument for bridges and crosschain communication from James Prestwich of cLabs. Academic Article: Survey of Crosschain Communications Protocols (Peter Robinson) A comprehensive technical overview of the requirements and characteristics of crosschain communication. Academic Article: General Purpose Atomic Crosschain Transaction protocol A standardized, composable way to conduct crosschain transactions across EVM blockchains. Article: Logging Events Using Smart Contracts (Ethereum.org) Wiki: Chainlist A list of EVM-compatible networks Wiki: Crosschain Comparison (Peter Robinson) Article: Blockchain Interoperability: An Overview Of The Current Research (ConsenSys) Article: Build {on} bridges, not [behind] walls (Andrew Hong) An analysis of the current state of bridges in blockchain networks.","title":"Crosschain Communication and Blockchain Interoperability"},{"location":"S08-scalability/M5-crosschain/L1/#crosschain-communication-and-blockchain-interoperability","text":"In this section, we're going to discuss about why blockchain networks might want to communicate to each other and how that occurs. We mentioned way back in the \"Blockchain Fundamentals: Network Configurations\" section about the compatibility of certain blockchains and how they might link together. We're going to spend this lesson discussing this in more detail","title":"Crosschain Communication and Blockchain Interoperability"},{"location":"S08-scalability/M5-crosschain/L1/#why","text":"From Peter Robinson: [T]here is no one blockchain to rule them all. There are public permissionless blockchains such as Ethereum Mainnet, Bitcoin, and Filecoin. There are many instances of permissioned, consortium blockchains such as Enterprise Ethereum / Quorum, Corda and Hyperledger Fabric. Within the public permissionless Ethereum space, Sidechains and Roll-ups (L2s) are emerging to improve scalability. All of these technologies need Crosschain Communications to allow them to communicate. Even if the world settled on one blockchain technology platform, all data and all functionality is unlikely to reside on just one instance of the blockchain platform. Crosschain communications allows data (state) and functionality (execution) that resides on one blockchain to be accessible from another blockchain. The seeds of crosschain communication can be found in the Bitcoin whitepaper. Under the section, \"Simplified Payment Verification,\" Nakamoto writes: It is possible to verify payments without running a full network node. A user only needs to keep a copy of the block headers of the longest proof-of-work chain, which he can get by querying network nodes until he's convinced he has the longest chain, and obtain the Merkle branch linking the transaction to the block it's timestamped in. He can't check the transaction for himself, but by linking it to a place in the chain, he can see that a network node has accepted it, and blocks added after it further confirm the network has accepted it. This \"light\" process proves the validity of a transaction by including the verified (mined) block header hash it appeared in as well as the Merkle Proof showing how the individual transaction is included in that hash. Developers realized they could use this implementation to build something that would validate Bitcoin payments on another chain entirely, and used it to build one of the first blockchain bridges, BTC Relay. Simplified Diagram of BTC Relay using transactions mined on Blockchain A to validate action on Blockchain B BTC Relay used the Simplified Payment Verification to move Ether from one account to the other using a Bitcoin transaction. Transactions could only be validated if the block header they relate to is on the longest chain and if at least six block headers have been posted on top of the block header that the transaction relates to. As attackers can not produce a longer chain than the main Bitcoin blockchain due to the mining difficulty, they are unable to confirm transactions based on a malicious fork. Other mechanisms of doing similar crosschain action include Hash Time Locked Contracts. These simple constructions allow two parties to exchange tokens on two separate blockchains without trusting each other.","title":"Why?"},{"location":"S08-scalability/M5-crosschain/L1/#how","text":"Crosschain communication relies on the concept of Atomicity. We previously spoke about atomicity in \"Trustless Consensus.\" An atomic message is accepted or rejected by a network, there is no in-between or halfway. Let's take a look at what that means: In the image above, a user submits a transaction that calls the crossSwap function on the Cons contract on the Source Blockchain . The function uses the message sender as the account to transfer from. The account to transfer to and the amount are specified by the to and the val parameters. If the user\u2019s account doesn\u2019t have a large enough balance then the transaction terminates with a revert error. Otherwise, the to and from account balances are updated and a crosschain function call is executed to execute the destination blockchain part of the crosschain transaction. Along with Atomicity, crosschain communication also requires Liveness of the networks involved. This means that the network is progressing in a stable, safe way. For example, the BTC Relay only works if the Ethereum network keeps up with the blocks being mined on Bitcoin. Only a series of uninterrupted Bitcoin blocks transmitted to the Ethereum smart contract (\"liveness\") will allow the Simplified Payment Verification to work. Without liveness, the networks involved cannot capture the crosschain communication and incorporate it into their respective global states.","title":"How?"},{"location":"S08-scalability/M5-crosschain/L1/#events","text":"Many EVM-based protocols rely on transferring information between blockchains using Events. Transactions that trigger events programmatically generate log events that are stored in transaction receipts. The transaction receipts for all transactions in a block are stored in a modified Merkle Patricia Tree. The root hash of this tree is stored in the Ethereum block header. The log event information includes: the address of the contract that emitted the event, an identifier known as a topic that specifies the type of event that is emitted, and a data blob containing the encoded event parameters. This ability to programmatically produce events can be used to produce information on a source blockchain that can be consumed on a destination blockchain.","title":"Events"},{"location":"S08-scalability/M5-crosschain/L1/#examples-of-crosschain-and-blockchain-interoperability","text":"Celo Optics by cLabs Validators sign and transfer state roots to destination blockchains. Validators can be slashed on the source chain for signing invalid state roots. Cross-Chain Interoperability Protocol by Chainlink Connext's Noncustodial Xchain Transfer Protocol (NXTP) Uses HTLCs as the underlying crosschain consensus mechanism. Cosmos Inter-Blockchain Communication (IBC) A multi-blockchain system in which blockchains called Zones communicate transactions via a central blockchain called a Hub. The Zones and the Hub typically use Tendermint a type of Practical Byzantine Fault Tolerance consensus algorithm. Rainbow Bridge by NEAR Protocol Uses NEAR's Rust execution environment to power crosschain Simplified Payment Verification transactions with a threshold number of validators to attest. Polkadot Uses a relay chain to coordinate information across many \"parachains\" . The parachains can also \"connect with external networks via bridges.\" BSC's Binance Bridge This is a two-way bridge between Binance's Smart Contract network (which is EVM-compatible) and Ethereum mainnet.","title":"Examples of Crosschain and Blockchain Interoperability"},{"location":"S08-scalability/M5-crosschain/L1/#additional-material","text":"Video: Building Bridges, Not Walled Gardens An argument for bridges and crosschain communication from James Prestwich of cLabs. Academic Article: Survey of Crosschain Communications Protocols (Peter Robinson) A comprehensive technical overview of the requirements and characteristics of crosschain communication. Academic Article: General Purpose Atomic Crosschain Transaction protocol A standardized, composable way to conduct crosschain transactions across EVM blockchains. Article: Logging Events Using Smart Contracts (Ethereum.org) Wiki: Chainlist A list of EVM-compatible networks Wiki: Crosschain Comparison (Peter Robinson) Article: Blockchain Interoperability: An Overview Of The Current Research (ConsenSys) Article: Build {on} bridges, not [behind] walls (Andrew Hong) An analysis of the current state of bridges in blockchain networks.","title":"Additional Material"},{"location":"S10-eth2/M1-background/","text":"A Brief History Ever since the launch of Ethereum in 2015, there have been plans to update its core protocol significantly. In December 2015, five months after the launch of Ethereum mainnet, Vitalik Buterin wrote a post describing the roadmap and eventual adoption of Ethereum 2.0: Buterin's post describing Ethereum roadmap, including Ethereum 2.0 AKA Serenity As we've discussed earlier, Ethereum has been able to regularly update its protocol through multiple, coordinated network hard forks ( no small feat! ) to meet these roadmap requirements. However, these have primarily been upgrades within the constructs of the Ethereum protocol originally formulated in the Yellow Paper and launched in 2015. Changes made to the protocol since 2015 can been seen as reactive adjustments to the realities of running a cryptocurrency network. If we were to think of the Ethereum network as a house, launch of Ethereum mainnet was the building of the house and everyone moving in. Network forks are the patches, repairs and additional features to address challenges that arise from more people moving into the house. The upgrade to Ethereum 2.0 (\"Serenity\" on the roadmap) will be akin to building an entirely new foundation and house to accommodate the next generation of network growth. (With regards to the mental model we've discussed earlier, the Ethereum 2.0 migration is an updating of the consensus mechanism, as circled below:) But we have this huge community and entire businesses in the current Ethereum home (called \"Ethereum 1.x\"). We can't risk moving everyone in to an entirely new house and then discovering a catastrophic error. Therefore, Ethereum 2.0 is being introduced in a series of phases. Imagine the Ethereum 2.0 house being beside the Ethereum 1.x home. At first, there will be a path connecting the two. Ultimately, Ethereum 2.0 will expand to include Ethereum 1.x within it. Launch of Beacon Chain (Proof-of-Stake) Phases 0 is the implementation of a Proof of Stake consensus mechanism for Ethereum. The blockchain secured by Phase 0's PoS mechanism is called the Beacon Chain. Miners for the Beacon Chain are called validators . In Proof of Work, miners must exert CPU to find a block and nonce that meets the difficulty. In Proof of Stake, the validators stake financial value to the blocks they propose and approve. They also are watching all the other validators to make sure no one is proposing incorrect blocks (containing errors or fraudulent activity). If a validator is caught proposing or confirming invalid blocks, they will be slashed (their financial stake is penalized). Phase 0 is the phase most public one now, as it's currently undergoing public testing and there are multiple companies developing software for it. Here is a list of the main validator clients: Teku \u2014 Java-based client developed by ConsenSys Software Prysm \u2014 Go-based client developed by Prysmatic Labs Lighthouse \u2014 Rust-based client developed by Sigma Prime Lodestar \u2014 Javascript-based client Nimbus \u2014 Nim-based client developed by Status The Medalla testnet was the first, public, multi-client testnet available. You can check out its progress here and join the testnet by following these steps. Beacon Chain Launch On October 14, 2020, the Beacon Chain deposit contract went live on Ethereum mainnet (L1) and people interested in validators could start locking up their ETH for validating using Launchpad. On November 24, 2020, the minimum amount of ETH needed to start the Beacon Chain (524,288) was reached, which triggered the go-live mechanism to start the network in seven days. On December 1st at 12pm UTC, the Beacon chain\u2019s first blocks were validated. The first block came from Validator 19026, with the enigmatic graffiti, \u201cMr F was here.\u201d Twelve seconds later came the next block, graffiti indicating the validator might be located in Zug, Switzerland. The Eth2 Beacon Chain grew steadily, block by block every 12 seconds. Then came the next hurdle: would enough validators be online to finalize the first Epoch? Yes! 82.27% of the validators attested to the validity of the Epoch 0, the proverbial ground floor of the Beacon Chain. You can see the latest blocks from the Beacon Chain on Beaconcha.in. Additional Material Ethereum 2.0 development develops and matures day-by-day. It can be challenging to keep up with the bleeding edge of this exciting new network. Here are some resources below to help: You can read the entire Ethereum 2.0 spec, including phases 0, 1 and 2 here. Ethereum 2.0 Glossary (ConsenSys) Ethereum 2.0 Terms Demystified Ethereum 2.0 Knowledge Base My Journey to Becoming a Validator on Ethereum 2.0 \u2014 Part 1 Coogan walks through a four-part series of becoming a validator on Ethereum 2.0. Started before the Beacon Chain launched! To follow the progress of Ethereum 2.0, subscribe to What's New in Eth2, which sends weekly updates.","title":"Index"},{"location":"S10-eth2/M1-background/#a-brief-history","text":"Ever since the launch of Ethereum in 2015, there have been plans to update its core protocol significantly. In December 2015, five months after the launch of Ethereum mainnet, Vitalik Buterin wrote a post describing the roadmap and eventual adoption of Ethereum 2.0: Buterin's post describing Ethereum roadmap, including Ethereum 2.0 AKA Serenity As we've discussed earlier, Ethereum has been able to regularly update its protocol through multiple, coordinated network hard forks ( no small feat! ) to meet these roadmap requirements. However, these have primarily been upgrades within the constructs of the Ethereum protocol originally formulated in the Yellow Paper and launched in 2015. Changes made to the protocol since 2015 can been seen as reactive adjustments to the realities of running a cryptocurrency network. If we were to think of the Ethereum network as a house, launch of Ethereum mainnet was the building of the house and everyone moving in. Network forks are the patches, repairs and additional features to address challenges that arise from more people moving into the house. The upgrade to Ethereum 2.0 (\"Serenity\" on the roadmap) will be akin to building an entirely new foundation and house to accommodate the next generation of network growth. (With regards to the mental model we've discussed earlier, the Ethereum 2.0 migration is an updating of the consensus mechanism, as circled below:) But we have this huge community and entire businesses in the current Ethereum home (called \"Ethereum 1.x\"). We can't risk moving everyone in to an entirely new house and then discovering a catastrophic error. Therefore, Ethereum 2.0 is being introduced in a series of phases. Imagine the Ethereum 2.0 house being beside the Ethereum 1.x home. At first, there will be a path connecting the two. Ultimately, Ethereum 2.0 will expand to include Ethereum 1.x within it.","title":"A Brief History"},{"location":"S10-eth2/M1-background/#launch-of-beacon-chain-proof-of-stake","text":"Phases 0 is the implementation of a Proof of Stake consensus mechanism for Ethereum. The blockchain secured by Phase 0's PoS mechanism is called the Beacon Chain. Miners for the Beacon Chain are called validators . In Proof of Work, miners must exert CPU to find a block and nonce that meets the difficulty. In Proof of Stake, the validators stake financial value to the blocks they propose and approve. They also are watching all the other validators to make sure no one is proposing incorrect blocks (containing errors or fraudulent activity). If a validator is caught proposing or confirming invalid blocks, they will be slashed (their financial stake is penalized). Phase 0 is the phase most public one now, as it's currently undergoing public testing and there are multiple companies developing software for it. Here is a list of the main validator clients: Teku \u2014 Java-based client developed by ConsenSys Software Prysm \u2014 Go-based client developed by Prysmatic Labs Lighthouse \u2014 Rust-based client developed by Sigma Prime Lodestar \u2014 Javascript-based client Nimbus \u2014 Nim-based client developed by Status The Medalla testnet was the first, public, multi-client testnet available. You can check out its progress here and join the testnet by following these steps.","title":"Launch of Beacon Chain (Proof-of-Stake)"},{"location":"S10-eth2/M1-background/#beacon-chain-launch","text":"On October 14, 2020, the Beacon Chain deposit contract went live on Ethereum mainnet (L1) and people interested in validators could start locking up their ETH for validating using Launchpad. On November 24, 2020, the minimum amount of ETH needed to start the Beacon Chain (524,288) was reached, which triggered the go-live mechanism to start the network in seven days. On December 1st at 12pm UTC, the Beacon chain\u2019s first blocks were validated. The first block came from Validator 19026, with the enigmatic graffiti, \u201cMr F was here.\u201d Twelve seconds later came the next block, graffiti indicating the validator might be located in Zug, Switzerland. The Eth2 Beacon Chain grew steadily, block by block every 12 seconds. Then came the next hurdle: would enough validators be online to finalize the first Epoch? Yes! 82.27% of the validators attested to the validity of the Epoch 0, the proverbial ground floor of the Beacon Chain. You can see the latest blocks from the Beacon Chain on Beaconcha.in.","title":"Beacon Chain Launch"},{"location":"S10-eth2/M1-background/#additional-material","text":"Ethereum 2.0 development develops and matures day-by-day. It can be challenging to keep up with the bleeding edge of this exciting new network. Here are some resources below to help: You can read the entire Ethereum 2.0 spec, including phases 0, 1 and 2 here. Ethereum 2.0 Glossary (ConsenSys) Ethereum 2.0 Terms Demystified Ethereum 2.0 Knowledge Base My Journey to Becoming a Validator on Ethereum 2.0 \u2014 Part 1 Coogan walks through a four-part series of becoming a validator on Ethereum 2.0. Started before the Beacon Chain launched! To follow the progress of Ethereum 2.0, subscribe to What's New in Eth2, which sends weekly updates.","title":"Additional Material"},{"location":"S10-eth2/M2-key-terms/","text":"Key Terms for Ethereum 2.0 There are some new terms that might be confusing with Ethereum 2.0, we wanted to list a few here to help you understand them better. First, we should mention there's a discussion around even the term \"Ethereum 2.0.\" Some folks would like there to just be the \"Application Layer\" and the \"Consensus Layer\". Here's the distinction, from Ethereum Foundation's Danny Ryan: What we call \u201ceth2\u201d is a series of major upgrades to Ethereum\u2019s consensus-layer \u2013 to ensure the protocol is secure, sustainable, and scalable \u2013 while \u201ceth2 clients\u201d are implementations of this proof-of-stake consensus. And, what we call \u201ceth1\u201d in this context is Ethereum\u2019s rich application-layer, and similarly, \u201ceth1 clients\u201d (after the upgrade to proof-of-stake) are the software that does the heavy lifting in this layer. Ethereum\u2019s application-layer is currently driven by a proof-of-work consensus algorithm but will soon be driven by the beacon chain \u2013 the proof-of-stake consensus mechanism that is currently in production and secured by ~3.5M ETH. People are still using the term Ethereum 2.0 or Eth2 (like we are in this course), but eventually both chains will just be \"Ethereum\"! Okay, here are some more Ethereum 2.0 terms and their explanations: Proof of Stake and Validators Proof of Stake is the new consensus mechanism for Ethereum. It currently is the consensus mechanism for the Beacon chain. Eventually, the Ethereum 1.0 chain will migrate to the Ethereum 2.0, replacing mainnet's consensus mechanism from Proof of Work to Proof of Stake. Actors that are validating the network state in the Beacon chain are called validators. Slashing Ethereum 2.0\u2019s consensus mechanism has a couple of rules that are designed to prevent attacks on the network. Any validator found to have broken these rules will be slashed and ejected from the network. Slashing means that a significant part of the validator\u2019s stake is removed: up to the whole stake of 32 ETH in the worst case. Slashing is interesting because it is a financial disincentive for behavior on public blockchains. As we read earlier about Proof of Work, there are only incentives for people to act in accordance with the network consensus layer. Staking Pools and Staking-as-a-Service Staking as a Service is where you give your (32*n) ETH to a service which handles all of the validating process for you for a fee. Much easier mental load, great for institutional interest. For people with less than 32ETH (which has become very expensive now!), there are some interesting options of decentralized pools where you can contribute either validator power or less than 32ETH. The staking pools combine that and distribute the staking rewards accordingly. A full list of these services can be found here. The Ethereum Due Diligence Committee is doing their best to provide an unbiased assessment of these services. Deposit Contract The Deposit contract is the pivot point from Ethereum mainnet to the Beacon Chain for actors wishing to become validators on the Beacon Chain. In order to register as a validator on the network, a user must generate Ethereum 2.0 keys by making a one-way deposit of ETH into the official deposit contract. (You should not send ETH to this contract directly! You will lose those funds forever!) Finality Proof of Stake consensus mechanisms offer finality: After a small period of time, a block is declared final, which means that it can never be changed. All the transactions in that block and all previous transactions are permanent, immutable, and guaranteed forever. However, Finality presents some issues around Weak Subjectivity: If \u2153 of validators withdraw their stake and continue signing blocks and attestations, they can form a chain which conflicts with the finalized state. If your node is far enough behind the chain head to not be aware that they\u2019ve withdrawn their funds, the exited validators can trick you into following the wrong chain. ( source ) Well-behaved validators who have successfully and properly exited the chain can sell their private keys on the black market to a malicious actor. (There is no financial disincentive for them to do this as their funds have safely exited the protocol). You can read more about Weak Subjectivity and how to address it here. Secret Shared Validators (SSV) From the SSV.network page: Secret Shared Validators (SSV) is the first secure and robust way to split a validator key for ETH staking between non-trusting nodes, or operators. The protocol enables the distributed control and operation of an Ethereum validator. The key is split in such a way that no operator must trust the other to operate, a certain amount can go offline without affecting network performance, and no operator can take unilateral control of the network. The result is decentralization, fault tolerance, and optimal security for staking on Ethereum. Database Sharding Database sharding is used in conventional computer programming to increases scalability of large systems. From this article: A database shard is a horizontal partition of data in a database or search engine. Each individual partition is referred to as a shard or database shard. Each shard is held on a separate database server instance, to spread load. Some data within a database remains present in all shards, but some appears only in a single shard. Each shard (or server) acts as the single source for this subset of data. Ethereum 2.0 leverages traditional database sharding to decrease the amount of memory needed to maintain the full state of the network. Originally meant to be 1024 shards, the current spec will produce 64 database shards. Each of these shards will have their own validators. They will periodically check into the beacon chain using crosslinks, which is a summary of the state of that shard and the only representation of the shard on the Beacon Chain. Phase 1 is meant to reduce the resources needed to use Ethereum in a decentralized way. Currently, to interact with Ethereum network on your own node, it requires 500GB of memory and significant RAM, not to mention syncing time. With sharding and light clients, the idea is to decrease the amount of information needed to submit your own, valid transactions. Read more about sharding on Ethereum 2.0 here. BLS Encryption The key signature system Ethereum 2.0 will be using is BLS. BLS allows multiple digital signatures to be collapsed into a single verifiable one. This is helpful with collecting attestations of the beacon ( \u201cvotes in regards to the validity of a shard block or beacon\u201d ). Most pertinent for us, the BLS scheme is different from the scheme used for Ethereum 1.0. To swap out the encryption curve, Ethereum core developers have come up with a clever solution, which is a classic handshake: In the diagram above, the blue key and boxes represent Ethereum 1.0 and its cryptographic scheme and the red key and boxes represent Ethereum 2.0 and its cryptographic scheme. The deposit contract, which exists on Ethereum 1.0 Mainnet, allows the user to prove they have private keys for Ethereum 1.0 and Ethereum 2.0. Here\u2019s how that works: The transaction submitted to the deposit contract on Ethereum 1.0 has to be signed by an Ethereum 1.0 private key (like any transaction submitted on Mainnet). However, that transaction is wrapped around another private key signature, the Ethereum 2.0 private key. The beacon chain is watching the deposit contract on Ethereum 1.0, if a valid transaction is submitted to the contract with the correct balance, the beacon chain then unwraps the first layer of encryption and accesses the second layer, the Ethereum 2.0 digital signature. That is used to confirm the Ethereum 2.0 validator address and connect it to an Ethereum 1.0 address. Here's how that handshake looks in the Beacon Chain contract deployed to Ethereum mainnet: Execution Environments (Computation) The final phase of Ethereum 2.0 deals with the execution environments present on each shard. in Ethereum 1.x, the execution environment is the Ethereum Virtual Machine, a Turing-complete programming language that provides a universal computation environment for all in the network to use. However, this universality comes at an efficiency cost. The EVM is slow compared to modern processing languages. Phase 2 addresses this processing cost by using a version of WebAssembly , a new type of code developed by Mozilla. It allows code written in C, C++ or Rust but executed in the browser to run at near-native speeds. (For more information about the implications of WebAssembly for the broader web, please see this video: \"Rust, WebAssembly and the Future of Serverless\" ) In Phase 2, each shard will be allowed a unique execution environment. While at least one will be running the EVM (for sake of continuity), it's possible others will be running execution environments (EEs) for Libra, Bitcoin, or any other blockchain network. Note: This may change as the Eth1-Eth2 merge gets closer. It seems as though the emphasis may be on getting the Beacon Chain executable on EVM, rather than various execution environments. Additional Material Ethereum 2.0 Glossay (ConsenSys) Beaconcha.in Knowledge Base Ethereum 2.0 (Ethhub.io) What's New in Eth2 (Ben Edgington) Great newsletter round-up Finalized article series by Danny Ryan For more information about BLS, please see this thread from Jeff Coleman or this Reddit post about the history of BLS development for Ethereum 2.0.","title":"Index"},{"location":"S10-eth2/M2-key-terms/#key-terms-for-ethereum-20","text":"There are some new terms that might be confusing with Ethereum 2.0, we wanted to list a few here to help you understand them better. First, we should mention there's a discussion around even the term \"Ethereum 2.0.\" Some folks would like there to just be the \"Application Layer\" and the \"Consensus Layer\". Here's the distinction, from Ethereum Foundation's Danny Ryan: What we call \u201ceth2\u201d is a series of major upgrades to Ethereum\u2019s consensus-layer \u2013 to ensure the protocol is secure, sustainable, and scalable \u2013 while \u201ceth2 clients\u201d are implementations of this proof-of-stake consensus. And, what we call \u201ceth1\u201d in this context is Ethereum\u2019s rich application-layer, and similarly, \u201ceth1 clients\u201d (after the upgrade to proof-of-stake) are the software that does the heavy lifting in this layer. Ethereum\u2019s application-layer is currently driven by a proof-of-work consensus algorithm but will soon be driven by the beacon chain \u2013 the proof-of-stake consensus mechanism that is currently in production and secured by ~3.5M ETH. People are still using the term Ethereum 2.0 or Eth2 (like we are in this course), but eventually both chains will just be \"Ethereum\"! Okay, here are some more Ethereum 2.0 terms and their explanations:","title":"Key Terms for Ethereum 2.0"},{"location":"S10-eth2/M2-key-terms/#proof-of-stake-and-validators","text":"Proof of Stake is the new consensus mechanism for Ethereum. It currently is the consensus mechanism for the Beacon chain. Eventually, the Ethereum 1.0 chain will migrate to the Ethereum 2.0, replacing mainnet's consensus mechanism from Proof of Work to Proof of Stake. Actors that are validating the network state in the Beacon chain are called validators.","title":"Proof of Stake and Validators"},{"location":"S10-eth2/M2-key-terms/#slashing","text":"Ethereum 2.0\u2019s consensus mechanism has a couple of rules that are designed to prevent attacks on the network. Any validator found to have broken these rules will be slashed and ejected from the network. Slashing means that a significant part of the validator\u2019s stake is removed: up to the whole stake of 32 ETH in the worst case. Slashing is interesting because it is a financial disincentive for behavior on public blockchains. As we read earlier about Proof of Work, there are only incentives for people to act in accordance with the network consensus layer.","title":"Slashing"},{"location":"S10-eth2/M2-key-terms/#staking-pools-and-staking-as-a-service","text":"Staking as a Service is where you give your (32*n) ETH to a service which handles all of the validating process for you for a fee. Much easier mental load, great for institutional interest. For people with less than 32ETH (which has become very expensive now!), there are some interesting options of decentralized pools where you can contribute either validator power or less than 32ETH. The staking pools combine that and distribute the staking rewards accordingly. A full list of these services can be found here. The Ethereum Due Diligence Committee is doing their best to provide an unbiased assessment of these services.","title":"Staking Pools and Staking-as-a-Service"},{"location":"S10-eth2/M2-key-terms/#deposit-contract","text":"The Deposit contract is the pivot point from Ethereum mainnet to the Beacon Chain for actors wishing to become validators on the Beacon Chain. In order to register as a validator on the network, a user must generate Ethereum 2.0 keys by making a one-way deposit of ETH into the official deposit contract. (You should not send ETH to this contract directly! You will lose those funds forever!)","title":"Deposit Contract"},{"location":"S10-eth2/M2-key-terms/#finality","text":"Proof of Stake consensus mechanisms offer finality: After a small period of time, a block is declared final, which means that it can never be changed. All the transactions in that block and all previous transactions are permanent, immutable, and guaranteed forever. However, Finality presents some issues around Weak Subjectivity: If \u2153 of validators withdraw their stake and continue signing blocks and attestations, they can form a chain which conflicts with the finalized state. If your node is far enough behind the chain head to not be aware that they\u2019ve withdrawn their funds, the exited validators can trick you into following the wrong chain. ( source ) Well-behaved validators who have successfully and properly exited the chain can sell their private keys on the black market to a malicious actor. (There is no financial disincentive for them to do this as their funds have safely exited the protocol). You can read more about Weak Subjectivity and how to address it here.","title":"Finality"},{"location":"S10-eth2/M2-key-terms/#secret-shared-validators-ssv","text":"From the SSV.network page: Secret Shared Validators (SSV) is the first secure and robust way to split a validator key for ETH staking between non-trusting nodes, or operators. The protocol enables the distributed control and operation of an Ethereum validator. The key is split in such a way that no operator must trust the other to operate, a certain amount can go offline without affecting network performance, and no operator can take unilateral control of the network. The result is decentralization, fault tolerance, and optimal security for staking on Ethereum.","title":"Secret Shared Validators (SSV)"},{"location":"S10-eth2/M2-key-terms/#database-sharding","text":"Database sharding is used in conventional computer programming to increases scalability of large systems. From this article: A database shard is a horizontal partition of data in a database or search engine. Each individual partition is referred to as a shard or database shard. Each shard is held on a separate database server instance, to spread load. Some data within a database remains present in all shards, but some appears only in a single shard. Each shard (or server) acts as the single source for this subset of data. Ethereum 2.0 leverages traditional database sharding to decrease the amount of memory needed to maintain the full state of the network. Originally meant to be 1024 shards, the current spec will produce 64 database shards. Each of these shards will have their own validators. They will periodically check into the beacon chain using crosslinks, which is a summary of the state of that shard and the only representation of the shard on the Beacon Chain. Phase 1 is meant to reduce the resources needed to use Ethereum in a decentralized way. Currently, to interact with Ethereum network on your own node, it requires 500GB of memory and significant RAM, not to mention syncing time. With sharding and light clients, the idea is to decrease the amount of information needed to submit your own, valid transactions. Read more about sharding on Ethereum 2.0 here.","title":"Database Sharding"},{"location":"S10-eth2/M2-key-terms/#bls-encryption","text":"The key signature system Ethereum 2.0 will be using is BLS. BLS allows multiple digital signatures to be collapsed into a single verifiable one. This is helpful with collecting attestations of the beacon ( \u201cvotes in regards to the validity of a shard block or beacon\u201d ). Most pertinent for us, the BLS scheme is different from the scheme used for Ethereum 1.0. To swap out the encryption curve, Ethereum core developers have come up with a clever solution, which is a classic handshake: In the diagram above, the blue key and boxes represent Ethereum 1.0 and its cryptographic scheme and the red key and boxes represent Ethereum 2.0 and its cryptographic scheme. The deposit contract, which exists on Ethereum 1.0 Mainnet, allows the user to prove they have private keys for Ethereum 1.0 and Ethereum 2.0. Here\u2019s how that works: The transaction submitted to the deposit contract on Ethereum 1.0 has to be signed by an Ethereum 1.0 private key (like any transaction submitted on Mainnet). However, that transaction is wrapped around another private key signature, the Ethereum 2.0 private key. The beacon chain is watching the deposit contract on Ethereum 1.0, if a valid transaction is submitted to the contract with the correct balance, the beacon chain then unwraps the first layer of encryption and accesses the second layer, the Ethereum 2.0 digital signature. That is used to confirm the Ethereum 2.0 validator address and connect it to an Ethereum 1.0 address. Here's how that handshake looks in the Beacon Chain contract deployed to Ethereum mainnet:","title":"BLS Encryption"},{"location":"S10-eth2/M2-key-terms/#execution-environments-computation","text":"The final phase of Ethereum 2.0 deals with the execution environments present on each shard. in Ethereum 1.x, the execution environment is the Ethereum Virtual Machine, a Turing-complete programming language that provides a universal computation environment for all in the network to use. However, this universality comes at an efficiency cost. The EVM is slow compared to modern processing languages. Phase 2 addresses this processing cost by using a version of WebAssembly , a new type of code developed by Mozilla. It allows code written in C, C++ or Rust but executed in the browser to run at near-native speeds. (For more information about the implications of WebAssembly for the broader web, please see this video: \"Rust, WebAssembly and the Future of Serverless\" ) In Phase 2, each shard will be allowed a unique execution environment. While at least one will be running the EVM (for sake of continuity), it's possible others will be running execution environments (EEs) for Libra, Bitcoin, or any other blockchain network. Note: This may change as the Eth1-Eth2 merge gets closer. It seems as though the emphasis may be on getting the Beacon Chain executable on EVM, rather than various execution environments.","title":"Execution Environments (Computation)"},{"location":"S10-eth2/M2-key-terms/#additional-material","text":"Ethereum 2.0 Glossay (ConsenSys) Beaconcha.in Knowledge Base Ethereum 2.0 (Ethhub.io) What's New in Eth2 (Ben Edgington) Great newsletter round-up Finalized article series by Danny Ryan For more information about BLS, please see this thread from Jeff Coleman or this Reddit post about the history of BLS development for Ethereum 2.0.","title":"Additional Material"},{"location":"S10-eth2/M3-future-considerations/","text":"Future Considerations As this course launches, on September 2021, there have been some interesting developments towards an Eth1-Eth2 merge. We'll discuss a few of these here. Please note, this may change as it is currently be developed! First, there is a \"mainnet readiness\" checklist the Ethereum developers will be using to judge when the time is right for an Eth1-Eth2 merge. Here's the rough timeline given by Tim Beiko in August 2021: There's also been talk of using rollups as a way to help with the Merge. Essentially, the L2s will function as a sort of shard-like data layer. You can read more about that here. Rayonism Network In March 2021, as part of ETHGlobal Scaling hackathon, a group of Ethereum 2.0 developers tried to create a merge testnet. Known as Rayonism, it was meant as a short-term proof of concept project to allow researchers to test certain assumptions being made about the Eth1-Eth2 merge. One of the features it meant to test was the notion of time, a particularly challenging issue with distributed networks, as we discussed earlier in the course. The Rayonism testnet did run for a short period of time. You can checkout the Merge testnet tutorial they ran here (although it most likely will not work now!) Additional Materials Video: What Happens after ETH1 and ETH2 merge? (Vitalik Buterin) Forum: A Rollup-centric Ethereum Roadmap Github: Mainnet Readiness Checklist Github: Eth1 and Eth2 Merge spec","title":"Index"},{"location":"S10-eth2/M3-future-considerations/#future-considerations","text":"As this course launches, on September 2021, there have been some interesting developments towards an Eth1-Eth2 merge. We'll discuss a few of these here. Please note, this may change as it is currently be developed! First, there is a \"mainnet readiness\" checklist the Ethereum developers will be using to judge when the time is right for an Eth1-Eth2 merge. Here's the rough timeline given by Tim Beiko in August 2021: There's also been talk of using rollups as a way to help with the Merge. Essentially, the L2s will function as a sort of shard-like data layer. You can read more about that here.","title":"Future Considerations"},{"location":"S10-eth2/M3-future-considerations/#rayonism-network","text":"In March 2021, as part of ETHGlobal Scaling hackathon, a group of Ethereum 2.0 developers tried to create a merge testnet. Known as Rayonism, it was meant as a short-term proof of concept project to allow researchers to test certain assumptions being made about the Eth1-Eth2 merge. One of the features it meant to test was the notion of time, a particularly challenging issue with distributed networks, as we discussed earlier in the course. The Rayonism testnet did run for a short period of time. You can checkout the Merge testnet tutorial they ran here (although it most likely will not work now!)","title":"Rayonism Network"},{"location":"S10-eth2/M3-future-considerations/#additional-materials","text":"Video: What Happens after ETH1 and ETH2 merge? (Vitalik Buterin) Forum: A Rollup-centric Ethereum Roadmap Github: Mainnet Readiness Checklist Github: Eth1 and Eth2 Merge spec","title":"Additional Materials"}]}